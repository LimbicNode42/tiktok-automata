[
  {
    "title": "Everything Apple Plans to Show at Its iOS 26-Focused WWDC 2025 Event",
    "content": "Article content from https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
    "summary": "10-minute read",
    "url": "https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "dev",
    "word_count": 2000
  },
  {
    "title": "Microsoft and Asus announce two Xbox Ally handhelds with new Xbox full-screen experience",
    "content": "News Microsoft and Asus announce two Xbox Ally handhelds with new Xbox full-screen experience Project Kennan arrives alongside a new fullscreen Xbox experience that hides the complexities of Windows away. Project Kennan arrives alongside a new fullscreen Xbox experience that hides the complexities of Windows away. by Tom Warren Jun 8, 2025, 5:10 PM UTC Link Facebook Threads Tom Warren is a senior editor and author of Notepad , who has been covering all things Microsoft, PC, and tech for over 20 years. Microsoft and Asus have been working together over the past year to create not one, but two new ROG Xbox Ally handhelds. Both of these Xbox Ally devices, part of the Project Kennan effort I reported on earlier this year , include a new full-screen Xbox experience on Windows that’s designed to be more handheld-friendly and hide away the complexity of Windows to focus on gaming instead. The white ROG Xbox Ally is designed for 720p gaming, and the more powerful black ROG Xbox Ally X targets 900p to 1080p gaming on the go. Like the existing ROG Ally and Ally X, the new Xbox Ally and Xbox Ally X share the same 7-inch 1080p screen, complete with a 120Hz refresh rate and VRR support. The Xbox Ally uses a previously unannounced AMD Ryzen Z2 A chip, combined with 16GB of LPDDR5X-6400 RAM and 512GB of M.2 2280 SSD storage. The Xbox Ally X upgrades the chip to AMD’s Ryzen AI Z2 Extreme processor, 24GB of LPDDR5X-8000 memory, and a 1TB M.2 2280 SSD. The white ROG Xbox Ally ships with AMD’s Ryzen Z2 A processor. Image: Microsoft The ROG Ally X adds a better AMD processor and impulse triggers. Image: Microsoft All of these specs make them very similar to Asus’ existing Windows-powered handheld gaming PCs at heart, but there are some much-needed changes to the software side that could make the Windows handheld experience a lot better. “We know that to take this handheld experience to the next level, we cannot do this alone,” says Shawn Yen, vice president of consumer at Asus, in a briefing with The Verge , admitting that some gamers have found it “frustrating and confusing” to navigate Windows with joysticks and button until now. Microsoft and Asus have been collaborating closely on these two new Xbox Ally devices over the past year, and Yen says Microsoft and Asus “share a joint obsession” on these new handhelds. That joint obsession includes Microsoft making good on its promise to combine “ the best of Xbox and Windows together ,” thanks to a new Xbox full-screen experience on Windows that’s designed specifically for handhelds. Not only can the Xbox Ally devices boot directly to this interface, but the companies claim you can easily get back to it using a new dedicated Xbox button on these handhelds, much like an Xbox console. Microsoft doesn’t load the full Windows desktop or a bunch of background processes in this full-screen Xbox experience, putting Windows firmly in the background and freeing up more memory for games. Instead, you launch straight into the Xbox PC app, which includes all of your PC games from the Microsoft Store, Battle.net, and what Microsoft calls “other leading storefronts.” The Xbox Ally with the new full-screen Xbox experience on Windows. Image: Microsoft This aggregated gaming library means you’ll see games from Xbox, Game Pass, and all your PC games installed from Steam, Epic Games Store, and elsewhere in a single interface, much like what the GOG launcher offers. Earlier this week, we started seeing parts of this unified library appear in the Xbox PC app, and Microsoft says you’ll be able to access your full Xbox console library through Xbox Cloud Gaming or Remote Play to an Xbox console. The idea is that you should be able to seamlessly launch any game you own, whether it’s actually installed on your handheld, streaming from your Xbox Series X over home Wi-Fi, or streaming from the cloud, though we have yet to try that ourselves. Microsoft has also made some additional tweaks to the Xbox PC app and Game Bar to make this all more handheld-friendly, including the ability to log in via the Windows lockscreen with your controller , no touchscreen taps required. You’ll also be able to use this handheld-friendly Game Bar interface to easily launch apps like Discord, or alt-tab between apps and games, or adjust settings without having to fiddle with the touchscreen. You can read all about all the Windows changes in my deep dive look at this new Xbox PC experience right here . Related This is how Microsoft is combining Windows and Xbox for handheld PCs Xbox Games Showcase 2025: all the news and trailers These two Xbox Ally devices also have Xbox-like contoured grips. It’s as if Microsoft and Asus have taken an Xbox controller and squeezed a screen between the grips, similar to what Sony did with its PlayStation Portal. The grips have been designed like this to make it easier to wrap your hands around the entire controls, so you access all the buttons and triggers. The Xbox Ally X even has impulse trigg...",
    "summary": "7-minute read",
    "url": "https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 860
  },
  {
    "title": "BYD's Five-Minute Charging Puts China in the Lead for EVs",
    "content": "Transportation News BYD’s Five-Minute Charging Puts China in the Lead for EVs The world’s largest EV maker now has the world’s fastest DC charger Lawrence Ulrich 06 Jun 2025 4 min read Lawrence Ulrich is an award-winning auto writer and former chief auto critic at The New York Times and The Detroit Free Press. Chinese EV manufacturer BYD is setting new records for charging times. BYD",
    "summary": "4-minute read",
    "url": "https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "dev",
    "word_count": 66
  },
  {
    "title": "Breakthrough in search for HIV cure leaves researchers ‘overwhelmed'",
    "content": "Article content from https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed?utm_source=tldrnewsletter",
    "summary": "2-minute read",
    "url": "https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 400
  },
  {
    "title": "Field Notes From Shipping Real Code With Claude",
    "content": "June 7, 2025 Field Notes From Shipping Real Code With Claude Vibe Coding Isn’t Just a Vibe Shimmering Substance - Jackson Pollock Note : This post comes with a NotebookLM podcast ( 1 linked at the bottom), and three generated audio recordings. You can read the conversation I had with ChatGPT while preparing drafts of this post. Comments and discussion on the related HN post . Audio Think of this post as your field guide to a new way of building software. By the time you finish reading, you’ll understand not just the how but the why behind AI-assisted development that actually works. Here’s What You’re Going to Learn First, we’ll explore how to genuinely achieve a 10x productivity boost—not through magic, but through deliberate practices that amplify AI’s strengths while compensating for its weaknesses. Next, I’ll walk you through the infrastructure we use at Julep to ship production code daily with Claude’s help. You’ll see our CLAUDE.md templates, our commit strategies, and guardrails. Most importantly, you’ll understand why writing your own tests remains absolutely sacred, even (especially) in the age of AI. This single principle will save you from many a midnight debugging sessions. This is the main insight: Good development practices aren’t just nice-to-haves—they’re the difference between AI that amplifies your capabilities versus your chaos. The research bears this out. 2 Teams using rigorous practices deploy 46 times more frequently and are 440 times faster from commit to deployment. This effect is even more pronounced when you add capable AI assistants into the mix. Why This Post Exists: From Meme to Method Let me take you back to when this all started. 3 Andrej Karpathy 4 tweeted about “vibe-coding”—this idea of letting AI write your code while you just vibe. The developer community had a good laugh. It sounded like the ultimate developer fantasy: kick back, sip coffee, let the machines do the work. The birth of “vibe coding” Then Anthropic released Sonnet 3.7 and Claude Code , and something unexpected happened. The joke stopped being funny because it started being… possible? Of course, our trusty friend Cursor had been around awhile but this new interface finally felt like true vibe coding . At Julep , we build AI workflow orchestration. Our backend has years of accumulated decisions, patterns, and occasional technical debt. We have taken the utmost care to keep code quality high, and ample documentation for ourselves. However, the sheer size, and historical context of why different parts of the code are organized the way they are takes weeks for a good engineer to grok. Without proper guardrails when using Claude, you’re basically playing whack-a-mole with an overeager intern. Understanding Vibe-Coding ‘pls fix’ 5 Steve Yegge brilliantly coined the term CHOP —Chat-Oriented Programming in a slightly-dramatic-titled post “The death of the junior developer” . It’s a perfect, and no-bs description of what it’s like to code with Claude. Think of traditional coding like sculpting marble. You start with a blank block and carefully chisel away, line by line, function by function. Every stroke is deliberate, every decision yours. It’s satisfying but slow. Vibe-coding is more like conducting an orchestra. You’re not playing every instrument—you’re directing, shaping, guiding. The AI provides the raw musical talent, but without your vision, it’s just noise. There are three distinct postures you can take when vibe-coding, each suited to different phases in the development cycle: AI as First-Drafter : Here, AI generates initial implementations while you focus on architecture and design. It’s like having a junior developer who can type at the speed of thought but needs constant guidance. Perfect for boilerplate, CRUD operations, and standard patterns. AI as Pair-Programmer : This is the sweet spot for most development. You’re actively collaborating, bouncing ideas back and forth. The AI suggests approaches, you refine them. You sketch the outline, AI fills in details. It’s like pair programming with someone who has read every programming book ever written but has never actually shipped code. AI as Validator : Sometimes you write code and want a sanity check. AI reviews for bugs, suggests improvements, spots patterns you might have missed. Think of it as an incredibly well-read code reviewer who never gets tired or cranky. Instead of crafting every line, you’re reviewing, refining, directing. But—and this cannot be overstated—you remain the architect. Claude is your intern with encyclopedic knowledge but zero context about your specific system, your users, your business logic. The Three Modes of Vibe-Coding: A Practical Framework After months of experimentation and more than a few production incidents, I’ve settled on three distinct modes of operation. Each has its own rhythm, its own guardrails, and its own use cases. Mode 1: The Playground Lighter Fluid When to use it : Weekend hacks, personal scripts, proof-...",
    "summary": "37-minute read",
    "url": "https://diwank.space/field-notes-from-shipping-real-code-with-claude?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 793
  },
  {
    "title": "MCP vs API",
    "content": "The HTTP API Problem MCP: A Wire Protocol, Not Documentation Why Not Just Use OpenAPI? Five Fundamental Differences Runtime Discovery vs Static Specs Deterministic Execution vs LLM-Generated Calls Bidirectional Communication Single-Request Human Tasks Local-First by Design The Training Advantage They're Layers, Not Competitors Real-World Example The Bottom Line Bonus: MCP vs API video Bonus: Existing Reddit discussions Every week a new thread emerges on Reddit asking about the difference between MCP and API. I've tried summarizing everything that's been said about MCP vs API in a single post (and a single table). Aspect Traditional APIs (REST/GraphQL) Model Context Protocol (MCP) What it is Interface styles (REST, GraphQL) with optional spec formats (OpenAPI, GraphQL SDL) Standardized protocol with enforced message structure Designed for Human developers writing code AI agents making decisions Data location REST: Path, headers, query params, body (multiple formats) Single JSON input/output per tool Discovery Static docs, regenerate SDKs for changes 1 2 Runtime introspection ( tools/list ) Execution LLM generates HTTP requests (error-prone) LLM picks tool, deterministic code runs Direction Typically client-initiated; server-push exists but not standardized Bidirectional as first-class feature Local access Requires port, auth, CORS setup Native stdio support for desktop tools Training target Impractical at scale due to heterogeneity Single protocol enables model fine-tuning I am making several broad generalizations to keep the article length reasonable. I will continue to update this article with feedback from the community. If you have any suggestions, please email me at frank@glama.ai . The HTTP API Problem HTTP APIs suffer from combinatorial chaos. To send data to an endpoint, you might encode it in: URL path ( /users/123 ) Request headers ( X-User-Id: 123 ) Query parameters ( ?userId=123 ) Request body (JSON, XML, form-encoded, CSV) OpenAPI/Swagger documents these variations, but as a specification format, it describes existing patterns rather than enforcing consistency. Building automated tools to reliably use arbitrary APIs remains hard because HTTP wasn't designed for this—it was the only cross-platform, firewall-friendly transport universally available from browsers. MCP: A Wire Protocol, Not Documentation Model Context Protocol (MCP) isn't another API standard—it's a wire protocol that enforces consistency. While OpenAPI documents existing interfaces with their variations, MCP mandates specific patterns: JSON-RPC 2.0 transport, single input schema per tool, deterministic execution. Key architecture: Transport : stdio (local) or streamable HTTP Discovery : tools/list , resources/list expose capabilities at runtime Primitives : Tools (actions), Resources (read-only data), Prompts (templates) There is more than the above. Refer to the MCP specification for complete overview. Why Not Just Use OpenAPI? The most common question: \"Why not extend OpenAPI with AI-specific features?\" Three reasons: OpenAPI describes; MCP prescribes . You can't fix inconsistency by documenting it better—you need enforcement at the protocol level. Retrofitting fails at scale . OpenAPI would need to standardize transport, mandate single-location inputs, require specific schemas, add bidirectional primitives—essentially becoming a different protocol. The ecosystem problem . Even if OpenAPI added these features tomorrow, millions of existing APIs wouldn't adopt them. MCP starts fresh with AI-first principles. Five Fundamental Differences 1. Runtime Discovery vs Static Specs API : Ship new client code when endpoints change MCP : Agents query capabilities dynamically and adapt automatically // MCP discovery - works with any server client.request('tools/list') // Returns all available tools with schemas 2. Deterministic Execution vs LLM-Generated Calls API : LLM writes the HTTP request → hallucinated paths, wrong parameters MCP : LLM picks which tool → wrapped code executes deterministically This distinction is critical for production safety. With MCP, you can test, sanitize inputs, and handle errors in actual code, not hope the LLM formats requests correctly. 3. Bidirectional Communication API : Server-push exists (WebSockets, SSE, GraphQL subscriptions) but lacks standardization MCP : Bidirectional communication as first-class feature: Request LLM completions from server Ask users for input ( elicitation ) Push progress notifications 4. Single-Request Human Tasks REST APIs fragment human tasks across endpoints. Creating a calendar event might require: POST /events (create) GET /conflicts (check) POST /invitations (notify) MCP tools map to complete workflows. One tool, one human task. 5. Local-First by Design API : Requires HTTP server (port binding, CORS, auth headers) MCP : Can run as local process via stdio—no network layer needed Why this matters: When MCP servers run locally via stdio, they inherit the host process's permissions...",
    "summary": "6-minute read",
    "url": "https://glama.ai/blog/2025-06-06-mcp-vs-api?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 712
  },
  {
    "title": "Welcome to Campus. Here's Your ChatGPT",
    "content": "Article content from https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
    "summary": "8-minute read",
    "url": "https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 1600
  },
  {
    "title": "The hidden time bomb in the tax code that's fueling mass tech layoffs",
    "content": "Search Free Newsletters Editions Español Deutsch Français About We may earn a commission from links on this page By Catherine Baab Published Wednesday 5:00AM We may earn a commission from links on this page . Illustration : Getty ( Getty Images ) For the past two years, it’s been a ghost in the machine of American tech. Between 2022 and today, a little-noticed tweak to the U.S. tax code has quietly rewired the financial logic of how American companies invest in research and development. Outside of CFO and accounting circles , almost no one knew it existed. “I work on these tax write-offs and still hadn’t heard about this,” a chief operating officer at a private-equity-backed tech company told Quartz. “It’s just been so weirdly silent.” Advertisement Still, the delayed change to a decades-old tax provision — buried deep in the 2017 tax law — has contributed to the loss of hundreds of thousands of high-paying, white-collar jobs. That’s the picture that emerges from a review of corporate filings, public financial data, analysis of timelines, and interviews with industry insiders. One accountant, working in-house at a tech company, described it as a “niche issue with broad impact,” echoing sentiments from venture capital investors also interviewed for this article. Some spoke on condition of anonymity to discuss sensitive political matters. Since the start of 2023, more than half-a-million tech workers have been laid off, according to industry tallies . Headlines have blamed over-hiring during the pandemic and, more recently, AI . But beneath the surface was a hidden accelerant: a change to what’s known as Section 174 that helped gut in-house software and product development teams everywhere from tech giants such as Microsoft ( MSFT ) and Meta ( META ) to much smaller, private, direct-to-consumer and other internet-first companies. Now, as a bipartisan effort to repeal the Section 174 change moves through Congress, bigger questions are surfacing: How did a single line in the tax code help trigger a tsunami of mass layoffs? And why did no one see it coming? A tax break that built the modern tech economy For almost 70 years, American companies could deduct 100% of qualified research and development spending in the year they incurred the costs. Salaries, software, contractor payments — if it contributed to creating or improving a product, it came off the top of a firm’s taxable income. Advertisement The deduction was guaranteed by Section 174 of the IRS Code of 1954, and under the provision, R&D flourished in the U.S. Microsoft was founded in 1975 . Apple ( AAPL ) launched its first computer in 1976. Google ( GOOGL ) incorporated in 1998. Facebook opened to the general public in 2006. All these companies, now among the most valuable in the world, developed their earliest products — programming tools, hardware, search engines — under a tax system that rewarded building now, not later. The subsequent rise of smartphones, cloud computing, and mobile apps also happened in an America where companies could immediately write off their investments in engineering, infrastructure, and experimentation. It was a baseline assumption — innovation and risk-taking subsidized by the tax code — that shaped how founders operated and how investors made decisions. In turn, tech companies largely built their products in the U.S. Advertisement Microsoft’s operating systems were coded in Washington state. Apple’s early hardware and software teams were in California. Google’s search engine was born at Stanford and scaled from Mountain View. Facebook’s entire social architecture was developed in Menlo Park. The deduction directly incentivized keeping R&D close to home, rewarding companies for investing in American workers, engineers, and infrastructure. That’s what makes the politics of Section 174 so revealing. For all the rhetoric about bringing jobs back and making things in America, the first Trump administration’s major tax bill arguably helped accomplish the opposite. Undercutting the incentive structure When Congress passed the Tax Cuts and Jobs Act (TCJA), the signature legislative achievement of President Donald Trump’s first term, it slashed the corporate tax rate from 35% to 21% — a massive revenue loss on paper for the federal government. To make the 2017 bill comply with Senate budget rules, lawmakers needed to offset the cost. So they added future tax hikes that wouldn’t kick in right away, wouldn’t provoke immediate backlash from businesses, and could, in theory, be quietly repealed later. Advertisement The delayed change to Section 174 — from immediate expensing of R&D to mandatory amortization, meaning that companies must spread the deduction out in smaller chunks over five or even 15-year periods — was that kind of provision. It didn’t start affecting the budget until 2022, but it helped the TCJA appear “deficit neutral” over the 10-year window used for legislative scoring. The delay wasn’t a technical necessity. It was a...",
    "summary": "14-minute read",
    "url": "https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 798
  },
  {
    "title": "SaaS Is Just Vendor Lock-In with Better Branding",
    "content": "Docs \\ Personal Software \\ Blog \\ Discord \\ Loading... SaaS Is Just Vendor Lock-In with Better Branding Developers are told \"to focus on the product\" and let SaaS vendors handle the rest, but integrating third-party services, whether it's auth, queuing, file storage, or image optimization, comes at a cost. Not just in dollars but in time, friction, and mental overhead. There are five hidden taxes you pay every time you integrate a SaaS into your stack. 1. The Discovery Tax Before you can integrate anything, you first have to figure out what they're actually selling? What problems are they solving? Is it compatible with your stack? Is their price sane at your scale? a Are their docs clear and do they reveal any implementation weirdness? This unpaid research work is usually non-transferable. What you learn about \"Uploady\" or \"MegaQueue\" doesn't help you next time when you're evaluating something else. It's also subjective. It's marketing, and does the marketing message resonate with you? 2. The Sign-Up Tax You've decided on a service, and this is the moment when you hand over your email and credit card. Do they support usage-based pricing or only lock-in tiers? Can your team members access the dashboard, or do you have to pay more for that functionality? Despite only using the service the same amount! Can you even test the product without hitting a paywall? You're now on the hook, even if you haven't written a single line of code. 3. The Integration Tax Now the real work begins. You read the docs. You install the libraries You wire it into your framework. And figure out the edge cases that the docs don't mention, because docs are marketing! Often you're left fighting your own tooling. They're aiming for the lowest common denominator, and you're bleeding edge. Or the other way around! 4. The Local Development Tax You need the SaaS service to work locally. Does it even offer a local emulator? Can you stub it out in tests? Do you need to tunnel to the cloud just to test one feature? Now you've got branching configuration logic, one for production, one for staging, one for local… If you're lucky. 5. The Production Tax This is the part where you're \"done,\" except you're not. Can you use this in your staging environment? What about pull request previews? You need to securely manage the API keys. Monitoring, logging, and alerting Wondering why something worked in your laptop but fails in production? You've integrated the service, but now you're on the hook for its reliability in production. Conclusion The pitch of modern SaaS is \"don't reinvent the wheel.\" But every wheel you bolt on comes with some friction. It's not just a service: It's a contract. It's a dependency. It's a subtle architectural shift, and it comes with taxes. No matter what choice you make, it's always going to be vendor-locked in. Switching out something, even if it's open source and self-hosted, means that you're rewriting a lot of code. So, my argument is, don't make those decisions. Just pick a platform. The thing that matters is the software that you want to write, not the framework or the services that it runs on. Platforms like Cloudflare or Supabase shine. Where your database, queue, image service, and storage all live within the same platform and speak the same language. You avoid paying these taxes repeatedly. You simply pick the product that's already there. No context switching between vendors. No API key wrangling. No compatibility hacks or configuration forks. Just fast, local feeling integrations that work the same in dev and production. It feels like everything is running on the same machine, and in a way it kind of is. That's the hidden superpower of integrated platforms. They collapse the distance between your code and your services. And in doing so, they give you back the one thing no SaaS vendor can sell you: \"Flow.\" GitHub 793",
    "summary": "3-minute read",
    "url": "https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "dev",
    "word_count": 658
  },
  {
    "title": "Reverse Engineering Cursor's LLM Client",
    "content": "Reverse Engineering Cursor's LLM Client June 5, 2025 · Viraj Mehta, Aaron Hill, Gabriel Bianconi What happens under the hood at Cursor? We wired TensorZero between Cursor and the LLMs to see every token fly by… and bend those API calls to our own will. TensorZero is an open-source framework that helps engineers optimize LLM applications with downstream feedback signals (e.g. production metrics, human feedback, user behavior), and we figured it would be interesting to see whether we could use TensorZero on the LLM application we use most heavily ourselves: Cursor. With our gateway between Cursor and the LLM providers, we can observe the LLM calls being made, run evaluations on individual inferences, use inference-time optimizations, and even experiment with and optimize the prompts and models that Cursor uses. Cursor is optimized for its population of users as a whole — beyond .cursorrules , is there room for improvement by diving deeper and tailoring it to individual users? What would it look like to be able to empirically experiment with and optimize Cursor for your individual usage patterns? Beyond optimization, Cursor still operates as a black box. Wouldn’t it be interesting to see what’s actually being sent to the models? Nearly all LLM optimization, evaluation, and experimentation techniques require data on what inferences were made and their real-world consequences. In this post, we’ll focus on the former and dive into how we set up TensorZero as a self-hosted proxy between Cursor and the LLMs that it calls. If you’d like to try it yourself, check out the example in our repository. We’ll follow up with a blog post on how we collect feedback and close the optimization loop. Wiring Things Together The first thing we noticed was that Cursor lets you override the OpenAI base URL and model names. Perfect. TensorZero exposes an OpenAI-compatible inference endpoint, so we can easily configure Cursor to call TensorZero instead of OpenAI. In the TensorZero configuration, we define a TensorZero function cursorzero so that we can automatically experiment with different models and prompts while storing provider-agnostic inference and feedback data in our database for observability and optimization. The First Roadblock: Cursor’s Servers Cursor was initially unable to connect to TensorZero running locally. It turns out that Cursor first sends a request to its own servers, where additional processing happens before making the LLM call, so it couldn’t connect to our gateway on localhost . (This also means that your credentials must be forwarded to Cursor’s servers, allowing them to collect data on your inferences and codebase.) As a test that our plan could work in the first place, we pointed Cursor to OpenRouter and realized we could use its models for the Ask, Agent, and Cmd+K interactions in Cursor. We were also still able to use the normal Cursor Tab completions, which rely on a proprietary model Cursor serves for inline completions (it is very good, so we’re glad to be able to keep it). The solution was to set up a reverse proxy to expose a public endpoint that would forward requests back to our machine. We used Ngrok to keep things simple. Since we were exposing a gateway with LLM credentials to the public Internet, we added Nginx in front of our gateway to authenticate requests. We set Cursor’s base URL to our Ngrok endpoint, added the credentials we set up in Nginx, turned off built-in models, and finally added our new TensorZero function under the model name tensorzero::function_name::cursorzero . In the end, the workflow looked like this: Cursor → Ngrok → Nginx (self-hosted) → TensorZero (self-hosted) → LLM providers But it didn’t work. The Second Roadblock: CORS The authentication process had failed. Nginx logs showed that there was an OPTIONS request hitting our endpoint, so we configured Nginx to return headers on OPTIONS requests and incrementally added headers we saw in responses from the OpenAI API. This is the initial verification request that comes from the local Cursor IDE. The CORS requirement likely comes from Electron. After the initial verification, all requests come from Cursor’s servers. Our Nginx Configuration to handle CORS headers # --- CORS helper macro --- set $CORS_ALLOW_ORIGIN $http_origin; # reflect the caller's origin set $CORS_ALLOW_HEADERS \"Authorization,Content-Type\"; location / { # --- pre-flight --- if ($request_method = OPTIONS) { add_header Access-Control-Allow-Origin $CORS_ALLOW_ORIGIN always; add_header Access-Control-Allow-Credentials \"true\" always; add_header Access-Control-Allow-Methods \"GET,POST,OPTIONS\" always; add_header Access-Control-Allow-Headers $CORS_ALLOW_HEADERS always; add_header Access-Control-Max-Age 86400 always; return 204; # 204 (No Content) is conventional for pre-flight } } You can find the entire codebase for “CursorZero” on GitHub . It finally worked! Finally: Observability for Cursor We could finally see everything coming in and out of Cursor — includ...",
    "summary": "17-minute read",
    "url": "https://www.tensorzero.com/blog/reverse-engineering-cursors-llm-client/?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 774
  },
  {
    "title": "Meta reportedly in talks to invest billions of dollars in Scale AI",
    "content": "Meta is discussing a multi-billion dollar investment in Scale AI, according to Bloomberg . In fact, the deal value could reportedly exceed $10 billion, making it the largest external AI investment by Facebook’s parent company and one of the largest funding events ever for a private company. Scale AI (whose CEO Alexandr Wang is pictured above) provides data labeling services to companies such as Microsoft and OpenAI to help them train their AI models. Much of that labeling work is done by contractors — in fact, the Department of Labor recently dropped its investigation into whether the company was misclassifying and underpaying employees. According to Bloomberg, the company saw $870 million in revenue last year and expects to bring in $2 billion this year. Meta was already an investor in Scale AI’s $1 billion Series F , which valued the company at $13.8 billion. Scale AI also built Defense Llama , a large language model designed for military use, on top of Meta’s Llama 3.",
    "summary": "1-minute read",
    "url": "https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 165
  },
  {
    "title": "Specification Engineering",
    "content": "Specification Engineering reading time: 2.90 mins published: 2025-03-18 updated: 2025-03-29 ... is a bet on better code gen and more complexity AI engineering wants to be declarative. Ultimately, the algorithms in language model attention heads which devs interleave with their Python and Typescript are fuzzy and inscrutable. Their natural abstractions are guarantees, predicates on co-images. We can’t know what they do, only what they will have done. This is fine when AI software is 99% Python logic and 1% AI. Most software accesses SQL databases, but that’s not a problem - the interaction is tightly scoped and managed. But what happens when the software is 99% AI and 1% Python, at least in terms of complexity/headspace? At that point, the software is poorly served by the imperative paradigm. There may be plenty of LoC, but as far as it’s owner is concerned, there’s nothing imperative about it. At that point, much of the actual logic of the program lives in … the heads of its developers. The prompts won’t speak for themselves - each maintainer will, as a byproduct of hours of whiteboarding and painful trial-and-error, have reams and reams of knowledge regarding how different AI components interrelate with one another that simply cannot be safely deduced from the code itself, possibly even by a superintelligence. Storing critical system information solely in human minds - and doing so more often as time goes on and AI becomes a bigger part of software - is not a good idea. The I/O bandwidth is low, the information degrades quickly, and collaboration scales poorly. Its a structural trend directly counter to the massive productivity gains the rest of software is seeing - and it’s holding AI software development back. Evals can sometimes help introduce some structure and legibility, but they’re too fragmented - the requirements your engineers care about are distributed across hundreds of test cases you will never read and likely struggle to version and update. As time goes on, teams and engineers will want AI systems like Synth to help them - and, to be most effective in controlling and intervening on the software, those systems will need a legible and durable source of truth. Finding the right abstractions will take time, but now is the time to start. Every abstraction is leaky, and so directly maintaining imperative Python in AI software will be a necessity for the foreseeable future. But, great engineering teams will use processes and tools to ensure that the system specification is syncronized and takes precedence. PRs and prompt updates can be compiled up into spec diffs, and rejected if they introduce breaking changes. Syncronization in the other direction is where the abstraction starts paying for itself. Add a requirement -> AI spools up 1k LoC and Synth stress-tests two new prompts and a sub-agent, with 5 new evals to boot. Evals just become a way to check guarantees and create impetus for the compiler to update prompts/code/LoRAs. Naturally, syncronization will sometimes go both ways. Adding a better model might require simpler code with fewer prompts, depending on how preferences are outlined, and so we might go models -> evals -> spec -> code -> evals -> spec. Suddenly equilibrium becomes a more apt description than compilation. But don’t let that scare you away. Declarative specs with guarantees aren’t new . They’ve been used as long as software’s been written. Engineering teams benefit greatly from clearly communicating system level guarantees, and maintained them even back when it took precious human-hours to do so. What is new is intelligence that helps us consistently and cheaply transpile between them and software in a git commit hook. Let’s use it.",
    "summary": "3-minute read",
    "url": "https://www.joshuapurtell.com/posts/spec_eng/?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 610
  },
  {
    "title": "Maintaining an Android app is a lot of work",
    "content": "Table of Contents Java vs Kotlin Google makes breaking changes to its libraries Media 3 Google Auth library Dropping support for older Android versions Upgrades for the sake of it The UI design guidelines for Android evolve unpredictably Google makes breaking changes to Android platform Crucial third-party libraries have been deprecated Picasso Glide OkHttp EventBus RateThisApp Two different versioning schemes for everything Forced upgrades Conclusion Update There was recent news about 47% decline in the number of apps on Google Play Store. As a hobby Android developer, who has been developing MusicSync , a Google Play Music + Podcast replacement for the last five years, I thought I would share my experience of maintaining an Android app. And why this reduction in the number of apps is not surprising to me. I have several side-projects that run on a backend server with a limited web UI, and it is much less effort to maintain them. However, maintaining an Android app as a side-project is a more involved affair. And here are some of the problems I have faced. Java vs Kotlin # Kotlin is clearly the preferred language of development if you are starting a new Android project in 2025. But what if you are maintaining a hobby project written in Java? You will start seeing incompatibility when your dependencies are re-written in Kotlin. If you depend on a library that uses Kotlin’s coroutines or relies on Kotlin’s suspend functions , then you will have to work around it, or rewrite your app in Kotlin as well! Jetpack Compose, an official Google UI library for Android is entirely unusable from Java. I would imagine that if you started with Kotlin first then a big chunk of StackOverflow questions written for Java audiences require you translate them to corresponding Kotlin code as well To their credit, Android documentation still gives code samples in both Java and Kotlin. Google makes breaking changes to its libraries # Google has a habit of making breaking changes to its Android libraries. Here’s a list of some of the libraries that I have used in my app and the issues I have faced. Media 3 # Android ships with MediaPlayer . Google recommends its open-source library ExoPlayer . ExoPlayer V1 was last released in 2017. It was replaced with backward-incompatible ExoPlayer V2 which was last released in July 2024. And now, it has now been replaced with backward-incompatible media3 . The Google provided migration script is far from being complete. Further, media3 does not follow semantic versioning, minor version upgrades has resulted in breaking API changes. Google Auth library # Google’s own Auth library had a bug and sign-in was broken for API 26 and lower for months . Java 1 2 3 4 5 6 7 8 9 10 java.lang.NoSuchMethodError: No virtual method getAndSetObject (Ljava / lang / Object;JLjava / lang / Object;)Ljava / lang / Object; in class Lsun / misc / Unsafe; or its super classes (declaration of 'sun.misc.Unsafe' appears in / system / framework / core - libart.jar) E at com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper.gasWaiters(AbstractFuture.java:1394) E at com.google.common.util.concurrent.AbstractFuture.releaseWaiters(AbstractFuture.java:1110) E at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1000) E at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:783) E at com.google.auth.oauth2.OAuth2Credentials$RefreshTask.access$400(OAuth2Credentials.java:600) E at com.google.auth.oauth2.OAuth2Credentials$RefreshTask$1.onSuccess(OAuth2Credentials.java:617) ... Dropping support for older Android versions # Google Ads library v24 dropped support for Android API 21. According to official Google statistics, API 21 is used by 0.1% (~4 million) users. The rationale behind this has been left unexplained. Upgrades for the sake of it # Material 2 was deprecated for Material 3. No clear migration guide was provided. I tried to upgrade and some components like Sliders won’t look good. Why? I don’t know, and I was never able to figure out the mystic. It does not help that most documentation now refers to Jetpack Compose which I cannot use! So, for the near term, Java-based codebase are likely stuck with Material 2. The UI design guidelines for Android evolve unpredictably # Bottom bar, a featured popular on iOS was discouraged and then became a standard feature in Material design. Back and up buttons used to behave differently and now they are supposed to behave the same . I only learnt about it last year when I posted about it on Reddit. You might think that you can just use Material Design components and be done with it. But migrating from one version of Material Design to another is not trivial either. And before you migrate from Material 1 to Material 2, Google deprecates it for Material 3. Google makes breaking changes to Android platform # Every major release of Android makes breaking changes that requires developer effort Toasts use to work for quick notific...",
    "summary": "7-minute read",
    "url": "https://ashishb.net/programming/maintaining-android-app/?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "ai",
    "word_count": 771
  },
  {
    "title": "magic namerefs",
    "content": "namerefs (introduced in bash 4.0) act as aliases for other variables var=meow declare -n ref=var echo $ref # prints meow ref=moo echo $var # prints moo they can also reference a specific element in an array, using declare -n ref='array[1234]' using this, i have been playing with a neat nameref trick: tmp=() declare -n var= ' tmp[tmp[0]=some_expression_here, 0] ' it uses the auxiliary array tmp to force an arithmetic context, in which it assigns the result of any arbitrary expression to an element of that array, then expands that same element. we can now create magic variables that evaluate any arbitrary expression. here's a basic counter: tmp=() x=0 declare -n counter= ' tmp[tmp[0]=x++,0] ' for i in {1..10} ; do echo $counter done # prints 0 1 2 3 4 5 6 7 8 9 here's an example that computes the fibonacci numbers: f=(0 1) declare -n fib= ' f[f[2]=f[0], f[0]+=f[1], f[1]=f[2], 0] ' for i in {1..10} ; do echo $fib done # prints 1 1 2 3 5 8 13 21 34 55 this is already very powerful, as it can do many magic things with numbers. but as it turns out, we can do even more: we can use dollar expansions too! here's a silly clock with magic variables that show the current date and time: # \\D{your-format-here} passes that format to strftime # but it only works in prompts like $PS1 # ${var@P} expands $var as if it was in your $PS1 date=( ' \\D{% ' {Y,m,d,H,M,S}}) # the formats we'll use months=(- jan feb mar apr may jun jul aug sep oct nov dec) numbers=({00..60}) tmp=() declare -n year= ' tmp[tmp=${date[0]@P},0] ' declare -n month= ' months[10#${date[1]@P}] ' declare -n day= ' numbers[10#${date[2]@P}] ' declare -n hour= ' numbers[10#${date[3]@P}] ' declare -n minute= ' numbers[10#${date[4]@P}] ' declare -n second= ' numbers[10#${date[5]@P}] ' while : ; do echo $year / $month / $day $hour : $minute : $second sleep 1 done # 2025/jun/06 09:54:13 # 2025/jun/06 09:54:14 # 2025/jun/06 09:54:15 # 2025/jun/06 09:54:16 this is probably one of the coolest things i've ever seen in bash. honestly i am a bit horrified that this works at all, but the resulting code is just so simple and elegant. and it feels like i'm just scratching the surface, there's so much potential. previous: recursive expansions",
    "summary": "2-minute read",
    "url": "https://gist.github.com/izabera/e4717562e20eb6cfb6e05f8019883efb?utm_source=tldrnewsletter",
    "published_date": "2025-06-09T00:00:00",
    "category": "dev",
    "word_count": 387
  }
]