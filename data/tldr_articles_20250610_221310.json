{
  "metadata": {
    "extraction_timestamp": "2025-06-10T22:13:10.550154",
    "scraper_version": "1.0",
    "extraction_statistics": {
      "total_articles": 12,
      "success_count": 6,
      "partial_count": 1,
      "failed_count": 5,
      "success_rate": 50.0,
      "partial_rate": 8.3,
      "failure_rate": 41.7,
      "failed_articles": [
        {
          "title": "Apple WWDC 2025: the 13 biggest announcements",
          "url": "https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "One Man Armies",
          "url": "https://quarter--mile.com/One-Man-Armies?utm_source=tldrnewsletter",
          "reason": "Content extraction failed - no content found using site-specific or generic strategies"
        },
        {
          "title": "AI's metrics question",
          "url": "https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
          "reason": "Content extraction failed - no content found using site-specific or generic strategies"
        },
        {
          "title": "Silicon Valley's quest to remove friction from our lives",
          "url": "https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "A bit more on Twitter/X's new encrypted messaging",
          "url": "https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        }
      ],
      "partial_articles": [
        {
          "title": "The SignalFire State of Talent Report - 2025",
          "url": "https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 1346/3400 words (39.6%)",
          "word_count": 1346
        }
      ]
    }
  },
  "articles": [
    {
      "title": "Apple WWDC 2025: the 13 biggest announcements",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "OpenAI hits $10 billion in annual recurring revenue fueled by ChatGPT growth",
      "content": "Key Points OpenAI has hit $10 billion in annual recurring revenue, according to a company spokesperson. The milestone comes roughly two and a half years after the company launched its popular ChatGPT chatbot. The figure includes revenue from the company's consumer products, ChatGPT business products and its API, an OpenAI spokesperson says. OpenAI CEO Sam Altman appears on screen during a talk with Microsoft Chair and CEO Satya Nadella at the Microsoft Build 2025 conference in Seattle on May 19, 2025. Jason Redmond | AFP | Getty Images OpenAI has hit $10 billion in annual recurring revenue, or ARR, less than three years after launching its popular ChatGPT chatbot. The figure includes sales from the company's consumer products; ChatGPT business products; and its application programming interface, or API. It excludes licensing revenue from Microsoft and large one-time deals, according to an OpenAI spokesperson. For all of last year, OpenAI had around $5.5 billion in ARR. Reaching its meteoric growth rates requires a substantial amount of cash. The San Francisco-based startup lost about $5 billion last year. OpenAI is also targeting $125 billion in revenue by 2029, according to a person familiar with the matter who asked not to be named because the details are confidential. The Information first reported on OpenAI's revenue ambitions. The new revenue metrics give some context to OpenAI's monster valuation. OpenAI closed a $40 billion funding round in March, marking the largest private tech deal on record. At today's metrics, OpenAI is valued at about 30 times revenue, which highlights the hyper growth expectations by some of its largest investors. OpenAI is backed by Japan's SoftBank , Microsoft , Coatue, Altimeter, Thrive and others. OpenAI burst onto the scene with the release of the consumer version of ChatGPT in late 2022, and began launching business products the following year. As of late March, OpenAI said it supports 500 million weekly active users. The company announced earlier this month that it has three million paying business users, up from the two million it reported in February. WATCH: OpenAI paying business users of ChatGPT hits 3 million, up from 2 million in February watch now VIDEO 1:59 01:59 OpenAI paying business users of ChatGPT hits 3 million, up from 2 million in February The Exchange FBI says Palm Springs bombing suspects used AI chat program to help plan attack Annie Palmer Nvidia CEO Jensen Huang hammers chip controls that 'effectively closed' China market Kif Leswing Telegram CEO announces $300 million partnership with Elon Musk's xAI and Grok Ashley Capoot Read More",
      "summary": "2-minute read",
      "url": "https://www.cnbc.com/2025/06/09/openai-hits-10-billion-in-annualized-revenue-fueled-by-chatgpt-growth.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 423,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Doctors Could Hack the Nervous System With Ultrasound",
      "content": "Biomedical Feature Doctors Could Hack the Nervous System With Ultrasound A new stimulation technique targets inflammation and diabetes 10 min read Vertical Shonagh Rae DarkBlue2 Inflammation: It’s the body’s natural response to injury and infection, but medical science now recognizes it as a double-edged sword. When inflammation becomes chronic, it can contribute to a host of serious health problems, including arthritis , heart disease , and certain cancers. As this understanding has grown, so too has the search for effective ways to manage harmful inflammation. Doctors and researchers are exploring various approaches to tackle this pervasive health issue, from new medications to dietary interventions. But what if one of the most promising treatments relies on a familiar technology that’s been in hospitals for decades? Enter focused ultrasound stimulation (FUS), a technique that uses sound waves to reduce inflammation in targeted areas of the body. It’s a surprising new application for ultrasound technology, which most people associate with prenatal checkups or diagnostic imaging. And FUS may help with many other disorders too, including diabetes and obesity . By modifying existing ultrasound technology, we might be able to offer a novel approach to some of today’s most pressing health challenges. Our team of biomedical researchers at the Institute of Bioelectronic Medicine (part of the Feinstein Institutes for Medical Research), in Manhasset, N.Y., has made great strides in learning the electric language of the nervous system. Rather than treating disease with drugs that can have broad side effects throughout the body, we’re learning how to stimulate nerve cells, called neurons , to intervene in a more targeted way. Our goal is to activate or inhibit specific functions within organs. The relatively new application of FUS for neuromodulation , in which we hypothesize that sound waves activate neurons, may offer a precise and safe way to provide healing treatments for a wide range of both acute and chronic maladies. The treatment doesn’t require surgery and potentially could be used at home with a wearable device. People are accustomed to being prescribing pills for these ailments, but we imagine that one day, the prescriptions could be more like this: “Strap on your ultrasound belt once per day to receive your dose of stimulation.” How Ultrasound Stimulation Works Ultrasound is a time-honored medical technology. Researchers began experimenting with ultrasound imaging in the 1940s, bouncing low-energy ultrasonic waves off internal organs to construct medical images, typically using intensities of a few hundred milliwatts per square centimeter of tissue. By the late 1950s, some doctors were using the technique to show expectant parents the developing fetus inside the mother’s uterus . And high-intensity ultrasound waves, which can be millions of milliwatts per square centimeter, have a variety of therapeutic uses, including destroying tumors . The use of low-intensity ultrasound (with intensities similar to that of imaging applications) to alter the activity of the nervous system, however, is relatively unexplored territory. To understand how it works, it’s helpful to compare FUS to the most common form of neuromodulation today, which uses electric current to alter the activity of neurons to treat conditions like Parkinson’s disease . In that technique, electric current increases the voltage inside a neuron, causing it to “fire” and release a neurotransmitter that’s received by connected neurons, which triggers those neurons to fire in turn. For example, the deep brain stimulation used to treat Parkinson’s activates certain neurons to restore healthy patterns of brain activity. How It Works In focused ultrasound stimulation, we theorize that sound waves’ vibrations (1) cause channels (2) on the neuron’s cell membrane to open. Positive ions flow in, causing a voltage change within the neuron that triggers it to fire with an action potential (3) that travels down the axon. The axon’s tips (4) release neurotransmitters that cause connected neurons to fire in turn. Chris Philpot In FUS, by contrast, the sound waves’ vibrations interact with the membrane of the neuron, opening channels that allow ions to flow into the cell, thus indirectly changing the cell’s voltage and causing it to fire. One promising use is transcranial ultrasound stimulation , which is being tested extensively as a noninvasive way to stimulate the brain and treat neurological and psychiatric diseases. We’re interested in FUS’s effect on the peripheral nerves—that is, the nerves outside the brain and spinal cord. We think that activating specific nerves in the abdomen that regulate inflammation or metabolism may help address the root causes of related diseases, rather than just treating the symptoms. FUS for Inflammation Inflammation is something that we know a lot about. Back in 2002, Kevin Tracey , currently the president and CEO of the Feinstein Institutes, upset the conventional wisdom that the nervous system and the immune system operate independently and serve distinct roles. He discovered the body’s inflammatory reflex : a two-way neural circuit that sends signals between the brain and body via the vagus nerve and the nerves of the spleen. These nerves control the release of cytokines , which are proteins released by immune cells to trigger inflammation. Tracey and colleagues found that stimulating nerves in this neural circuit suppressed the inflammatory response. The discoveries led to the first clinical trials of electrical neuromodulation devices to treat chronic inflammation and launched the field of bioelectronic medicine. Hacking the Immune System When focused ultrasound (1) is applied to the spleen (2), it activates neurons (3), causing them to release certain neurotransmitters that interact with immune cells called T-cells (4) and macrophages (5). Those interactions release another neurotransmitter that binds to receptors on the macrophages, inhibiting them from producing and releasing cytokines, thus putting the brakes on the inflammatory response. Chris Philpot Tracey has been a pioneer in treating inflammation with vagus nerve stimulation (VNS), in which electrical stimulation of the vagus nerve activates neurons in the spleen. In animals and humans, VNS has been shown to reduce harmful inflammation in both chronic diseases such as arthritis and acute conditions such as sepsis. But direct VNS requires surgery to place an implant in the body, which makes it risky for the patient and expensive. That’s why we’ve pursued noninvasive ultrasound stimulation of the spleen. Working with Tracey, collaborators at GE Research , and others, we first experimented with rodents to show that ultrasound stimulation of the spleen affects an anti-inflammatory pathway , just as VNS does, and reduces cytokine production as much as a VNS implant does. We then conducted the first-in-human trial of FUS for controlling inflammation . We initially enrolled 60 healthy people, none of whom had signs of chronic inflammation. To test the effect of a 3-minute ultrasound treatment, we were measuring the amount of a molecule called tumor necrosis factor (TNF), which is a biomarker of inflammation that’s released when white blood cells go into action against a perceived pathogen. At the beginning of the study, 40 people received focused ultrasound stimulation, while 20 others, serving as the control group, simply had their spleens imaged by ultrasound. Yet, when we looked at the early data, everyone had lower levels of TNF, even the control group. It seemed that even imaging with ultrasound for a few minutes had a moderate anti-inflammatory effect! To get a proper control group, we had to recruit 10 more people for the study and devise a different sham experiment, this time unplugging the ultrasound machine. Shonagh Rae After the subjects received either the real or sham stimulation, we took blood samples from all of them. We next simulated an infection by adding a bacterial toxin to the blood in the test tubes, then measured the amount of TNF released by the white blood cells to fight the toxin. The results, which we published in the journal Brain Stimulation in 2023, showed that people who had received FUS treatments had lower levels of TNF than the true control group. We saw no problematic side effects of the ultrasound: The treatment didn’t adversely affect heart rate , blood pressure, or the many other biomarkers that we checked. The results also showed that when we repeated the blood draw and experiment 24 hours later, the treatment groups’ TNF levels had returned to baseline. This finding suggests that if FUS becomes a treatment option for inflammatory diseases, people might require regular, perhaps even daily, treatments. One surprising result was that it didn’t seem to matter which location within the spleen we targeted—all the locations we tried produced similar results. Our hypothesis is that hitting any target within the spleen activates enough nerves to produce the beneficial effect. What’s more, it didn’t matter which energy intensity we used. We tried intensities ranging from about 10 to 200 mW per cm 2 , well within the range of intensities used in ultrasound imaging; remarkably, even the lowest intensity level caused subjects’ TNF levels to drop. Our big takeaway from that first-in-human study was that targeting the spleen with FUS is not just a feasible treatment but could be a gamechanger for inflammatory diseases. Our next steps are to investigate the mechanisms by which FUS affects the inflammatory response, and to conduct more animal and human studies to see whether prolonged administration of FUS to the spleen can treat chronic inflammatory diseases. FUS for Obesity and Diabetes For much of our research on FUS, we’ve partnered with GE Research, whose parent company is one of the world’s leading makers of ultrasound equipment. One of our first projects together explored the potential of FUS as a treatment for the widespread inflammation that often accompanies obesity, a condition that now affects about 890 million people around the world. In this study, we fed lab mice a high-calorie and high-fat “Western diet” for eight weeks. During the following eight weeks, half of them received ultrasound stimulation while the other half received daily sham stimulation. We found that the mice that received FUS had lower levels of cytokines —and to our surprise, those mice also ate less and lost weight. In related work with our GE colleagues, we examined the potential of FUS as a treatment for diabetes , which now affects 830 million people around the world. In a healthy human body, the liver stores glucose as a reserve and releases it only when it registers that glucose levels in the bloodstream have dropped. But in people with diabetes, this sensing system is dysfunctional, and the liver releases glucose even when blood levels are already high, causing a host of health problems. Hacking the Metabolic System When focused ultrasound (1) is applied to an area in the liver (2) called the porta hepatis, it activates glucose-sensing neurons. Those neurons send signals up the vagus nerve (3) to the brain, where a region called the hypothalamus (4) commands the body to reduce glucose production and increase glucose uptake. Chris Philpot For diabetes, our ultrasound target was the network of nerves that transmit signals between the liver and the brain: specifically, glucose-sensing neurons in the porta hepatis , which is essentially the gateway to the liver. We gave diabetic rats 3-minute daily ultrasound stimulation over a period of 40 days. Within just a few days, the treatment brought down the rats’ glucose levels from dangerously high to normal range. We got similar results in mice and pigs, and published these exciting results in 2022 in Nature Biomedical Engineering . Those diabetes experiments shed some light on why ultrasound had this effect. We decided to zero in on a brain region called the hypothalamus , which controls many crucial automatic body functions, including metabolism, circadian rhythms, and body temperature. Our colleagues at GE Research started investigating by blocking the nerve signals that travel from the liver to the hypothalamus in two different ways—both cutting the nerves physically and using a local anesthetic. When we then applied FUS, we didn’t see the beneficial decrease in glucose levels. This result suggests that the ultrasound treatment works by changing glucose-sensing signals that travel from the liver to the brain—which in turn changes the commands the hypothalamus issues to the metabolic systems of the body, essentially telling them to lower glucose levels. The next steps in this research involve both technical development and clinical testing. Currently, administering FUS requires technical expertise, with a sonographer looking at ultrasound images, locating the target, and triggering the stimulation. But if FUS is to become a practical treatment for a chronic disease, we’ll need to make it usable by anyone and available as an at-home system. That could be a wearable device that uses ultrasound imaging to automatically locate the anatomical target and then delivers the FUS dose: All the patient would have to do is put on the device and turn it on. But before we get to that point, FUS treatment will have to be tested clinically in randomized controlled trials for people with obesity and diabetes. GE HealthCare recently partnered with Novo Nordisk to work on the clinical and product development of FUS in these areas. FUS for Cardiopulmonary Diseases FUS may also help with chronic cardiovascular diseases, many of which are associated with immune dysfunction and inflammation. We began with a disorder called pulmonary arterial hypertension , a rare but incurable disease in which blood pressure increases in the arteries within the lungs . At the start of our research, it wasn’t clear whether inflammation around the pulmonary arteries was a cause or a by-product of the disease, and whether targeting inflammation was a viable treatment. Our group was the first to try FUS of the spleen in order to reduce the inflammation associated with pulmonary hypertension in rats. The results, published last year, were very encouraging. We found that 12-minute FUS sessions reduced pulmonary pressure , improved heart function, and reduced lung inflammation in the animals in the experimental group (as compared to animals that received sham stimulation). What’s more, in the animals that received FUS, the progression of the disease slowed significantly even after the experiment ended, suggesting that this treatment could provide a lasting effect. One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation. This study was, to our knowledge, the first to successfully demonstrate an ultrasound-based therapy for any cardiopulmonary disease. And we’re eager to build on it. We’re next interested in studying whether FUS can help with congestive heart failure , a condition in which the heart can’t pump enough blood to meet the body’s needs. In the United States alone, more than 6 million people are living with heart failure, and that number could surpass 8 million by 2030. We know that inflammation plays a significant role in heart failure by damaging the heart’s muscle cells and reducing their elasticity. We plan to test FUS of the spleen in mice with the condition. If those tests are successful, we could move toward clinical testing in humans. The Future of Ultrasound Stimulation We have one huge advantage as we think about how to bring these results from the lab to the clinic: The basic hardware for ultrasound already exists, it’s already FDA approved, and it has a stellar safety record through decades of use. Our collaborators at GE have already experimented with modifying the typical ultrasound devices used for imaging so that they can be used for FUS treatments. Once we get to the point of optimizing FUS for clinical use, we’ll have to determine the best neuromodulation parameters. For instance, what are the right acoustic wavelengths and frequencies? Ultrasound imaging typically uses higher frequencies than FUS does, but human tissue absorbs more acoustic energy at higher frequencies than it does at lower frequencies. So to deliver a good dose of FUS, researchers are exploring a wide range of frequencies. We’ll also have to think about how long to transmit that ultrasound energy to make up a single pulse, what rate of pulses to use, and how long the treatment should be. In addition, we need to determine how long the beneficial effect of the treatment lasts. For some of the ailments that researchers are exploring, like FUS of the brain to treat chronic pain , a patient might be able to go to the doctor’s office once every three months for a dose. But for diseases associated with inflammation, a regular, several-times-per-week regimen might prove most effective, which would require at-home treatments. For home use to be possible, the wearable device would have to locate the targets automatically via ultrasound imaging. As vast databases already exist of human ultrasound images from the liver, spleen, and other organs, it seems feasible to train a machine-learning algorithm to detect targets automatically and in real time. One day, an AI system might be able to guide at-home users as they place a wearable device on their body and trigger the stimulation. A few startups are working on building such wearable devices , which could take the form of a belt or a vest. For example, the company SecondWave Systems , which has partnered with the University of Minnesota, in Minneapolis, has already conducted a small pilot study of its wearable device, trying it out on 13 people with rheumatoid arthritis and seeing positive outcomes. While it will be many years before FUS treatments are approved for clinical use, and likely still more years for wearable devices to be proven safe enough for home use, the path forward looks very promising. We believe that FUS and other forms of bioelectronic medicine offer a new paradigm for human health, one in which we reduce our reliance on pharmaceuticals and begin to speak directly to the body electric. From Your Site Articles DARPA Wants to Jolt the Nervous System with Electricity, Lasers, Sound Waves, and Magnets › Ultrasound-Powered Nerve Implant Works Deep in Body › New Neurotech Eschews Electricity for Ultrasound › Related Articles Around the Web Neuromodulation - Focused Ultrasound Foundation › ultrasound neuromodulation neural stimulation diabetes arthritis {\"imageShortcodeIds\":[]}",
      "summary": "12-minute read",
      "url": "https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 2984,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "XRobotics' countertop robots are cooking up 25,000 pizzas a month",
      "content": "XRobotics thinks it has cracked the code on getting pizza restaurants to adopt robotics. The San Francisco-based robotics company built a countertop robot called xPizza Cube, which is roughly the size of a stackable washing machine and uses machine learning to apply sauce, cheese, and pepperoni to pizza dough. The machines, which lease for $1,300 a month for three years, can make up to 100 pizzas an hour and be retrofitted to work with pies of different sizes and styles, like Detroit and Chicago deep dish. “This saves like almost 70, sometimes 80% of the time for the staff,” Denis Rodionov, the co-founder and CEO of XRobotics, told TechCrunch. “It is just repeatable work. If you have a pepperoni pizza, you need to place 50 slices of pepperoni one by one.” XRobotics is not the only company that has tried to introduce robotics into the restaurant industry — nor the only one focused on pizza. Zume is the most notable pizza robotics company — if that can be considered its own category. The company raised more than $420 million in venture capital for its robotic pizza trucks, before pivoting to focus on sustainable packaging in 2020 and shuttering entirely in 2023. Rodionov argues that they’ve been successful where other companies haven’t because they aren’t trying to fully transform the pizza-making process, as Zume was, but rather build technology to help existing pizza makers save on time and labor. Because they are building assistive technology, as opposed to replacement tech, Rodionov said they’ve been able to keep their device small enough to fit in existing kitchens and priced at a level that pizzerias — from mom-and-pop shops to large chains, both of which the company counts as customers — could afford. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The company found this out the hard way. XRobotics launched in 2019 and introduced the first version of the technology in 2021 . Its first robot was significantly larger and could work with more than 20 toppings, and ran into the same problems as their competitors. “We did a real pilot in the restaurant with our huge machine,” Rodionov said. “We learned a lot from that, and we figured out we needed a very small, compact solution. It was a bit scary. All the numbers, all the feelings, all the gut said you need to do this, not this . And we just followed the gut and said, ‘Yeah, we would go and make a smaller version,’ and it was tremendous success.” XRobotics launched their current model in 2023. The company declined to share how many customers it has. It said its robots are producing 25,000 pizzas per month, but how many customers that translates to is hard to calculate. The startup also recently raised a $2.5 million seed round led by FinSight Ventures with participation from SOSV, MANA Ventures, and Republic Capital. Rodionov said the company will use the capital to produce more units and install more robots for customers. XRobotics is committed to the pizza industry, at least for now, Rodionov said, considering the sheer size of the market — there are more than 73,000 pizza chains in the U.S. alone . The company plans to expand to Mexico and Canada next. “I love pizza, my co-founder too,” Rodionov said. “We have tested probably any pizza in San Francisco. Also, we test pizza in New York and Chicago.” Rodionov added that Detroit-style pizza, known for its square shape and crispy cheese crust, is his favorite.",
      "summary": "4-minute read",
      "url": "https://techcrunch.com/2025/06/09/xrobotics-countertop-robots-are-cooking-up-25000-pizzas-a-month/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 658,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple Announces Foundation Models Framework for Developers to Leverage AI",
      "content": "Apple Announces Foundation Models Framework for Developers to Leverage AI Monday June 9, 2025 10:10 am PDT by Tim Hardwick Apple at WWDC today announced Foundation Models Framework, a new API allowing third-party developers to leverage the large language models at the heart of Apple Intelligence and build it into their apps. With the Foundation Models Framework, developers can integrate Apple's on-device models directly into apps, allowing them to build on Apple Intelligence. \"Last year, we took the first steps on a journey to bring users intelligence that's helpful, relevant, easy to use, and right where users need it, all while protecting their privacy. Now, the models that power Apple Intelligence are becoming more capable and efficient, and we're integrating features in even more places across each of our operating systems,\" said Craig Federighi, Apple's senior vice president of Software Engineering. \"We're also taking the huge step of giving developers direct access to the on-device foundation model powering Apple Intelligence, allowing them to tap into intelligence that is powerful, fast, built with privacy, and available even when users are offline. We think this will ignite a whole new wave of intelligent experiences in the apps users rely on every day. We can't wait to see what developers create.\" The Foundation Models framework lets developers build AI-powered features that work offline, protect privacy, and incur no inference costs. For example, an education app can generate quizzes from user notes on-device, and an outdoors app can offer offline natural language search. Apple says the framework is available for testing starting today through the Apple Developer Program at developer.apple.com, and a public beta will be available through the Apple Beta Software Program next month at beta.apple.com. It includes built-in features like guided generation and tool calling for easy integration of generative capabilities into existing apps. Tag: WWDC 2025 [ 6 comments ]",
      "summary": "2-minute read",
      "url": "https://www.macrumors.com/2025/06/09/foundation-models-framework/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 309,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "LLMs are cheap",
      "content": "This post is making a point - generative AI is relatively cheap - that might seem so obvious it doesn't need making. I'm mostly writing it because I've repeatedly had the same discussion in the past six months where people claim the opposite. Not only is the misconception still around, but it's not even getting less frequent. This is mainly written to have a document I can point people at, the next time it repeats. It seems to be a common, if not a majority, belief that Large Language Models (in the colloquial sense of \"things that are like ChatGPT\") are very expensive to operate. This then leads to a ton of innumerate analyses about how AI companies must be obviously doomed, as well as a myopic view on how consumer AI businesses can/will be monetized. It's an understandable mistake, since inference was indeed very expensive at the start of the AI boom, and those costs were talked about a lot. But inference has gotten cheaper even faster than models have gotten better, and nobody has an intuition for something becoming 1000x cheaper in two years. It just doesn't happen. It doesn't help that the common pricing model (\"$ per million tokens\") is very hard to visualize. So let's compare LLMs to web search. I'm choosing search as the comparison since it's in the same vicinity and since it's something everyone uses and nobody pays for, not because I'm suggesting that ungrounded generative AI is a good substitute for search. (It should also go without saying that these are just my personal opinions.) What is the price of a web search? Here's the public API pricing for some companies operating their own web search infrastructure, retrieved on 2025-05-02: The Gemini API pricing lists a \"Grounding with Google Search\" feature at $35/1k queries. I believe that's the best number we can get for Google, they don't publish prices for a \"raw\" search result API. The Bing Search API is priced at $15/1k queries at the cheapest tier. Brave has a price of $5/1k searches at the cheapest tier. Though there's something very strange about their pricing structure, with the unit pricing increasing as the quota increases, which is the opposite of what you'd expect. The tier with real quota is priced at $9/1k searches. So there's a range of prices, but not a horribly wide one, and with the engines you'd expect to be of higher quality also having higher prices. What is the price of LLMs in a similar domain? To make a reasonable comparison between those search prices and LLM prices, we need two numbers: How many tokens are output per query? What's the price per token? I picked a few arbitrary queries from my search history, and phrased them as questions, and ran them on Gemini 2.5 Flash (thinking mode off) in AI Studio: [When was the term LLM first used?] -> 361 tokens, 2.5 seconds [What are the top javascript game engines?] -> 1145 tokens, 7.6 seconds [What are the typical carry-on bag size limits in europe?] -> 506 tokens, 3.4 seconds [List the 10 largest power outages in history] -> 583 tokens, 3.7 seconds Note that I'm not judging the quality of the answers here. The purpose is just to get rough numbers for how large typical responses are. A 500-1000 token range seems like a reasonable estimate. What's the price of a token? The pricing is sometimes different for input and output tokens. Input tokens tend to be cheaper, and our inputs are very short compared to the outputs, so for simplicity let's consider all the tokens to be outputs. Here's the pricing of some relevant models, retrieved on 2025-05-02: Model Price / 1M tokens Gemma 3 27B $0.20 ( source ) Qwen3 30B A3B $0.30 ( source ) Gemini 2.0 Flash $0.40 ( source ) GPT-4.1 nano $0.40 ( source ) Gemini 2.5 Flash Preview $0.60 ( source ) Deepseek V3 $1.10 ( source ) GPT-4.1 mini $1.60 ( source ) Deepseek R1 $2.19 ( source ) Claude 3.5 Haiku $4.00 ( source ) GPT-4.1 $8.00 ( source ) Gemini 2.5 Pro Preview $10.00 ( source ) Claude 3.7 Sonnet $15.00 ( source ) o3 $40.00 ( source ) If we assume the average query uses 1k tokens, these prices would be directly comparable to the prices per 1k search queries. That's convenient. The low end of that spectrum is at least an order of magnitude cheaper than even the cheapest search API, and even the models at the low end are pretty capable. The high end is about on par with the highest end of search pricing. To compare a midrange pair on quality, the Bing Search vs. a Gemini 2.5 Flash comparison shows the LLM being 1/25th the price. Note that many of the above models have cheaper pricing in exchange for more flexible scheduling (Anthropic, Google and OpenAI give a 50% discount for batch requests, Deepseek is 50%-75% cheaper during off-peak hours). I've not included those cheaper options in the table to keep things comparable, but the presence of those cheaper tiers is worth keeping in mind when thinking about the next section... Objection! I know some people are going to have objections to this back-of-the-envelope calculation, and a lot of them will be totally legit concerns. I'll try to address some of them preemptively. Slightly different assumptions can easily lead to clawing back 10% here and 50% there. But I don't see how to bridge a 25x gap just for breaking even, let alone making the AI significantly more expensive. If you want to play around with different assumptions, there's a little calculator widget below. Surely the typical LLM response is longer than that - I already picked the upper end of what the (very light) testing suggested as a reasonable range for the type of question that I'd use web search for. There's a lot of use cases where the inputs and outputs are going to be much longer (e.g. coding), but then you'd need to also switch the comparison to something in that same domain as well. The LLM API prices must be subsidized to grab market share -- i.e. the prices might be low, but the costs are high - I don't think they are, for a few reasons. I'd instead assume APIs are typically profitable on a unit basis. I have not found any credible analysis suggesting otherwise. First, there's not that much motive to gain API market share with unsustainably cheap prices. Any gains would be temporary, since there's no long-term lock-in, and better models are released weekly. Data from paid API queries will also typically not be used for training or tuning the models, so getting access to more data wouldn't explain it. Note that it's not just that you'd be losing money on each of these queries for no benefit, you're losing the compute that could be spent on training, research, or more useful types of inference. Second, some of those models have been released with open weights and API access is also available from third-party providers who would have no motive to subsidize inference. (Or the number in the table isn't even first party hosting -- I sure can't figure out what the Vertex AI pricing for Gemma 3 is). The pricing of those third-party hosted APIs appears competitive with first-party hosted APIs. For example, the Artificial Analysis summary on Deepseek R1 hosting . Third, Deepseek released actual numbers on their inference efficiency in February. Those numbers suggest that their normal R1 API pricing has about 80% margins when considering the GPU costs, though not any other serving costs. Fourth, there are a bunch of first-principles analyses on the cost structure of models with various architectures should be. Those are of course mathematical models, but those costs line up pretty well with the observed end-user pricing of models whose architecture is known. See the references section for links. The search API prices amortize building and updating the search index, LLM inference is based on just the cost of inference - This seems pretty likely to be true, actually? But the effect can't really be that large for a popular model: e.g. the allegedly leaked OpenAI financials claimed $4B/year spent on inference vs. $3B/year on training. Given the crazy growth of inference volumes (e.g. Google recently claimed a 50x increase in token volumes in the last year ) the training costs are getting amortized much more effectively. The search API prices must have higher margins than LLM inference - It's possible. I certainly don't know what the margins of any Search API providers are, though it seems fair to assume they're pretty robust. But, well, see the point above about Deepseek's releasd numbers on the R1 profit margins. Also, it seems quite plausible that some Search providers would accept lower margins, since at least Microsoft execs have testified under oath that they'd be willing to pay more for the iOS query stream than their revenue, just to get more usage data. Web search returns results 20x-100x faster than an LLM finishes the query, how could it be more expensive? - Search latency can be improved by parallelizing the problem, while LLM inference is (for now) serial in nature. The task of predicting a single token can be parallelized, but the you can't predict all the output tokens at once. But OpenAI made a loss, and they don't expect to make profit for years! - That's because a huge proportion of their usage is not monetized at all, despite the usage pattern being ideal for it. OpenAI reportedly made a loss of $5B in 2024. They also reportedly have 500M MAUs. To reach break-even, they'd just need to monetize (e.g. with ads) those free users for an average of $10/year, or $1/month. A $1 ARPU for a service like this would be pitifully low. If the reported numbers are true, OpenAI doesn't actually have high costs for a consumer service that popular, which is what you'd expect to see if the high cost of inference was the problem. They just have a very low per-user revenue, by choice. Sensitivity analysis If you want to play around with different assumptions, here's a calculator: Open in new tab Why does this matter? I mean, you're right to ask that. Nothing really matters and eventually we'll all be dead. But it is interesting how many people have built their mental model for the near future on a premise that was true for only a brief moment. Some things that will come as a surprise to them even assuming all progress stops right now: There's an argument advanced by some people about how low prices mean it'll be impossible for AI companies to ever recoup model training costs. The thinking seems to be that it's just the prices that have been going down, but not the costs, and the low prices must be an unprofitable race to the bottom for what little demand there is. What's happening and will continue to happen instead is that as costs go down, the prices go down too, and demand increases as new uses become viable. For an example, look at the OpenRouter API traffic volumes , both in aggregate and in the relative share of cheaper models. This post was mainly about APIs, but consumer usage will have exactly the same cost structure, just a different monetization structure. And given how low the unit costs must be, advertising isn't merely viable but lucrative. From this it follows that the financials of frontier AI labs are a lot better than some innumerate pundits would have you believe. They're making a loss because they're not under pressure to be profitable, and aren't actively trying to monetize consumer traffic yet. This could well be a land grab unlike APIs, since unpaid consumer queries may be used for training while paid API queries typically are not. Even the subscription pricing might be there mainly for demand management rather than trying to run a profit. The real cost problem isn't going to be with the LLMs themselves, it's with all the backend services that AI agents will want to access if even a rudimentary form of the agentic vision actually materializes. Running the AI is already cheap, will keep getting cheaper, and will always have a monetization model of some sort since it's what the end user is interacting with. Neither of those is true for the end-user services that have been turned into AI backends without their consent. An AI trying to, I don't know, book concert tickets whenever a band I like plays in my town will probably be phenomenally expensive to its third-party backends (e.g. scraping ticket sites). Those sites will be uncompensated for the expense while also removing their actual revenue streams. I don't really know how that plays out. Obviously many service owners will try to make unauthorized scraping harder, but that's a very hard problem to solve on the web. Maybe some of them give up on the web entirely, and move to mobile where they can at least get device attestations. Some might just give up on the open web, and require all usage to be signed in, with account creation being gated on something scarce. Some might become unviable and close up shop entirely. If/when that happens, what's the play on the AI agent side? Will they choose an escalating adversarial arms race with increasingly dodgy tactics, or will they eventually decide that it's better to pay for the services they use? The former seems unsustainable. If the latter, then it feels like the core engineering challenge becomes one of building data provider backends optimized specifically for AI use, with the goal of scaling to massive volumes and cheaper unit prices, with the trade-off being higher latency, lower reliability and lower quality. That could be quite interesting from a systems perspective. (Yes, I'm aware of MCP , but it's a solution to an orthogonal issue.) But one thing I'm confident won't be happening is that it's the AIs that turn out to be too expensive to run. Additional reading Below are some additional references that were not worked into the main narrative (this article was long-winded enough already). Inference economics of language models (2025) - A mathematical model for estimating the cost structure, latency/cost tradeoffs, optimal cluster size, and optimal batching based on the LLM architecture. LLM Inference Economics from First Principles - (2025) A very detailed cost-per-token computation on the cost structure of one specific model, LLama 3.3 70B. Observations About LLM Inference Pricing - (2025) Analysis of the economics driven by pricing data rather than first-principles cost structure; concludes that proprietary models have very significant markups. Large Language Models Search Architecture And Cost - (2023) Analysis on the cost of integrating LLMs into search; the LLM cost data is no longer very relevant due to the age of the article (GPT-3.5) but it uses a different way of estimating the search cost structure.",
      "summary": "16-minute read",
      "url": "https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 2507,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "One Man Armies",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://quarter--mile.com/One-Man-Armies?utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://quarter--mile.com/One-Man-Armies?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content extraction failed - no content found using site-specific or generic strategies"
    },
    {
      "title": "The SignalFire State of Talent Report - 2025",
      "content": "Over the last couple of years, we’ve seen companies rewrite the playbook for hiring AI and tech talent. In a landscape defined by fierce competition and rapid change, our latest tech talent report zeroes in on the bold moves, early trends, and strategic pivots reshaping how companies attract and retain top-tier talent in 2025 and beyond. At SignalFire, ourBeacon AIplatform tracks over 650 million professionals and 80 million organizations to give us a front-row seat to the talent transformations reshaping the industry. Our latest report reveals some sharp shifts: - Entry-level hiring is collapsing - A generational hiring shift is leaving new graduates behind - Elite AI labs are locking in top talent, and retaining them - Geographic power centers are evolving fast While headlines continue to highlight labor market shifts amidst political change and intense AI rivalry among tech giants, our data reveals a more nuanced story. This isn’t just about layoffs or remote work. It’s about a fundamental reset in how, where, and who companies are hiring to build the next generation of technology. The tech world has long been synonymous with innovation, breakneck growth, and boundless opportunities. The door to tech once swung wide open for new grads. Today, it’s barely cracked. The industry's obsession with hiring bright-eyed grads right out of college is colliding with new realities: smaller funding rounds, shrinking teams, fewer new grad programs, and the rise of AI. Everyone took a hit in 2023, but while hiring bounced back in 2024 for mid- and senior-level roles, the cut keeps getting deeper for new grads: Even top computer science grads aren’t spared.As demand for junior roles declines, even highly credentialed engineering grads are struggling to break into tech, especially at the Big Tech companies. The share of new graduates landing roles at the Magnificent Seven (Alphabet, Amazon, Apple, Meta, Microsoft, NVIDIA and Tesla) has dropped by more than half since 2022. It’s not just a hiring slowdown, it’s a shift in expectations. Today’s tech employers aren’t looking for potential, they’re looking for proof. That’s left new grads stuck in a Catch-22: you need experience to get the job, but you need the job to get experience. In a world of leaner teams and tighter budgets, there’s little room for investment in training. As the pipeline for new talent shrinks at breakneck speed, it’s creating fierce competition for the few entry-level roles that remain. The cruel irony? Companies are posting junior roles but filling them with senior individual contributors (ICs)—a phenomenon known as theexperience paradox. AI gets a lot of blame for wiping out junior roles post-2022, but the real story is more nuanced, as you can see from the 200+ comments on thisHacker News postand this recent article fromThe Atlantic. Yes, automation is alreadyreplacing some routine tasks, but the bigger driver may be the end of the “free money madness”driven by low interest rates that we saw in 2020-2022, along with the overhiring and inflation it led to. Now, with tighter budgets and shorter runways, companies are hiring leaner and later.Carta datashows that Series A tech startups are 20% smaller than they were in 2020. This shift isn’t just about hiring less—it’s a hiring reset. As AI toolstake overmore routine, entry-level tasks, companies are prioritizing roles that deliver high-leverage technical output. Big Tech is doubling down on machine learning and data engineering, while non-technical functions like recruiting, product, and sales keep shrinking, making it especially tough for Gen Z and early-career talent to break in. November 2022 didn’t just mark the launch of ChatGPT—it kicked off the AI talent race.We analyzed retention across top AI labs, and one clear leader emerged: Anthropic.An impressive 80% of employees hired at least 2 years ago at Anthropic were still at the company at the end of their second year—a figure that stands out in an industry known for high turnover. DeepMind follows closely at 78%, while OpenAI’s retention trails at 67% but remains on par with large FAANG companies like Meta (64%). Anthropic’s 80% retention isn’t just impressive—it’s a strategic advantage. In a field where turnover means lost time and money, keeping top talent consistently is a game-changer. And it’s not just about keeping talent, it’s about winning it. The tech talent map continues to shift. While San Francisco and New York remain dominant, rising hubs like Toronto, Miami, and San Diego are gaining ground. Meanwhile, Texas cities like Austin are losing momentum, prompting founders to rethink hiring and compensation strategies. More companies are embracing hub-and-spoke models and tailoring compensation philosophies to ensure they secure the right talent mix across diverse locations. Hiring is slowing in most metros, but we’re seeing a reconsolidation of tech jobs around major tech hubs like the Bay Area, Seattle, and NYC. Here are the key geographic trends we saw in 2024: 1. Silicon Valley and New York City hold strong: -High talent, high cost:SF and NYC still anchor the AI ecosystem, with over 65% of AI engineers based in these two metros. Despite rising housing costs, shrinking salary advantages, and remote work flexibility, SF and NY continue to attract more tech talent. 2. The sunshine surge - Miami and San Diego are rising fast: These two cities are attracting tech talent not with massive budgets, but with sun, lifestyle, and lower living costs. - Miami’s mixof tax perks and quality of life has fueled a 12% jump in AI roles, according to recent hiring data. - San Diegosaw a 7% rise in Big Tech roles, even as startups in the region lost 3.5% of their workforce in 2024, suggesting talent is being poached upward. In 2024, San Diego County startupsraised $5.7 billionin venture capital, marking one of the region's best-performing years on record. 3. Texas cools off: Are Austin and Houston losing their luster? Once the darlings of tech growth, Austin and Houston have been losing startup talent. Lagging infrastructure, a cultural mismatch, fluctuating housing costs, and a renewed emphasis on hybrid RTO policies are motivating startup employees to live closer to traditional hubs. Last year, Austin, which was a post-pandemic growth leader, saw a 6% drop in headcount at VC-backed startups. Houston's drop is even steeper at 10.9%. Some of our predictions fromlast year’s reportheld strong while others veered off course. Here’s what we got right—and what we predict is next. The past year made one thing clear: technology alone doesn’t build the future, people do. Anthropic’s retention edge and the reshaping of talent hubs prove that the real advantage lies in how you hire, grow, and keep great talent. - For new grads:The training wheels are gone. With fewer entry-level roles, the path forward will rely on bootcamps, open-source, freelancing, and creative projects. It’s not enough to just master the latest AI tools; learn to fix their flaws—debugging messy, machine-generated code may be your superpower. -For employers:AI might reduce the short-term need for junior hires, but skipping them entirely risks breaking the long-term talent pipeline. The industry’s future depends on equipping the next generation with skills that grow alongside the evolving technology landscape. (Read our article on how to build a multi-generational companyhere.) ________________________ Methodology Note: This report uses data from our proprietary Beacon AI platform, an intelligence engine that tracks 650+ million individuals and 80+ million organizations. We analyze millions of data points on hiring trends, geographic movements, and more to spot emerging talent and help our portfolio companies build teams and products faster. Here's the approach we used to analyze the data for this report: - “Big tech” represents the top 15 technology companies by market cap.- “Startups” represents companies funded by the Top 100 VC firms that closed a Seed through Series C round in the previous 4 years.- “Top computer science graduates” represents graduates from the top 20 engineering programs in the U.S. according to the U.S. News’ Best Undergraduate Engineering Programs Rankings. - For the AI Labs retention analysis, we excluded some newer AI labs (like DeepSeek and xAI/Grok) because they had not been operating over the whole time period we looked at (2023-2024). We focused on entities with distinct",
      "summary": "17-minute read",
      "url": "https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 1346,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 1346/3400 words (39.6%)"
    },
    {
      "title": "AI's metrics question",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content extraction failed - no content found using site-specific or generic strategies"
    },
    {
      "title": "The Code Review Frustration",
      "content": "Vibe code isn't meant to be reviewed: how to stay in control of codebase and not lose vibe code productivity boost 76 views > Disclaimer: The views and opinions expressed in this post are those of the author and do not necessarily reflect the official position of Monadical. Any content provided is for informational purposes only. ## The Code Review Frustration Another day, another slop. Chasing 10x productivity, you run several Claude code agents simultaneously and push code with blazing fast ferocity. No juniors left on the team — they just couldn't catch up. This morning, walking in the park with his dog, your team lead wrote and deployed a 100-story-point system just by casually talking on his phone to a remote agent. Meanwhile, you're stuck with a code review: ![](https://docs.monadical.com/uploads/042fee53-f1f2-40ba-9e3a-0e65fed1ee4a.png) \"It's fucking vibe code, I don't care!\" — this sentiment is just the tip of the iceberg of a current industry problem: <center><big>Treating all code the same when it fundamentally isn't</big></center> ## The Ownership Trap Here's the brutal reality: **the moment you start treating AI-generated code as \"precious,\" you lose AI's biggest superpower.** Once you've spent time reviewing, fixing, and improving that generated code, you become invested. You're going to be extra careful about AI \"breaking\" it. And it's ok, some code should be like this! But many times you just want to vibe and have your 10x productivity dream come true. Treating vibe code as precious is the productivity killer nobody talks about. You get the initial speed boost, then gradually slide back to normal development velocity as your codebase fills up with \"improved vibe code\" that you're reluctant to throw away. And you aren't ready to regenerate it from scratch anymore — an LLM skill that it excels at sometimes when it's stuck with a loop of never ending edits. Meanwhile, every code review becomes a battle between two conflicting mental models: - Reviewer: \"This could be cleaner\" (treating it like human code) - Author: \"It works, who cares\" (treating it like disposable vibe code) **The industry needs a way to keep these two types of code separate.** ## The Modular Solution: Giving Form to Chaos The solution is neither to abandon AI coding nor to accept messy codebases. It's to **architect the separation explicitly.** Think of it like this: Human code provides the \"form\" or \"mold\" that vibe code must fill. Just like the inpainting/outpainting feature in image generation. The human code contains your domain knowledge, business logic, and architectural decisions. The vibe code is just the implementation details that make it work. When AI code is clearly separated and constrained by interfaces, tests, and clear boundaries, you can also regenerate it fearlessly while keeping your valuable insights intact. ### The Overseer Package Approach #### The High-Level View Before scaring managers and leads with implementation details, here's the conceptual framework: 1. **Interface packages** - Define contracts, data shapes, and the most important tests (human-written) 2. **Implementation packages** - Fulfill those constraints (Vibe-generated, marked as @vibe-coded in README or in files) 3. **Clear dependency direction** - Implementation depends on interfaces, never the reverse 4. **Regeneration freedom** - Any @vibe-coded package can be deleted and rewritten without fear This creates a \"constraint sandwich\" - your domain knowledge stays protected while AI handles the tedious implementation work. > Technical implementation example awaits you in one of the last paragraphs. ## Two Flavors of Review: Classic vs YOLO With the modular approach and vibe code as \"second class citizen,\" we can now reframe code review practices: ### **Classic review: High standards, educational, expertise-building** Continue your nitpicky craftsmanship, talk about loops vs. combinators, and document your findings into project code style for LLMs to conform to. Share knowledge, discover new details about the system. ### **YOLO review**: \"Does it work? Does it pass tests? **Doesn't it sneak around the overseer package requirements**? Does it look safe enough? Ship it.\" <center><big>Clear separation of code \"types\" eliminates misunderstanding and friction</big></center> ## The Control Paradox Solved There are deeper psychological and sociological nth-order benefits to this approach. ### Professional Confidence When someone asks about a feature, you want to give an answer. \"I don't know, I vibed it\" destroys professional credibility. \"Cursor can't find the answer today, try again tomorrow\" makes you incompetent. With explicit separation, you can confidently say: \"The business logic is in the interface packages - here's exactly how it works. The implementation details are auto-generated, but the core logic is solid.\" ### Competitive Advantage While others choose between \"fast and messy\" or \"slow and clean,\" you get both. Your company's competitors using 100% vibe coding will hit complexity walls. Your company's competitors avoiding AI will be slower. You'll maintain AI productivity gains while building systems that actually scale. ### Better \"Boomer Business\" Adoption I believe that this approach could tip the scale for businesses who are still indecisive about vibe coding due to reliability and security concerns. ## Looking Forward: The Tooling Evolution I strongly believe that in the near future, the distinction between vibe code and human code will be admitted by industry and integrated into existing tools. ### Git-Level Integration Git commits automatically tagged as vibed. GitHub PRs showing clear visual distinction between human and AI contributions, up to the code line level. ### AI Agent Constraints Future coding agents will have an option to respect \"human code zones\" - like content-aware inpainting for image generation, but for code. Agents could regenerate entire implementations, not only files or packages, but code line-wise, leaving human guidance code untouched. ### IDE Evolution Syntax highlighting that dims unchecked vibe code while emphasizing human code. Folding options that hide implementation details. Search that prioritizes guidance, domain, architectural code. ### Corporate Adoption This separation makes AI coding auditable and controllable - exactly what the bloody enterprise needs. CTOs can require that all business logic lives in human-controlled packages while allowing rapid development in implementation packages. **So the idea of vibe code separation isn't just about individual productivity. It's about making AI coding enterprise-ready.** ## Technical Implementation in TypeScript One of the ways to split vibe and human code using current tooling that I found is a per-package approach. It's easy to do with a monorepo, but another structural or file-naming convention could work well too. For the case of monorepo, I used the \"-interface\" packages that contain concentrated domain knowledge and shape-setting code (tests, interfaces). I used dependency injection to draw more explicit frontiers between modules. ```ts export type GenerateAndSyncTasksDeps = { taskmaster: { generateTasks: ReturnType<GenerateTasksF>; }; tasktracker: { syncTasks: ReturnType<SyncTasksF>; }; }; export const generateAndSyncTasks = (di: GenerateAndSyncTasksDeps) => async (prd: PrdText) => { const tasks = await di.taskmaster.generateTasks(prd); return await di.tasktracker.syncTasks(tasks.tasks); }; ``` This is the \"entry point\" of a module that receives a PRD document, uses https://github.com/eyaltoledano/claude-task-master to generate tasks, and then syncs them to a task tracker. You can tell the coding agent to pick up from there, but it won't have enough guidance yet. Therefore, \"we need to go deeper.\" That's an example of how I defined the interface for taskmaster.generateTask, in its own package: ```ts export type GenerateTasksDeps = { savePrd: (path: NonEmptyString, prd: PrdText) => Promise<AsyncDisposable>; cli: { generate: ( prdPath: NonEmptyString, tasksJsonPath: NonEmptyString ) => Promise<TasksFileContent>; }; readTasksJson: (tasksJsonPath: NonEmptyString) => Promise<TasksFileContent>; }; export type GenerateTasksF = ( deps: GenerateTasksDeps ) => ( prd: PrdText, current: Option.Option<TasksFileContent> ) => Promise<TasksFileContent>; export const generateTasks: GenerateTasksF = (deps) => async (prd, current) => { if (Option.isSome(current)) { throw new Error(\"panic! PRD update not implemented\"); } const prdPath = castNonEmptyString(\"scripts/prd.txt\"); // not obvious: taskmaster CLI wants the prd first saved in file system await using _letFileGo = await deps.savePrd(prdPath, prd); const outputPath = castNonEmptyString(\"tasks/tasks.json\"); await deps.cli.generate(prdPath, outputPath); // don't clean up here // we read file system after CLI ran to return parsed tasks.json return await deps.readTasksJson(outputPath); }; ``` Past this point, it's already possible to tell Claude Code to generate the `GenerateTasksDeps` providing code that calls the CLI, saves and reads from the file system. Important details that we want to be preserved - \"PRD file is temporary and we want to have it in the file system before calling CLI,\" \"we also want to read the result of CLI call from the file system\" are well-preserved as strong contextual harness for LLM code. Data shape definitions are also a great candidate to use as controlling code: ```ts export const TaskFileContent = Schema.Struct({ id: TaskId, title: Schema.NonEmptyString, description: Schema.String, status: TaskStatus, dependencies: Schema.Array(TaskId), priority: Schema.optional(Schema.String), details: Schema.String, testStrategy: Schema.String, subtasks: Schema.Array(SubtaskFileContent), }); ``` Interfaces too: ```ts export interface TasksService { list: (filters?: { project?: ProjectId; status?: StatusId; user_story?: UserStoryId; }) => Promise<readonly TaskDetail[]>; create: (task: CreateTaskRequest) => Promise<TaskDetail>; get: (id: TaskId) => Promise<TaskDetail>; update: (id: TaskId, task: UpdateTaskRequest) => Promise<TaskDetail>; delete: (id: TaskId) => Promise<void>; } ``` And of course, unit tests are a great candidate for putting into controlling packages, especially [property-based tests](https://monadical.com/posts/property-based-testing-for-temporal-graph-storage.html). Also, you can put there all the code that you *could* 100% vibe but better *don't* - that you're supposed to know, if not by heart, then at least \"at some point when you wrote/reviewed it.\" Tell the agent to conform to those interfaces, test and shape-setting functions, writing the \"-implementation\" package counterpart to your \"-interface.\" An example system query: https://github.com/Monadical-SAS/taiga-taskmaster/blob/master/.llm-docs/PACKAGE_ORGANISATION_AND_CODE_SEPARATION_STANDARDS.md To me, it worked handsomely. The agent was very strong at regenerating code anew if I didn't like something and wanted to add more context to the \"control packages.\" And it never loses any context because of strict conformance to controlling code. And for the PR process, it now becomes clear which code is worth more attention and which you can just glance over, which unlocks much more of vibe power without compromising quality. Igor Loskutov is a Slop Enchanter of Monadical Recent posts Vibe code isn't meant to be reviewed * The Scraping-With-Cookies Dilemma Conversations are the New Oil Don't Give Big Tech Your Papaya View more posts... Back to top Let's transform your organization. BOOK A FREE AI STRATEGY CALL",
      "summary": "7-minute read",
      "url": "https://monadical.com/posts/vibe-code-how-to-stay-in-control.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 1660,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Silicon Valley's quest to remove friction from our lives",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
      "summary": "20-minute read",
      "url": "https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "A bit more on Twitter/X's new encrypted messaging",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
      "summary": "24-minute read",
      "url": "https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "crypto",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    }
  ]
}