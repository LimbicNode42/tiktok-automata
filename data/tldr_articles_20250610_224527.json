{
  "metadata": {
    "extraction_timestamp": "2025-06-10T22:45:27.002758",
    "scraper_version": "1.0",
    "extraction_statistics": {
      "total_articles": 26,
      "success_count": 12,
      "partial_count": 3,
      "failed_count": 11,
      "success_rate": 46.2,
      "partial_rate": 11.5,
      "failure_rate": 42.3,
      "failed_articles": [
        {
          "title": "Apple WWDC 2025: the 13 biggest announcements",
          "url": "https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Doctors Could Hack the Nervous System With Ultrasound",
          "url": "https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "AI's metrics question",
          "url": "https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
          "reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
        },
        {
          "title": "Silicon Valley's quest to remove friction from our lives",
          "url": "https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "A bit more on Twitter/X's new encrypted messaging",
          "url": "https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Everything Apple Plans to Show at Its iOS 26-Focused WWDC 2025 Event",
          "url": "https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Microsoft and Asus announce two Xbox Ally handhelds with new Xbox full-screen experience",
          "url": "https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "BYD's Five-Minute Charging Puts China in the Lead for EVs",
          "url": "https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Welcome to Campus. Here's Your ChatGPT",
          "url": "https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 435/1600 words (27.2%)"
        },
        {
          "title": "The hidden time bomb in the tax code that's fueling mass tech layoffs",
          "url": "https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Meta reportedly in talks to invest billions of dollars in Scale AI",
          "url": "https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        }
      ],
      "partial_articles": [
        {
          "title": "The SignalFire State of Talent Report - 2025",
          "url": "https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 2308/3400 words (67.9%)",
          "word_count": 2308
        },
        {
          "title": "Field Notes From Shipping Real Code With Claude",
          "url": "https://diwank.space/field-notes-from-shipping-real-code-with-claude?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 4071/7400 words (55.0%)",
          "word_count": 4071
        },
        {
          "title": "Reverse Engineering Cursor's LLM Client",
          "url": "https://www.tensorzero.com/blog/reverse-engineering-cursors-llm-client/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 2185/3400 words (64.3%)",
          "word_count": 2185
        }
      ]
    }
  },
  "articles": [
    {
      "title": "Apple WWDC 2025: the 13 biggest announcements",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "OpenAI hits $10 billion in annual recurring revenue fueled by ChatGPT growth",
      "content": "Key Points OpenAI has hit $10 billion in annual recurring revenue, according to a company spokesperson. The milestone comes roughly two and a half years after the company launched its popular ChatGPT chatbot. The figure includes revenue from the company's consumer products, ChatGPT business products and its API, an OpenAI spokesperson says. OpenAI CEO Sam Altman appears on screen during a talk with Microsoft Chair and CEO Satya Nadella at the Microsoft Build 2025 conference in Seattle on May 19, 2025. Jason Redmond | AFP | Getty Images OpenAI has hit $10 billion in annual recurring revenue, or ARR, less than three years after launching its popular ChatGPT chatbot. The figure includes sales from the company's consumer products; ChatGPT business products; and its application programming interface, or API. It excludes licensing revenue from Microsoft and large one-time deals, according to an OpenAI spokesperson. For all of last year, OpenAI had around $5.5 billion in ARR. Reaching its meteoric growth rates requires a substantial amount of cash. The San Francisco-based startup lost about $5 billion last year. OpenAI is also targeting $125 billion in revenue by 2029, according to a person familiar with the matter who asked not to be named because the details are confidential. The Information first reported on OpenAI's revenue ambitions. The new revenue metrics give some context to OpenAI's monster valuation. OpenAI closed a $40 billion funding round in March, marking the largest private tech deal on record. At today's metrics, OpenAI is valued at about 30 times revenue, which highlights the hyper growth expectations by some of its largest investors. OpenAI is backed by Japan's SoftBank , Microsoft , Coatue, Altimeter, Thrive and others. OpenAI burst onto the scene with the release of the consumer version of ChatGPT in late 2022, and began launching business products the following year. As of late March, OpenAI said it supports 500 million weekly active users. The company announced earlier this month that it has three million paying business users, up from the two million it reported in February. WATCH: OpenAI paying business users of ChatGPT hits 3 million, up from 2 million in February watch now VIDEO 1:59 01:59 OpenAI paying business users of ChatGPT hits 3 million, up from 2 million in February The Exchange FBI says Palm Springs bombing suspects used AI chat program to help plan attack Annie Palmer Nvidia CEO Jensen Huang hammers chip controls that 'effectively closed' China market Kif Leswing Telegram CEO announces $300 million partnership with Elon Musk's xAI and Grok Ashley Capoot Read More",
      "summary": "2-minute read",
      "url": "https://www.cnbc.com/2025/06/09/openai-hits-10-billion-in-annualized-revenue-fueled-by-chatgpt-growth.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 423,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Doctors Could Hack the Nervous System With Ultrasound",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
      "summary": "12-minute read",
      "url": "https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "XRobotics' countertop robots are cooking up 25,000 pizzas a month",
      "content": "XRobotics thinks it has cracked the code on getting pizza restaurants to adopt robotics. The San Francisco-based robotics company built a countertop robot called xPizza Cube, which is roughly the size of a stackable washing machine and uses machine learning to apply sauce, cheese, and pepperoni to pizza dough. The machines, which lease for $1,300 a month for three years, can make up to 100 pizzas an hour and be retrofitted to work with pies of different sizes and styles, like Detroit and Chicago deep dish. “This saves like almost 70, sometimes 80% of the time for the staff,” Denis Rodionov, the co-founder and CEO of XRobotics, told TechCrunch. “It is just repeatable work. If you have a pepperoni pizza, you need to place 50 slices of pepperoni one by one.” XRobotics is not the only company that has tried to introduce robotics into the restaurant industry — nor the only one focused on pizza. Zume is the most notable pizza robotics company — if that can be considered its own category. The company raised more than $420 million in venture capital for its robotic pizza trucks, before pivoting to focus on sustainable packaging in 2020 and shuttering entirely in 2023. Rodionov argues that they’ve been successful where other companies haven’t because they aren’t trying to fully transform the pizza-making process, as Zume was, but rather build technology to help existing pizza makers save on time and labor. Because they are building assistive technology, as opposed to replacement tech, Rodionov said they’ve been able to keep their device small enough to fit in existing kitchens and priced at a level that pizzerias — from mom-and-pop shops to large chains, both of which the company counts as customers — could afford. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The company found this out the hard way. XRobotics launched in 2019 and introduced the first version of the technology in 2021 . Its first robot was significantly larger and could work with more than 20 toppings, and ran into the same problems as their competitors. “We did a real pilot in the restaurant with our huge machine,” Rodionov said. “We learned a lot from that, and we figured out we needed a very small, compact solution. It was a bit scary. All the numbers, all the feelings, all the gut said you need to do this, not this . And we just followed the gut and said, ‘Yeah, we would go and make a smaller version,’ and it was tremendous success.” XRobotics launched their current model in 2023. The company declined to share how many customers it has. It said its robots are producing 25,000 pizzas per month, but how many customers that translates to is hard to calculate. The startup also recently raised a $2.5 million seed round led by FinSight Ventures with participation from SOSV, MANA Ventures, and Republic Capital. Rodionov said the company will use the capital to produce more units and install more robots for customers. XRobotics is committed to the pizza industry, at least for now, Rodionov said, considering the sheer size of the market — there are more than 73,000 pizza chains in the U.S. alone . The company plans to expand to Mexico and Canada next. “I love pizza, my co-founder too,” Rodionov said. “We have tested probably any pizza in San Francisco. Also, we test pizza in New York and Chicago.” Rodionov added that Detroit-style pizza, known for its square shape and crispy cheese crust, is his favorite.",
      "summary": "4-minute read",
      "url": "https://techcrunch.com/2025/06/09/xrobotics-countertop-robots-are-cooking-up-25000-pizzas-a-month/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 658,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple Announces Foundation Models Framework for Developers to Leverage AI",
      "content": "Apple Announces Foundation Models Framework for Developers to Leverage AI Monday June 9, 2025 10:10 am PDT by Tim Hardwick Apple at WWDC today announced Foundation Models Framework, a new API allowing third-party developers to leverage the large language models at the heart of Apple Intelligence and build it into their apps. With the Foundation Models Framework, developers can integrate Apple's on-device models directly into apps, allowing them to build on Apple Intelligence. \"Last year, we took the first steps on a journey to bring users intelligence that's helpful, relevant, easy to use, and right where users need it, all while protecting their privacy. Now, the models that power Apple Intelligence are becoming more capable and efficient, and we're integrating features in even more places across each of our operating systems,\" said Craig Federighi, Apple's senior vice president of Software Engineering. \"We're also taking the huge step of giving developers direct access to the on-device foundation model powering Apple Intelligence, allowing them to tap into intelligence that is powerful, fast, built with privacy, and available even when users are offline. We think this will ignite a whole new wave of intelligent experiences in the apps users rely on every day. We can't wait to see what developers create.\" The Foundation Models framework lets developers build AI-powered features that work offline, protect privacy, and incur no inference costs. For example, an education app can generate quizzes from user notes on-device, and an outdoors app can offer offline natural language search. Apple says the framework is available for testing starting today through the Apple Developer Program at developer.apple.com, and a public beta will be available through the Apple Beta Software Program next month at beta.apple.com. It includes built-in features like guided generation and tool calling for easy integration of generative capabilities into existing apps. Tag: WWDC 2025 [ 6 comments ] Get weekly top MacRumors stories in your inbox. Popular Stories Monday June 9, 2025 9:00 am PDT by Jordan Golson Apple's Worldwide Developers Conference (WWDC) starts today with the traditional keynote kicking things off at 10:00 a.m. Pacific Time. MacRumors is on hand for the event and we'll be sharing details and our thoughts throughout the day. We're expecting to see a number of software-related announcements led by a design revamp across Apple's platforms that will also see the numbering of all of... • 1211 comments Friday June 6, 2025 7:27 am PDT by Joe Rossignol Apple is planning to announce several new features for the Messages and Phone apps on iOS 26, according to Bloomberg's Mark Gurman. In a lengthy report outlining his WWDC 2025 expectations today, Gurman said that the two main changes in the Messages app will be the ability to create polls, as well as the option to set a background image within a conversation. 9to5Mac was first to report... • 50 comments Monday June 9, 2025 10:13 am PDT by Hartley Charlton Apple today announced a complete redesign of all of its major software platforms called \"Liquid Glass.\" Announced simultaneously for iOS, iPadOS, macOS, watchOS, tvOS, visionOS, and CarPlay, Liquid Glass forms a new universal design language for the first time. At its WWDC 2025 keynote address, Apple's software chief Craig Federighi said \"Apple Silicon has become dramatically more powerful... • 229 comments Friday June 6, 2025 6:17 am PDT by Joe Rossignol While the so-called \"iPhone 17 Air\" is not expected to launch until September, there are already plenty of rumors about the ultra-thin device. Overall, the iPhone 17 Air sounds like a mixed bag. While the device is expected to have an impressively thin and light design, rumors indicate it will have some compromises compared to iPhone 17 Pro models, including worse battery life, only a single ... • 77 comments Monday June 9, 2025 11:22 am PDT by Joe Rossignol Apple today announced that iPadOS 26 will be compatible with the iPad models listed below. iPadOS 26 features a new Liquid Glass design, a menu bar, improved app windowing, and more. iPadOS 26 supports the following iPad models:iPad Pro (M4) iPad Pro 12.9-inch (3rd generation and later) iPad Pro 11-inch (1st generation and later) iPad Air (M2 and later) iPad Air (3rd generation and... • 97 comments Sunday June 8, 2025 11:44 am PDT by Joe Rossignol Apple's long-rumored AirTag 2 might be coming soon. In his Power On",
      "summary": "2-minute read",
      "url": "https://www.macrumors.com/2025/06/09/foundation-models-framework/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 726,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "LLMs are cheap",
      "content": "LLMs are cheap Posted on 2025-06-02 in General This post is making a point - generative AI is relatively cheap - that might seem so obvious it doesn't need making. I'm mostly writing it because I've repeatedly had the same discussion in the past six months where people claim the opposite. Not only is the misconception still around, but it's not even getting less frequent. This is mainly written to have a document I can point people at, the next time it repeats. It seems to be a common, if not a majority, belief that Large Language Models (in the colloquial sense of \"things that are like ChatGPT\") are very expensive to operate. This then leads to a ton of innumerate analyses about how AI companies must be obviously doomed, as well as a myopic view on how consumer AI businesses can/will be monetized. It's an understandable mistake, since inference was indeed very expensive at the start of the AI boom, and those costs were talked about a lot. But inference has gotten cheaper even faster than models have gotten better, and nobody has an intuition for something becoming 1000x cheaper in two years. It just doesn't happen. It doesn't help that the common pricing model (\"$ per million tokens\") is very hard to visualize. So let's compare LLMs to web search. I'm choosing search as the comparison since it's in the same vicinity and since it's something everyone uses and nobody pays for, not because I'm suggesting that ungrounded generative AI is a good substitute for search. (It should also go without saying that these are just my personal opinions.) What is the price of a web search? Here's the public API pricing for some companies operating their own web search infrastructure, retrieved on 2025-05-02: The Gemini API pricing lists a \"Grounding with Google Search\" feature at $35/1k queries. I believe that's the best number we can get for Google, they don't publish prices for a \"raw\" search result API. The Bing Search API is priced at $15/1k queries at the cheapest tier. Brave has a price of $5/1k searches at the cheapest tier. Though there's something very strange about their pricing structure, with the unit pricing increasing as the quota increases, which is the opposite of what you'd expect. The tier with real quota is priced at $9/1k searches. So there's a range of prices, but not a horribly wide one, and with the engines you'd expect to be of higher quality also having higher prices. What is the price of LLMs in a similar domain? To make a reasonable comparison between those search prices and LLM prices, we need two numbers: How many tokens are output per query? What's the price per token? I picked a few arbitrary queries from my search history, and phrased them as questions, and ran them on Gemini 2.5 Flash (thinking mode off) in AI Studio: [When was the term LLM first used?] -> 361 tokens, 2.5 seconds [What are the top javascript game engines?] -> 1145 tokens, 7.6 seconds [What are the typical carry-on bag size limits in europe?] -> 506 tokens, 3.4 seconds [List the 10 largest power outages in history] -> 583 tokens, 3.7 seconds Note that I'm not judging the quality of the answers here. The purpose is just to get rough numbers for how large typical responses are. A 500-1000 token range seems like a reasonable estimate. What's the price of a token? The pricing is sometimes different for input and output tokens. Input tokens tend to be cheaper, and our inputs are very short compared to the outputs, so for simplicity let's consider all the tokens to be outputs. Here's the pricing of some relevant models, retrieved on 2025-05-02: Model Price / 1M tokens Gemma 3 27B $0.20 ( source ) Qwen3 30B A3B $0.30 ( source ) Gemini 2.0 Flash $0.40 ( source ) GPT-4.1 nano $0.40 ( source ) Gemini 2.5 Flash Preview $0.60 ( source ) Deepseek V3 $1.10 ( source ) GPT-4.1 mini $1.60 ( source ) Deepseek R1 $2.19 ( source ) Claude 3.5 Haiku $4.00 ( source ) GPT-4.1 $8.00 ( source ) Gemini 2.5 Pro Preview $10.00 ( source ) Claude 3.7 Sonnet $15.00 ( source ) o3 $40.00 ( source ) If we assume the average query uses 1k tokens, these prices would be directly comparable to the prices per 1k search queries. That's convenient. The low end of that spectrum is at least an order of magnitude cheaper than even the cheapest search API, and even the models at the low end are pretty capable. The high end is about on par with the highest end of search pricing. To compare a midrange pair on quality, the Bing Search vs. a Gemini 2.5 Flash comparison shows the LLM being 1/25th the price. Note that many of the above models have cheaper pricing in exchange for more flexible scheduling (Anthropic, Google and OpenAI give a 50% discount for batch requests, Deepseek is 50%-75% cheaper during off-peak hours). I've not included those cheaper options in the table to keep things comparable, but the presence of those cheaper tiers is worth keeping in mind when thinking about the next section... Objection! I know some people are going to have objections to this back-of-the-envelope calculation, and a lot of them will be totally legit concerns. I'll try to address some of them preemptively. Slightly different assumptions can easily lead to clawing back 10% here and 50% there. But I don't see how to bridge a 25x gap just for breaking even, let alone making the AI significantly more expensive. If you want to play around with different assumptions, there's a little calculator widget below. Surely the typical LLM response is longer than that - I already picked the upper end of what the (very light) testing suggested as a reasonable range for the type of question that I'd use web search for. There's a lot of use cases where the inputs and outputs are going to be much longer (e.g. coding), but then you'd need to also switch the comparison to something in that same domain as well. The LLM API prices must be subsidized to grab market share -- i.e. the prices might be low, but the costs are high - I don't think they are, for a few reasons. I'd instead assume APIs are typically profitable on a unit basis. I have not found any credible analysis suggesting otherwise. First, there's not that much motive to gain API market share with unsustainably cheap prices. Any gains would be temporary, since there's no long-term lock-in, and better models are released weekly. Data from paid API queries will also typically not be used for training or tuning the models, so getting access to more data wouldn't explain it. Note that it's not just that you'd be losing money on each of these queries for no benefit, you're losing the compute that could be spent on training, research, or more useful types of inference. Second, some of those models have been released with open weights and API access is also available from third-party providers who would have no motive to subsidize inference. (Or the number in the table isn't even first party hosting -- I sure can't figure out what the Vertex AI pricing for Gemma 3 is). The pricing of those third-party hosted APIs appears competitive with first-party hosted APIs. For example, the Artificial Analysis summary on Deepseek R1 hosting . Third, Deepseek released actual numbers on their inference efficiency in February. Those numbers suggest that their normal R1 API pricing has about 80% margins when considering the GPU costs, though not any other serving costs. Fourth, there are a bunch of first-principles analyses on the cost structure of models with various architectures should be. Those are of course mathematical models, but those costs line up pretty well with the observed end-user pricing of models whose architecture is known. See the references section for links. The search API prices amortize building and updating the search index, LLM inference is based on just the cost of inference - This seems pretty likely to be true, actually? But the effect can't really be that large for a popular model: e.g. the allegedly leaked OpenAI financials claimed $4B/year spent on inference vs. $3B/year on training. Given the crazy growth of inference volumes (e.g. Google recently claimed a 50x increase in token volumes in the last year ) the training costs are getting amortized much more effectively. The search API prices must have higher margins than LLM inference - It's possible. I certainly don't know what the margins of any Search API providers are, though it seems fair to assume they're pretty robust. But, well, see the point above about Deepseek's releasd numbers on the R1 profit margins. Also, it seems quite plausible that some Search providers would accept lower margins, since at least Microsoft execs have testified under oath that they'd be willing to pay more for the iOS query stream than their revenue, just to get more usage data. Web search returns results 20x-100x faster than an LLM finishes the query, how could it be more expensive? - Search latency can be improved by parallelizing the problem, while LLM inference is (for now) serial in nature. The task of predicting a single token can be parallelized, but the you can't predict all the output tokens at once. But OpenAI made a loss, and they don't expect to make profit for years! - That's because a huge proportion of their usage is not monetized at all, despite the usage pattern being ideal for it. OpenAI reportedly made a loss of $5B in 2024. They also reportedly have 500M MAUs. To reach break-even, they'd just need to monetize (e.g. with ads) those free users for an average of $10/year, or $1/month. A $1 ARPU for a service like this would be pitifully low. If the reported numbers are true, OpenAI doesn't actually have high costs for a consumer service that popular, which is what you'd expect to see if the high cost of inference was the problem. They just have a very low per-user revenue, by choice. Sensitivity analysis If you want to play around with different assumptions, here's a calculator: Open in new tab Why does this matter? I mean, you're right to ask that. Nothing really matters and eventually we'll all be dead. But it is interesting how many people have built their mental model for the near future on a premise that was true for only a brief moment. Some things that will come as a surprise to them even assuming all progress stops right now: There's an argument advanced by some people about how low prices mean it'll be impossible for AI companies to ever recoup model training costs. The thinking seems to be that it's just the prices that have been going down, but not the costs, and the low prices must be an unprofitable race to the bottom for what little demand there is. What's happening and will continue to happen instead is that as costs go down, the prices go down too, and demand increases as new uses become viable. For an example, look at the OpenRouter API traffic volumes , both in aggregate and in the relative share of cheaper models. This post was mainly about APIs, but consumer usage will have exactly the same cost structure, just a different monetization structure. And given how low the unit costs must be, advertising isn't merely viable but lucrative. From this it follows that the financials of frontier AI labs are a lot better than some innumerate pundits would have you believe. They're making a loss because they're not under pressure to be profitable, and aren't actively trying to monetize consumer traffic yet. This could well be a land grab unlike APIs, since unpaid consumer queries may be used for training while paid API queries typically are not. Even the subscription pricing might be there mainly for demand management rather than trying to run a profit. The real cost problem isn't going to be with the LLMs themselves, it's with all the backend services that AI agents will want to access if even a rudimentary form of the agentic vision actually materializes. Running the AI is already cheap, will keep getting cheaper, and will always have a monetization model of some sort since it's what the end user is interacting with. Neither of those is true for the end-user services that have been turned into AI backends without their consent. An AI trying to, I don't know, book concert tickets whenever a band I like plays in my town will probably be phenomenally expensive to its third-party backends (e.g. scraping ticket sites). Those sites will be uncompensated for the expense while also removing their actual revenue streams. I don't really know how that plays out. Obviously many service owners will try to make unauthorized scraping harder, but that's a very hard problem to solve on the web. Maybe some of them give up on the web entirely, and move to mobile where they can at least get device attestations. Some might just give up on the open web, and require all usage to be signed in, with account creation being gated on something scarce. Some might become unviable and close up shop entirely. If/when that happens, what's the play on the AI agent side? Will they choose an escalating adversarial arms race with increasingly dodgy tactics, or will they eventually decide that it's better to pay for the services they use? The former seems unsustainable. If the latter, then it feels like the core engineering challenge becomes one of building data provider backends optimized specifically for AI use, with the goal of scaling to massive volumes and cheaper unit prices, with the trade-off being higher latency, lower reliability and lower quality. That could be quite interesting from a systems perspective. (Yes, I'm aware of MCP , but it's a solution to an orthogonal issue.) But one thing I'm confident won't be happening is that it's the AIs that turn out to be too expensive to run. Additional reading Below are some additional references that were not worked into the main narrative (this article was long-winded enough already). Inference economics of language models (2025) - A mathematical model for estimating the cost structure, latency/cost tradeoffs, optimal cluster size, and optimal batching based on the LLM architecture. LLM Inference Economics from First Principles - (2025) A very detailed cost-per-token computation on the cost structure of one specific model, LLama 3.3 70B. Observations About LLM Inference Pricing - (2025) Analysis of the economics driven by pricing data rather than first-principles cost structure; concludes that proprietary models have very significant markups. Large Language Models Search Architecture And Cost - (2023) Analysis on the cost of integrating LLMs into search; the LLM cost data is no longer very relevant due to the age of the article (GPT-3.5) but it uses a different way of estimating the search cost structure. If you liked this and want to be notified of new posts, follow me on",
      "summary": "16-minute read",
      "url": "https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 2530,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "One Man Armies",
      "content": "One Man Armies In an overdecorated office somewhere in California, there is someone being paid so much money they could take six months of their salary, move to Brazil, and never work another day in their life — and yet this person is getting a remarkably little amount of work done every day. Just down the street, in a similar-looking office, there is a similar-looking person who is producing astonishing work at a breakneck pace. The variation in human output is kind of insane. What some may consider impossible without a team and $5M can sometimes be built by a single person who cares a lot and works hard. It’s easy to understimate just how much a one person army can achieve. So here are some of our favorite one-person (or mostly one-person) projects. We’re not including things that you wouldn't be surprised one person made, like a music album or a novel, though those are also very impressive. Stardew Valley was made by Eric Barone , who put in 70 hours a week for 4.5 years before releasing the game for the first time. He still updates the game with loads of new content. Linux was created by Linus Torvalds . He is not responsible for all of Linux and did not do everything single handedly, but he was its origin (which was a lot of work). Minecraft, before it sold to Microsoft, was developed by Markus Persson . It took him 2 years! The Art of Computer Programming is a massive tome of a lot of knowledge about computer programming and algorithms and math. It’s used by all sorts of people (from scientists to mathematicians to linguists) and it is not yet done. The one person behind this book is Donald Knuth . The Sagrada Familia was designed by Antonio Gaudí , who spent more than 40 years of his life on it. The first scientific map of the Atlantic Ocean floor, which discovered the rift valley in the Mid-Atlantic ridge and was foundational evidence for the plate tectonics theory, was created solo by Marie Tharp. Bruce Heezen, a colleague, collected the raw data as women were not allowed to work on ships at the time. The Coral Castle, a kind of weird-but-cool garden of structures in Florida, was built singlehandedly by a Latvian immigrant named Edward Leedskalnin . When asked how he moved such heavy rocks on his own for his structures, he said he knew the secrets of those who built the Pyramids. The first version of a computer program was written in the 1800s by Ada Lovelace . The Cathedral of Justo in Spain was designed and built almost exclusively by Justo Gallego Martinez . He worked on it for the better part of his life, all the way up until he died. Rollercoaster Tycoon, a game you should definitely play, was made by Chris Sawyer . Kenshi, a complex RPG game, was made in 12 years by Chris Hunt . The Theory of General Relativity, as you probably know, was created by this guy . jQuery was made by one person: John Resig . Fun fact, he works at Khan Academy as of this writing. Christianity, though it later spread in many other ways, was pioneered by Jesus of Nazareth . Islam, though it also later spread in many ways, was pioneered by Muhammad ibn Abdullah . Light is a series of seven operas, one for each day of the week. It takes nearly 30 hours to perform. It was composed over decades by Karlheinz Stockhausen . 24 Preludes, Op. 28 by Frédéric Chopin is a project that includes one prelude for each key. He wrote it entirely on his own. Many of these pieces were masterpieces and pushed the boundaries of piano at the time. Beethoven, Liszt, Rachmaninov, Mozart, Bartók, Bach, Scriabin, Debussy, and the list goes on — many of the most influential composers also put together most of their ambitious projects solo. You can take away whatever you want from this list. One takeaway is that it is extremely impressive. If the Theory of General Relativity, entire cathedrals, 29-hour operas, and billion-dollar video games can be made by a single person, your own grand and seemingly unrealistic ideas may not be too ambtious, either. And, if you care enough about them, they might be worth trying. It sounds a bit like a feel-good platitude—but it may also be true. If you are looking for a list of ideas to try, you could read this. * * * Enjoy these essays? Enter your email and we'll let you know when we post a new one. No spam, just essays. Or, if you have any feedback,",
      "summary": "4-minute read",
      "url": "https://quarter--mile.com/One-Man-Armies?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 783,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "The SignalFire State of Talent Report - 2025",
      "content": "’s talent gap widens: new grad hiring plummets, Texas loses startup talent, and Anthropic's 80% retention dominance. Over the last couple of years, we’ve seen companies rewrite the playbook for hiring AI and tech talent. In a landscape defined by fierce competition and rapid change, our latest tech talent report zeroes in on the bold moves, early trends, and strategic pivots reshaping how companies attract and retain top-tier talent in 2025 and beyond. At SignalFire, our Beacon AI platform tracks over 650 million professionals and 80 million organizations to give us a front-row seat to the talent transformations reshaping the industry. Our latest report reveals some sharp shifts: - Entry-level hiring is collapsing - A generational hiring shift is leaving new graduates behind - Elite AI labs are locking in top talent, and retaining them - Geographic power centers are evolving fast While headlines continue to highlight labor market shifts amidst political change and intense AI rivalry among tech giants, our data reveals a more nuanced story. This isn’t just about layoffs or remote work. It’s about a fundamental reset in how, where, and who companies are hiring to build the next generation of technology. #1 Tech's lost generation? New grad hiring drops 50% compared to pre-pandemic levels The tech world has long been synonymous with innovation, breakneck growth, and boundless opportunities. The door to tech once swung wide open for new grads. Today, it’s barely cracked. The industry's obsession with hiring bright-eyed grads right out of college is colliding with new realities: smaller funding rounds, shrinking teams, fewer new grad programs, and the rise of AI. Everyone took a hit in 2023, but while hiring bounced back in 2024 for mid- and senior-level roles, the cut keeps getting deeper for new grads: Big Tech: New grads now account for just 7% of hires, with new hires down 25% from 2023 and over 50% from pre-pandemic levels in 2019. Startups: New grads make up under 6% of hires, with new hires down 11% from 2023 and over 30% from pre-pandemic levels in 2019. ‎As budgets tighten and AI capabilities increase, companies are reducing their investment in new grad opportunities. The latest data from the Federal Reserve Bank of New York shows that the unemployment rate for new college grads has risen 30% since bottoming out in September 2022, versus about 18% for all workers. The perception gap isn’t helping— 55% of employers say Gen Z struggles with teamwork, and 37% of managers said they’d rather use AI than hire a Gen Z employee. Even top computer science grads aren’t spared. As demand for junior roles declines, even highly credentialed engineering grads are struggling to break into tech, especially at the Big Tech companies. The share of new graduates landing roles at the Magnificent Seven (Alphabet, Amazon, Apple, Meta, Microsoft, NVIDIA and Tesla) has dropped by more than half since 2022. ‎The experience paradox It’s not just a hiring slowdown, it’s a shift in expectations. Today’s tech employers aren’t looking for potential, they’re looking for proof. That’s left new grads stuck in a Catch-22: you need experience to get the job, but you need the job to get experience. In a world of leaner teams and tighter budgets, there’s little room for investment in training. As the pipeline for new talent shrinks at breakneck speed, it’s creating fierce competition for the few entry-level roles that remain. The cruel irony? Companies are posting junior roles but filling them with senior individual contributors (ICs)—a phenomenon known as the experience paradox . Series A squeeze or AI hype: What’s really stalling Gen Z hiring? AI gets a lot of blame for wiping out junior roles post-2022, but the real story is more nuanced, as you can see from the 200+ comments on this Hacker News post and this recent article from The Atlantic . Yes, automation is already replacing some routine tasks , but the bigger driver may be the end of the “free money madness” driven by low interest rates that we saw in 2020-2022, along with the overhiring and inflation it led to. Now, with tighter budgets and shorter runways, companies are hiring leaner and later. Carta data shows that Series A tech startups are 20% smaller than they were in 2020. This shift isn’t just about hiring less—it’s a hiring reset. As AI tools take over more routine, entry-level tasks, companies are prioritizing roles that deliver high-leverage technical output. Big Tech is doubling down on machine learning and data engineering, while non-technical functions like recruiting, product, and sales keep shrinking, making it especially tough for Gen Z and early-career talent to break in. But despite the hype, AI hasn’t wiped out entire job categories—yet. So far, the fallout has hit new grads hardest, while demand for experienced engineers is still rising. As AI capabilities continue to grow, the real differentiator will be talent, and no one’s playing the game harder than the top AI labs . #2 AI Labs: Anthropic sets the pace in the talent race November 2022 didn’t just mark the launch of ChatGPT—it kicked off the AI talent race. We analyzed retention across top AI labs, and one clear leader emerged: Anthropic. An impressive 80% of employees hired at least 2 years ago at Anthropic were still at the company at the end of their second year—a figure that stands out in an industry known for high turnover. DeepMind follows closely at 78%, while OpenAI’s retention trails at 67% but remains on par with large FAANG companies like Meta (64%). With outsized salary packages at DeepMind and even Senate scrutiny over Big Tech talent poaching, the AI talent war is real, and the spoils go to those who can attract and keep the best minds. Anthropic’s winning formula and where they’re hiring from: Anthropic’s 80% retention isn’t just impressive—it’s a strategic advantage. In a field where turnover means lost time and money, keeping top talent consistently is a game-changer. And it’s not just about keeping talent, it’s about winning it. Talent poaching with precision: Anthropic is siphoning top talent from two of its biggest rivals: OpenAI and DeepMind. Engineers are 8 times more likely to leave OpenAI for Anthropic than the reverse. From DeepMind, the ratio is nearly 11:1 in Anthropic’s favor. Some of that’s expected—Anthropic is the hot new startup, while DeepMind’s larger, tenured team is ripe for movement. But the scale of the shift is striking. Beyond the salary: While Big Tech leans on big paychecks and brand cachet, Anthropic’s edge is its unique culture that embraces unconventional thinkers and gives employees true autonomy to drive impact. Flexible work options, no title politics or forced management tracks—just clear paths for career growth. Employees say Anthropic embraces intellectual discourse and researcher autonomy, making it a magnet for AI talent stifled by bureaucracy elsewhere. Claude is quickly emerging as a favorite among developers by several informal measures , and that kind of affinity can influence career decisions. Engineers often gravitate toward companies whose products they admire and use, and that perceived product resonance may be giving Anthropic an edge in recruiting. It’s not just rival AI labs feeling the pull— Big Tech has become a prime hunting ground, too . Google, Meta, Microsoft, Amazon, and Stripe serve as primary talent pools for AI labs, and Anthropic has been particularly successful at poaching senior researchers and engineers from these companies.‎ #3 Hot spots, cooling zones, and America’s new tech hubs The tech talent map continues to shift. While San Francisco and New York remain dominant, rising hubs like Toronto, Miami, and San Diego are gaining ground. Meanwhile, Texas cities like Austin are losing momentum, prompting founders to rethink hiring and compensation strategies. More companies are embracing hub-and-spoke models and tailoring compensation philosophies to ensure they secure the right talent mix across diverse locations. Key trends shaping the new geographic playbook Hiring is slowing in most metros, but we’re seeing a reconsolidation of tech jobs around major tech hubs like the Bay Area, Seattle, and NYC. Here are the key geographic trends we saw in 2024: 1. Silicon Valley and New York City hold strong: - High talent, high cost: SF and NYC still anchor the AI ecosystem, with over 65% of AI engineers based in these two metros. Despite rising housing costs, shrinking salary advantages, and remote work flexibility, SF and NY continue to attract more tech talent. 2. The sunshine surge - Miami and San Diego are rising fast: These two cities are attracting tech talent not with massive budgets, but with sun, lifestyle, and lower living costs. - Miami’s mix of tax perks and quality of life has fueled a 12% jump in AI roles, according to recent hiring data. - San Diego saw a 7% rise in Big Tech roles, even as startups in the region lost 3.5% of their workforce in 2024, suggesting talent is being poached upward. In 2024, San Diego County startups raised $5.7 billion in venture capital, marking one of the region's best-performing years on record. 3. Texas cools off: Are Austin and Houston losing their luster? Once the darlings of tech growth, Austin and Houston have been losing startup talent. Lagging infrastructure, a cultural mismatch, fluctuating housing costs, and a renewed emphasis on hybrid RTO policies are motivating startup employees to live closer to traditional hubs. Last year, Austin, which was a post-pandemic growth leader, saw a 6% drop in headcount at VC-backed startups. Houston's drop is even steeper at 10.9%. This isn’t just a story about regional reshuffling—it’s a signal of something bigger. The media’s return-to-office narrative is overstated, and companies are rethinking what really matters. The new model? Proximity over presence. For many tech companies, it’s not about clocking in five days a week—it’s about being close enough for hybrid schedules and anchor days. The result: a surge in in-state hiring as employers strike a new balance between flexibility and face time. #4 Hits and misses from 2024 and predictions for 2025 Some of our predictions from last year’s report held strong while others veered off course. Here’s what we got right—and what we predict is next. 2024: What we saw coming: Fractional work stuck around: Fractional roles are thriving—even at the top. CMOs, CFOs, and CTOs are increasingly working as consultants. Whether this trend holds in a stronger market remains to be seen, but for now, this model seems to be working for companies and execs alike. Cybersecurity kept growing: AI-powered threats are on the rise , and the demand for cybersecurity talent has continued to grow as predicted. Compensation is up, roles are harder to fill, and hiring urgency is higher than ever. Remote work didn’t die—it evolved: The RTO debate rages on, but the reality is more nuanced. Companies are increasingly adopting hybrid models while talent continues to demand flexibility. We won’t know the long-term balance until the next economic cycle resets the supply-demand equation. Our predictions for 2025: The rise of the generalist engineer: Specialists won the last decade. Generalists may win the next. With tools like Copilot, Replit, and Cursor maturing, engineers don’t need deep ML expertise to build with AI. Companies will prioritize flexible, collaborative, generalist engineers who can move fast and collaborate effectively with powerful tools—no PhD required. The year of the equity advisor: With lean startups hesitant to hire for both junior and C-suite roles, founders will tap seasoned experts as equity advisors. Carta data shows these roles are more affordable than before, offering startups a low-cost way to gain experience and mentorship without inflating their burn rate. New jobs—not just disappearing ones: While headlines warn of jobs lost to AI, we see another shift: new roles emerging. Expect to see titles like AI governance lead, AI ethics and privacy specialists, agentic AI engineers, and non-human security ops specialists become commonplace. It’ll take time to scale, but these are some of the roles new grads should be paying attention to. People, not just technology, will shape 2025 and beyond The past year made one thing clear: technology alone doesn’t build the future, people do. Anthropic’s retention edge and the reshaping of talent hubs prove that the real advantage lies in how you hire, grow, and keep great talent. What it means for the road ahead: - For new grads: The training wheels are gone. With fewer entry-level roles, the path forward will rely on bootcamps, open-source, freelancing, and creative projects. It’s not enough to just master the latest AI tools; learn to fix their flaws—debugging messy, machine-generated code may be your superpower. - For employers: AI might reduce the short-term need for junior hires, but skipping them entirely risks breaking the long-term talent pipeline. The industry’s future depends on equipping the next generation with skills that grow alongside the evolving technology landscape. ( Read our article on how to build a multi-generational company here .) ________________________ Methodology Note: This report uses data from our proprietary Beacon AI platform, an intelligence engine that tracks 650+ million individuals and 80+ million organizations. We analyze millions of data points on hiring trends, geographic movements, and more to spot emerging talent and help our portfolio companies build teams and products faster. Here's the approach we used to analyze the data for this report: - “Big tech” represents the top 15 technology companies by market cap. - “Startups” represents companies funded by the Top 100 VC firms that closed a Seed through Series C round in the previous 4 years. - “Top computer science graduates” represents graduates from the top 20 engineering programs in the U.S. according to the U.S. News’ Best Undergraduate Engineering Programs Rankings. - For the AI Labs retention analysis, we excluded some newer AI labs (like DeepSeek and xAI/Grok) because they had not been operating over the whole time period we looked at (2023-2024). We focused on entities with distinct",
      "summary": "17-minute read",
      "url": "https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 2308,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 2308/3400 words (67.9%)"
    },
    {
      "title": "AI's metrics question",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
    },
    {
      "title": "The Code Review Frustration",
      "content": "Vibe code isn't meant to be reviewed: how to stay in control of codebase and not lose vibe code productivity boost 76 views > Disclaimer: The views and opinions expressed in this post are those of the author and do not necessarily reflect the official position of Monadical. Any content provided is for informational purposes only. ## The Code Review Frustration Another day, another slop. Chasing 10x productivity, you run several Claude code agents simultaneously and push code with blazing fast ferocity. No juniors left on the team — they just couldn't catch up. This morning, walking in the park with his dog, your team lead wrote and deployed a 100-story-point system just by casually talking on his phone to a remote agent. Meanwhile, you're stuck with a code review: ![](https://docs.monadical.com/uploads/042fee53-f1f2-40ba-9e3a-0e65fed1ee4a.png) \"It's fucking vibe code, I don't care!\" — this sentiment is just the tip of the iceberg of a current industry problem: <center><big>Treating all code the same when it fundamentally isn't</big></center> ## The Ownership Trap Here's the brutal reality: **the moment you start treating AI-generated code as \"precious,\" you lose AI's biggest superpower.** Once you've spent time reviewing, fixing, and improving that generated code, you become invested. You're going to be extra careful about AI \"breaking\" it. And it's ok, some code should be like this! But many times you just want to vibe and have your 10x productivity dream come true. Treating vibe code as precious is the productivity killer nobody talks about. You get the initial speed boost, then gradually slide back to normal development velocity as your codebase fills up with \"improved vibe code\" that you're reluctant to throw away. And you aren't ready to regenerate it from scratch anymore — an LLM skill that it excels at sometimes when it's stuck with a loop of never ending edits. Meanwhile, every code review becomes a battle between two conflicting mental models: - Reviewer: \"This could be cleaner\" (treating it like human code) - Author: \"It works, who cares\" (treating it like disposable vibe code) **The industry needs a way to keep these two types of code separate.** ## The Modular Solution: Giving Form to Chaos The solution is neither to abandon AI coding nor to accept messy codebases. It's to **architect the separation explicitly.** Think of it like this: Human code provides the \"form\" or \"mold\" that vibe code must fill. Just like the inpainting/outpainting feature in image generation. The human code contains your domain knowledge, business logic, and architectural decisions. The vibe code is just the implementation details that make it work. When AI code is clearly separated and constrained by interfaces, tests, and clear boundaries, you can also regenerate it fearlessly while keeping your valuable insights intact. ### The Overseer Package Approach #### The High-Level View Before scaring managers and leads with implementation details, here's the conceptual framework: 1. **Interface packages** - Define contracts, data shapes, and the most important tests (human-written) 2. **Implementation packages** - Fulfill those constraints (Vibe-generated, marked as @vibe-coded in README or in files) 3. **Clear dependency direction** - Implementation depends on interfaces, never the reverse 4. **Regeneration freedom** - Any @vibe-coded package can be deleted and rewritten without fear This creates a \"constraint sandwich\" - your domain knowledge stays protected while AI handles the tedious implementation work. > Technical implementation example awaits you in one of the last paragraphs. ## Two Flavors of Review: Classic vs YOLO With the modular approach and vibe code as \"second class citizen,\" we can now reframe code review practices: ### **Classic review: High standards, educational, expertise-building** Continue your nitpicky craftsmanship, talk about loops vs. combinators, and document your findings into project code style for LLMs to conform to. Share knowledge, discover new details about the system. ### **YOLO review**: \"Does it work? Does it pass tests? **Doesn't it sneak around the overseer package requirements**? Does it look safe enough? Ship it.\" <center><big>Clear separation of code \"types\" eliminates misunderstanding and friction</big></center> ## The Control Paradox Solved There are deeper psychological and sociological nth-order benefits to this approach. ### Professional Confidence When someone asks about a feature, you want to give an answer. \"I don't know, I vibed it\" destroys professional credibility. \"Cursor can't find the answer today, try again tomorrow\" makes you incompetent. With explicit separation, you can confidently say: \"The business logic is in the interface packages - here's exactly how it works. The implementation details are auto-generated, but the core logic is solid.\" ### Competitive Advantage While others choose between \"fast and messy\" or \"slow and clean,\" you get both. Your company's competitors using 100% vibe coding will hit complexity walls. Your company's competitors avoiding AI will be slower. You'll maintain AI productivity gains while building systems that actually scale. ### Better \"Boomer Business\" Adoption I believe that this approach could tip the scale for businesses who are still indecisive about vibe coding due to reliability and security concerns. ## Looking Forward: The Tooling Evolution I strongly believe that in the near future, the distinction between vibe code and human code will be admitted by industry and integrated into existing tools. ### Git-Level Integration Git commits automatically tagged as vibed. GitHub PRs showing clear visual distinction between human and AI contributions, up to the code line level. ### AI Agent Constraints Future coding agents will have an option to respect \"human code zones\" - like content-aware inpainting for image generation, but for code. Agents could regenerate entire implementations, not only files or packages, but code line-wise, leaving human guidance code untouched. ### IDE Evolution Syntax highlighting that dims unchecked vibe code while emphasizing human code. Folding options that hide implementation details. Search that prioritizes guidance, domain, architectural code. ### Corporate Adoption This separation makes AI coding auditable and controllable - exactly what the bloody enterprise needs. CTOs can require that all business logic lives in human-controlled packages while allowing rapid development in implementation packages. **So the idea of vibe code separation isn't just about individual productivity. It's about making AI coding enterprise-ready.** ## Technical Implementation in TypeScript One of the ways to split vibe and human code using current tooling that I found is a per-package approach. It's easy to do with a monorepo, but another structural or file-naming convention could work well too. For the case of monorepo, I used the \"-interface\" packages that contain concentrated domain knowledge and shape-setting code (tests, interfaces). I used dependency injection to draw more explicit frontiers between modules. ```ts export type GenerateAndSyncTasksDeps = { taskmaster: { generateTasks: ReturnType<GenerateTasksF>; }; tasktracker: { syncTasks: ReturnType<SyncTasksF>; }; }; export const generateAndSyncTasks = (di: GenerateAndSyncTasksDeps) => async (prd: PrdText) => { const tasks = await di.taskmaster.generateTasks(prd); return await di.tasktracker.syncTasks(tasks.tasks); }; ``` This is the \"entry point\" of a module that receives a PRD document, uses https://github.com/eyaltoledano/claude-task-master to generate tasks, and then syncs them to a task tracker. You can tell the coding agent to pick up from there, but it won't have enough guidance yet. Therefore, \"we need to go deeper.\" That's an example of how I defined the interface for taskmaster.generateTask, in its own package: ```ts export type GenerateTasksDeps = { savePrd: (path: NonEmptyString, prd: PrdText) => Promise<AsyncDisposable>; cli: { generate: ( prdPath: NonEmptyString, tasksJsonPath: NonEmptyString ) => Promise<TasksFileContent>; }; readTasksJson: (tasksJsonPath: NonEmptyString) => Promise<TasksFileContent>; }; export type GenerateTasksF = ( deps: GenerateTasksDeps ) => ( prd: PrdText, current: Option.Option<TasksFileContent> ) => Promise<TasksFileContent>; export const generateTasks: GenerateTasksF = (deps) => async (prd, current) => { if (Option.isSome(current)) { throw new Error(\"panic! PRD update not implemented\"); } const prdPath = castNonEmptyString(\"scripts/prd.txt\"); // not obvious: taskmaster CLI wants the prd first saved in file system await using _letFileGo = await deps.savePrd(prdPath, prd); const outputPath = castNonEmptyString(\"tasks/tasks.json\"); await deps.cli.generate(prdPath, outputPath); // don't clean up here // we read file system after CLI ran to return parsed tasks.json return await deps.readTasksJson(outputPath); }; ``` Past this point, it's already possible to tell Claude Code to generate the `GenerateTasksDeps` providing code that calls the CLI, saves and reads from the file system. Important details that we want to be preserved - \"PRD file is temporary and we want to have it in the file system before calling CLI,\" \"we also want to read the result of CLI call from the file system\" are well-preserved as strong contextual harness for LLM code. Data shape definitions are also a great candidate to use as controlling code: ```ts export const TaskFileContent = Schema.Struct({ id: TaskId, title: Schema.NonEmptyString, description: Schema.String, status: TaskStatus, dependencies: Schema.Array(TaskId), priority: Schema.optional(Schema.String), details: Schema.String, testStrategy: Schema.String, subtasks: Schema.Array(SubtaskFileContent), }); ``` Interfaces too: ```ts export interface TasksService { list: (filters?: { project?: ProjectId; status?: StatusId; user_story?: UserStoryId; }) => Promise<readonly TaskDetail[]>; create: (task: CreateTaskRequest) => Promise<TaskDetail>; get: (id: TaskId) => Promise<TaskDetail>; update: (id: TaskId, task: UpdateTaskRequest) => Promise<TaskDetail>; delete: (id: TaskId) => Promise<void>; } ``` And of course, unit tests are a great candidate for putting into controlling packages, especially [property-based tests](https://monadical.com/posts/property-based-testing-for-temporal-graph-storage.html). Also, you can put there all the code that you *could* 100% vibe but better *don't* - that you're supposed to know, if not by heart, then at least \"at some point when you wrote/reviewed it.\" Tell the agent to conform to those interfaces, test and shape-setting functions, writing the \"-implementation\" package counterpart to your \"-interface.\" An example system query: https://github.com/Monadical-SAS/taiga-taskmaster/blob/master/.llm-docs/PACKAGE_ORGANISATION_AND_CODE_SEPARATION_STANDARDS.md To me, it worked handsomely. The agent was very strong at regenerating code anew if I didn't like something and wanted to add more context to the \"control packages.\" And it never loses any context because of strict conformance to controlling code. And for the PR process, it now becomes clear which code is worth more attention and which you can just glance over, which unlocks much more of vibe power without compromising quality. Igor Loskutov is a Slop Enchanter of Monadical Recent posts Vibe code isn't meant to be reviewed * The Scraping-With-Cookies Dilemma Conversations are the New Oil Don't Give Big Tech Your Papaya View more posts... Back to top Let's transform your organization. BOOK A FREE AI STRATEGY CALL",
      "summary": "7-minute read",
      "url": "https://monadical.com/posts/vibe-code-how-to-stay-in-control.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 1660,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Silicon Valley's quest to remove friction from our lives",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
      "summary": "20-minute read",
      "url": "https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "A bit more on Twitter/X's new encrypted messaging",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
      "summary": "24-minute read",
      "url": "https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "crypto",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Everything Apple Plans to Show at Its iOS 26-Focused WWDC 2025 Event",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
      "summary": "10-minute read",
      "url": "https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Microsoft and Asus announce two Xbox Ally handhelds with new Xbox full-screen experience",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Breakthrough in search for HIV cure leaves researchers ‘overwhelmed'",
      "content": "A cure for HIV could be a step closer after researchers found a new way to force the virus out of hiding inside human cells. The virus’s ability to conceal itself inside certain white blood cells has been one of the main challenges for scientists looking for a cure. It means there is a reservoir of the HIV in the body, capable of reactivation, that neither the immune system nor drugs can tackle. Now researchers from the Peter Doherty Institute for Infection and Immunity in Melbourne, have demonstrated a way to make the virus visible, paving the way to fully clear it from the body. It is based on mRNA technology, which came to prominence during the Covid-19 pandemic when it was used in vaccines made by Moderna and Pfizer/BioNTech. In a paper published in Nature Communications , the researchers have shown for the first time that mRNA can be delivered into the cells where HIV is hiding, by encasing it in a tiny, specially formulated fat bubble. The mRNA then instructs the cells to reveal the virus. Globally, there are almost 40 million people living with HIV, who must take medication for the rest of their lives in order to suppress the virus and ensure they do not develop symptoms or transmit it. For many it remains deadly, with UNAids figures suggesting one person died of HIV every minute in 2023. It was “previously thought impossible” to deliver mRNA to the type of white blood cell that is home to HIV, said Dr Paula Cevaal, research fellow at the Doherty Institute and co-first author of the study, because those cells did not take up the fat bubbles, or lipid nanoparticles (LNPs), used to carry it. The team have developed a new type of LNP that those cells will accept, known as LNP X. She said: “Our hope is that this new nanoparticle design could be a new pathway to an HIV cure.” When a colleague first presented test results at the lab’s weekly meeting, Cevaal said, they seemed too good to be true. “We sent her back into the lab to repeat it, and she came back the next week with results that were equally good. So we had to believe it. And of course, since then, we’ve repeated it many, many, many more times. “We were overwhelmed by how [much of a] night and day difference it was – from not working before, and then all of a sudden it was working. And all of us were just sitting gasping like, ‘wow’.” Further research will be needed to determine whether revealing the virus is enough to allow the body’s immune system to deal with it, or whether the technology will need to be combined with other therapies to eliminate HIV from the body. The study is laboratory based and was carried out in cells donated by HIV patients. The path to using the technology as part of a cure for patients is long, and would require successful tests in animals followed by safety trials in humans, likely to take years, before efficacy trials could even begin. “In the field of biomedicine, many things eventually don’t make it into the clinic – that is the unfortunate truth; I don’t want to paint a prettier picture than what is the reality,” stressed Cevaal. “But in terms of specifically the field of HIV cure, we have never seen anything close to as good as what we are seeing, in terms of how well we are able to reveal this virus. “So from that point of view, we’re very hopeful that we are also able to see this type of response in an animal, and that we could eventually do this in humans.” Dr Michael Roche of the University of Melbourne and co-senior author of the research, said the discovery could have broader implications beyond HIV, with the relevant white blood cells also involved in other diseases including cancers. Dr Jonathan Stoye, a retrovirologist and emeritus scientist at the Francis Crick Institute, who was not involved in the study, said the approach taken by the Melbourne team appeared be a major advance on existing strategies to force the virus out of hiding, but further studies would be needed to determine how best to kill it after that. He added: “Ultimately, one big unknown remains. Do you need to eliminate the entire reservoir for success or just the major part? If just 10% of the latent reservoir survives will that be sufficient to seed new infection? Only time will tell. “However, that does not detract from the significance of the current study, which represents a major potential advance in delivery of mRNA for therapeutic purposes to blood cells.” Prof Tomáš Hanke of the Jenner Institute, University of Oxford, disputed the idea that getting RNA into white blood cells had been a significant challenge. He said the hope that all cells in the body where HIV was hiding could be reached in this way was “merely a dream”.",
      "summary": "2-minute read",
      "url": "https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 834,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "BYD's Five-Minute Charging Puts China in the Lead for EVs",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Field Notes From Shipping Real Code With Claude",
      "content": "June 7, 2025 Field Notes From Shipping Real Code With Claude Vibe Coding Isn’t Just a Vibe Shimmering Substance - Jackson Pollock Note : This post comes with a NotebookLM podcast ( 1 linked at the bottom), and three generated audio recordings. You can read the conversation I had with ChatGPT while preparing drafts of this post. Comments and discussion on the related HN post . Audio Think of this post as your field guide to a new way of building software. By the time you finish reading, you’ll understand not just the how but the why behind AI-assisted development that actually works. Here’s What You’re Going to Learn First, we’ll explore how to genuinely achieve a 10x productivity boost—not through magic, but through deliberate practices that amplify AI’s strengths while compensating for its weaknesses. Next, I’ll walk you through the infrastructure we use at Julep to ship production code daily with Claude’s help. You’ll see our CLAUDE.md templates, our commit strategies, and guardrails. Most importantly, you’ll understand why writing your own tests remains absolutely sacred, even (especially) in the age of AI. This single principle will save you from many a midnight debugging sessions. This is the main insight: Good development practices aren’t just nice-to-haves—they’re the difference between AI that amplifies your capabilities versus your chaos. The research bears this out. 2 Teams using rigorous practices deploy 46 times more frequently and are 440 times faster from commit to deployment. This effect is even more pronounced when you add capable AI assistants into the mix. Why This Post Exists: From Meme to Method Let me take you back to when this all started. 3 Andrej Karpathy 4 tweeted about “vibe-coding”—this idea of letting AI write your code while you just vibe. The developer community had a good laugh. It sounded like the ultimate developer fantasy: kick back, sip coffee, let the machines do the work. The birth of “vibe coding” Then Anthropic released Sonnet 3.7 and Claude Code , and something unexpected happened. The joke stopped being funny because it started being… possible? Of course, our trusty friend Cursor had been around awhile but this new interface finally felt like true vibe coding . At Julep , we build AI workflow orchestration. Our backend has years of accumulated decisions, patterns, and occasional technical debt. We have taken the utmost care to keep code quality high, and ample documentation for ourselves. However, the sheer size, and historical context of why different parts of the code are organized the way they are takes weeks for a good engineer to grok. Without proper guardrails when using Claude, you’re basically playing whack-a-mole with an overeager intern. Understanding Vibe-Coding ‘pls fix’ 5 Steve Yegge brilliantly coined the term CHOP —Chat-Oriented Programming in a slightly-dramatic-titled post “The death of the junior developer” . It’s a perfect, and no-bs description of what it’s like to code with Claude. Think of traditional coding like sculpting marble. You start with a blank block and carefully chisel away, line by line, function by function. Every stroke is deliberate, every decision yours. It’s satisfying but slow. Vibe-coding is more like conducting an orchestra. You’re not playing every instrument—you’re directing, shaping, guiding. The AI provides the raw musical talent, but without your vision, it’s just noise. There are three distinct postures you can take when vibe-coding, each suited to different phases in the development cycle: AI as First-Drafter : Here, AI generates initial implementations while you focus on architecture and design. It’s like having a junior developer who can type at the speed of thought but needs constant guidance. Perfect for boilerplate, CRUD operations, and standard patterns. AI as Pair-Programmer : This is the sweet spot for most development. You’re actively collaborating, bouncing ideas back and forth. The AI suggests approaches, you refine them. You sketch the outline, AI fills in details. It’s like pair programming with someone who has read every programming book ever written but has never actually shipped code. AI as Validator : Sometimes you write code and want a sanity check. AI reviews for bugs, suggests improvements, spots patterns you might have missed. Think of it as an incredibly well-read code reviewer who never gets tired or cranky. Instead of crafting every line, you’re reviewing, refining, directing. But—and this cannot be overstated—you remain the architect. Claude is your intern with encyclopedic knowledge but zero context about your specific system, your users, your business logic. The Three Modes of Vibe-Coding: A Practical Framework After months of experimentation and more than a few production incidents, I’ve settled on three distinct modes of operation. Each has its own rhythm, its own guardrails, and its own use cases. Mode 1: The Playground Lighter Fluid When to use it : Weekend hacks, personal scripts, proof-of-concepts, and those “I wonder if…” moments that make programming fun. In Playground Mode , you embrace the chaos. Claude writes 80-90% of the code while you provide just enough steering to keep things on track. It’s liberating and slightly terrifying. Pro Tip: check out claude-composer for going full-YOLO mode. Here’s what Playground Mode looks like: You have an idea for a script to analyze your Spotify history. You open Claude, describe what you want in plain English, and watch as it generates a complete solution. No CLAUDE.md file, no careful prompting—just raw, unfiltered AI-written code. The beauty of Playground Mode is its speed. You can go from idea to working prototype in minutes. The danger is that this cowboy coding style is absolutely inappropriate for anything that matters. Use it for experiments, never for production. Trust me, while the amazing folks preaching otherwise, good engineering principles still matter, now more than ever . Mode 2: Pair Programming Compiling When to use it : Projects under ~5,000 lines of code , side projects with real users, demos (you don’t want to break), or well-scoped small services in larger systems. This is where vibe-coding starts to shine. You need structure, but not so much that it slows you down. The key innovation here is the CLAUDE.md file—custom documentation that Claude automatically reads when invoked. From Anthropic’s Best practices for Claude Code : CLAUDE.md is a special file that Claude automatically pulls into context when starting a conversation: Common bash commands Core files and utility functions Code style guidelines Testing instructions Repository etiquette (e.g., branch naming, merge vs. rebase, etc.) Other information you want Claude to remember Instead of repeatedly explaining your project’s conventions, you document them once. Here’s a real example from a recent side project: ## Project: Analytics Dashboard This is a Next.js dashboard for visualizing user analytic: ### Architecture Decisions - Server Components by default, Client Components only when necessary - tRPC for type-safe API calls - Prisma for database access with explicit select statements - Tailwind for styling (no custom CSS files) ### Code Style - Formatting: Prettier with 100-char lines - Imports: sorted with simple-import-sort - Components: Pascal case, co-located with their tests - Hooks: always prefix with 'use' ### Patterns to Follow - Data fetching happens in Server Components - Client Components receive data as props - Use Zod schemas for all external data - Error boundaries around every data display component ### What NOT to Do - Don't use useEffect for data fetching - Don't create global state without explicit approval - Don't bypass TypeScript with 'any' types Audio With this context, Claude becomes remarkably effective. It’s like the difference between explaining your project to a new hire every single day versus having them read the onboarding docs once. But Pair Programming Mode requires more than just documentation. You need to actively guide the AI with what I call “anchor comments”—breadcrumbs that prevent Claude from wandering into the wilderness: // AIDEV- NOTE: This component uses virtual scrolling for performance // See: https://tanstack.com/virtual/latest // Don't convert to regular mapping—we handle 10k+ items export function DataTable ( { items }: DataTableProps ) { // Claude, when you edit this, maintain the virtual scrolling ... } These comments serve a dual purpose: they guide the AI and document your code for humans. It’s documentation that pays dividends in both directions. The key distinction between such “anchor comments” and regular comments: these are written , maintained , and meant to be used by Claude itself. Here’s an actual snippet from our project’s CLAUDE.md : ## Anchor comments Add specially formatted comments throughout the codebase, where appropriate, for yourself as inline knowledge that can be easily `grep` ped for. ### Guidelines: - Use `AIDEV-NOTE:` , `AIDEV-TODO:` , or `AIDEV-QUESTION:` (all-caps prefix) for comments aimed at AI and developers. - Keep them concise (≤ 120 chars). - **Important:** Before scanning files, always first try to **locate existing anchors** `AIDEV-*` in relevant subdirectories. - **Update relevant anchors** when modifying associated code. - **Do not remove `AIDEV-NOTE`s** without explicit human instruction. Example: # AIDEV-NOTE: perf-hot-path; avoid extra allocations (see ADR-24) async def render _feed(...): ... Mode 3: Production/Monorepo Scale RTFM When to use it : Large codebases, systems with real users, anything where bugs cost money or reputation. Claude can generate tremendous amounts of code, but integrating it into a complex system requires careful orchestration. Let me start with a big caveat: vibe coding at this scale does NOT scale very well, yet. I definitely do see these systems getting significantly better at handling larger codebases but , for them to be effective, significant effort is needed to help them navigate, understand, and safely hack on them without getting lost in a maze. Generally speaking, it’s better to section them into individual services, and 6 sub modules when possible. As a universal principle, good engineering practices apply to large-scale projects, vibe coded or not. For example, at production scale, boundaries become critical. Every integration point needs explicit documentation: # AIDEV- NOTE: API Contract Boundary - v2.3.1 # ANY changes require version bump and migration plan # See: docs/api-versioning.md @router.get( \"/users/{user_id}/feed\" ) async def get_user_feed ( user_id: UUID ) -> FeedResponse: # Claude: the response shape here is sacred # Changes break real apps in production ... Without these boundaries, Claude will happily “improve” your API and break every client in production. Bottom line: larger projects should definitely start adopting vibe coding in parts, and adopt methodologies that enhance that experience but, don’t expect to land large features reliably just yet. (as of June 7, 2025 / AI epoch ) Infrastructure: The Foundation of Sustainable AI Development CLAUDE.md : Your Single Source of Truth Let me be absolutely clear about this: CLAUDE.md is not optional documentation. Every minute you spend updating it saves an hour of cleanup later. Think of CLAUDE.md as a constitution for your codebase. It establishes the fundamental laws that govern how code should be written, how systems interact, and what patterns to follow or avoid. Organizations that invest in developing the skills and capabilities of their teams get better outcomes—and your CLAUDE.md is that investment crystallized into documentation. Here’s an abridged version of our production CLAUDE.md structure, refined over thousands of AI-assisted commits: # `CLAUDE.md` - Julep Backend Service ## The Golden Rule When unsure about implementation details, ALWAYS ask the developer. ## Project Context Julep enables developers to build stateful AI agents using declarative workflows. ## Critical Architecture Decisions ### Why Temporal? We use Temporal for workflow orchestration because: 1. Workflows can run for days/weeks with perfect reliability 2. Automatic recovery from any failure point ### Why PostgreSQL + pgvector? 1. ACID compliance for workflow state (can't lose user data) 2. Vector similarity search for agent memory ### Why TypeSpec? Single source of truth for API definitions: - OpenAPI specs - TypeScript/Python clients - Validation schemas ## Code Style and Patterns ### Anchor comments Add specially formatted comments throughout the codebase, where appropriate, for yourself as inline knowledge that can be easily `grep` ped for. ### Guidelines: - Use `AIDEV-NOTE:` , `AIDEV-TODO:` , or `AIDEV-QUESTION:` (all-caps prefix) for comments aimed at AI and developers. - **Important:** Before scanning files, always first try to **grep for existing anchors** `AIDEV-*` in relevant subdirectories. - **Update relevant anchors** when modifying associated code. - **Do not remove `AIDEV-NOTE`s** without explicit human instruction. - Make sure to add relevant anchor comments, whenever a file or piece of code is: * too complex, or * very important, or * confusing, or * could have a bug ## Domain Glossary (Claude, learn these!) - **Agent** : AI entity with memory, tools, and defined behavior - **Task** : Workflow definition composed of steps (NOT a Celery task) - **Execution** : Running instance of a task - **Tool** : Function an agent can call (browser, API, etc.) - **Session** : Conversation context with memory - **Entry** : Single interaction within a session ## What AI Must NEVER Do 1. **Never modify test files** - Tests encode human intent 2. **Never change API contracts** - Breaks real applications 3. **Never alter migration files** - Data loss risk 4. **Never commit secrets** - Use environment variables 5. **Never assume business logic** - Always ask 6. **Never remove AIDEV- comments** - They're there for a reason Remember: We optimize for maintainability over cleverness. When in doubt, choose the boring solution. This document becomes the shared context between you and Claude. It’s like having a senior developer whispering guidance in Claude’s ear throughout the coding session. Anchor Comments: Breadcrumbs at Scale As your codebase grows, CLAUDE.md alone isn’t enough. You need inline guidance—what I call anchor comments. These serve as local context that prevents AI from making locally bad decisions. Think of your codebase as a city and anchor comments as street signs. Without them, even smart visitors get lost. Here’s how we use them effectively: # AIDEV- NOTE: Critical performance path - this serves 100k req/sec # DO NOT add database queries here def get_user_feed ( user_id: UUID, cached_data: FeedCache ) -> List[FeedItem]: # We need to avoid mutating the cached data items = cached_data.items[:] # AIDEV- TODO: Implement pagination (ticket: FEED-123) # Need cursor-based pagination for infinite scroll # AIDEV-QUESTION: Why do we filter private items here instead of in cache? # AIDEV-ANSWER: Historical context: Privacy rules can change between cache updates filtered = [item for item in items if user_has_access(user_id, item)] return filtered These comments create a narrative that helps both AI and humans understand not just what the code does, but why it does it that way. Git Workflows for AI Development One of the most underappreciated aspects of AI-assisted development is how it changes your git workflow. You’re now generating code at a pace that can quickly pollute your git history if you’re not careful. It really only applies to very large codebases because it is not a very straightforward tool, but I recommend using git worktrees to create isolated environments for AI experiments: # Create an AI playground without polluting main git worktree add ../ai-experiments/cool-feature -b ai/cool-feature # Let Claude go wild in the isolated worktree cd ../ai-experiments/cool-feature # ... lots of experimental commits ... # Cherry-pick the good stuff back to main cd ../main-repo git cherry-pick abc123 # Just the commits that worked # Clean up when done git worktree remove ../ai-experiments/cool-feature Pro tip : Read about how to use worktrees , and check out the nifty wt tool. This approach gives you the best of both worlds: Claude can experiment freely while your main branch history stays clean and meaningful. For commit messages, we’ve standardized on tagging AI-assisted commits: feat: implement user feed caching [AI] - Add Redis-based cache for user feeds - Implement cache warming on user login - Add metrics for cache hit rate AI-assisted: core logic generated, tests human-written This transparency helps during code review—reviewers know to pay extra attention to AI-generated code. The Sacred Rule: Humans Write Tests Now we come to the most important principle in AI-assisted development. It’s so important that I’m going to repeat it in multiple ways until it’s burned into your memory: Never. Let. AI. Write. Your. Tests. Tests are not just code that verifies other code works. Tests are executable specifications. They encode your actual intentions, your edge cases, your understanding of the problem domain. High performers excel at both speed and stability—there’s no trade-off. Tests are how you achieve both. Beware… Let me illustrate why this matters with an example. Let’s say we asked Claude to implement a rate limiter: class RateLimiter : def __init__ ( self, max_requests: int , window_seconds: int ): self.max_requests = max_requests self.window_seconds = window_seconds self.requests = defaultdict( list ) def is_allowed ( self, user_id: str ) -> bool: now = time.time() user_requests = self.requests[user_id] # Clean old requests self.requests[user_id] = [ req_time for req_time in user_requests if now - req_time < self.window_seconds ] if len (self.requests[user_id]) < self.max_requests: self.requests[user_id].append(now) return True return False Looks reasonable, right? Claude even helpfully generated tests: def test_rate_limiter (): limiter = RateLimiter(max_requests= 3 , window_seconds= 60 ) assert limiter.is_allowed( \"user1\" ) == True assert limiter.is_allowed( \"user1\" ) == True assert limiter.is_allowed( \"user1\" ) == True assert limiter.is_allowed( \"user1\" ) == False # Limit reached But here’s what Claude’s tests missed—what only a human who understands the business requirements would test: Claude’s implementation has a memory leak. Users who hit the API once and never return leave their data in memory forever. The AI-generated tests check the happy path but miss this critical production concern. Vibe coding at its best This is why humans write tests. We understand the context, the production environment, the edge cases that matter. At Julep, our rule is absolute: ## Testing Discipline | What | AI CAN Do | AI MUST NOT Do | |------|-----------|----------------| | Implementation | Generate business logic | Touch test files | | Test Planning | Suggest test scenarios | Write test code | | Debugging | Analyze test failures | Modify test expectations | If an AI tool touches a test file, the PR gets rejected. No exceptions. Your tests are your specification. They’re your safety net. They’re the encoded wisdom of every bug you’ve fixed and every edge case you’ve discovered. Guard them zealously. Scaling Without Drowning: Token Economics and Context Management One of the most counterintuitive lessons in AI-assisted development is that being stingy with context to save tokens actually costs you more. It’s like trying to save money on gas by only filling your tank halfway—you just end up making more trips to the gas station. Token budgets matter. Provide focused prompts, reduce diff length, and avoid large-file bloat by summarizing intent in advance. But “focused” doesn’t mean “minimal”—it means “relevant and complete.” Let me show you the false economy of starved prompts: Starved Prompt Attempt: \"Add caching to the user endpoint\" Claude’s Response: Implements caching… but: Uses in-memory cache (won’t work with multiple servers) No cache invalidation strategy No metrics or monitoring No consideration of cache stampede Result: 3 more rounds of fixes, 4x the tokens spent . Proper Context-Rich Prompt: Add Redis caching to the GET /users/{id} endpoint. Context: - This endpoint serves 50k requests/minute - We run 12 API servers behind a load balancer - User data changes infrequently (few times per day) - We already have Redis at cache.redis.internal:6379 - Use our standard cache key pattern: \"user:v1:{id}\" - Include cache hit/miss metrics (we use Prometheus) - Implement cache-aside pattern with 1 hour TTL - Handle cache stampede with probabilistic early expiration See our caching guide: docs/patterns/caching.md The lesson? Front-load context to avoid iteration cycles. Think of tokens like investing in good tools—the upfront cost pays for itself many times over. In fact, I recommend that all projects should routinely ask Claude to look through the codebase changes, and add context to CLAUDE.md Fresh Sessions and Mental Models Here’s another counterintuitive practice: use fresh Claude sessions for distinct tasks. It’s tempting to keep one long-running conversation, but this leads to context pollution. Think of it like this: you wouldn’t use the same cutting board for vegetables after cutting raw chicken. Similarly, don’t use the same Claude session for database migrations after discussing frontend styling. The context bleeds through in subtle ways. Our rule: One task, one session. When the task is done, start fresh. This keeps Claude’s “mental model” clean and focused. Audio Case Study: Shipping Structured Errors in Production Let me walk you through a real refactoring we did at Julep that showcases production-scale vibe-coding. We needed to replace our ad-hoc error handling with a structured error hierarchy across 500+ endpoints. The Human Decisions (The Why): First, we had to decide on our error taxonomy. This is pure architectural work—Claude can’t make these decisions because they involve understanding our business, our users, and our operational needs: # SPEC.md - Error Hierarchy Design (Human-Written) ## Error Philosophy - Client errors (4xx) must include actionable feedback - System errors (5xx) must include trace IDs for debugging - All errors must be JSON-serializable - Error codes must be stable (clients depend on them) ## Hierarchy BaseError ├── ClientError (4xx) │ ├── ValidationError │ │ ├── SchemaValidationError - Request doesn't match schema │ │ ├── BusinessRuleError - Valid schema, invalid business logic │ │ └── RateLimitError - Too many requests │ └── AuthError │ ├── AuthenticationError - Who are you? │ └── AuthorizationError - You can't do that └── SystemError (5xx) ├── DatabaseError - Connection, timeout, deadlock ├── ExternalServiceError - APIs, webhooks failing └── InfrastructureError - Disk full, OOM, etc. ## Error Response Format { \"error\": { \"code\": \"VALIDATION_FAILED\", // Stable code for clients \"message\": \"Email already exists\", // Human-readable \"details\": { ... }, // Structured data \"trace_id\": \"abc-123-def\" // For debugging } } The AI Execution (The How): With the specification clear, we unleashed Claude on the mechanical refactoring: ### Prompt to Claude: Refactor our error handling to match SPEC.md. Current state: - raise ValueError(\"Invalid email\") - return {\"error\": \"Something went wrong\"}, 500 Target state: - Use error hierarchy from SPEC.md - Include proper error codes - Add trace _id to all 5xx errors Start with the auth module. Show me the plan before implementing. Claude’s plan was solid: 1. Create error hierarchy in `common/errors.py` 2. Create error response formatter 3. Update each module systematically 4. Add error handling middleware Claude was able to handle the tedious work of finding and updating 500+ error sites, while we focused on reviewing: # Before (Claude found these patterns): if not user: raise Exception( \"User not found\" ) # After (Claude's refactoring): if not user: raise AuthenticationError( message= \"User not found\" , code= \"USER_NOT_FOUND\" , details={ \"identifier\" : email} ) Combined with our carefully written CLAUDE.md file, meticulous docs, regularly updated anchor comments, and clear instructions, results: Time: 4 hours instead of 2 days Coverage: All 500+ error sites updated Leadership and Culture in the AI Era Your role as a senior engineer has fundamentally shifted. You’re no longer just writing code—you’re curating knowledge, setting boundaries, and teaching both humans and AI systems how to work effectively. Lean management and continuous delivery practices help improve software delivery performance, which in turn improves organizational performance—and this includes how you manage AI collaboration. The New Onboarding Checklist When new developers join our team, they get two onboarding tracks: one for humans, one for working with AI. Here’s our combined checklist: Week 1: Foundation □ Read team `CLAUDE.md` files (start with root, then service-specific) □ Set up development environment □ Make first PR (human-written, no AI) Week 2: Guided AI Collaboration □ Set up Claude with team templates □ Complete \"toy problem\" with AI assistance □ Practice prompt patterns □ Create first AI-assisted PR (with supervision) Week 3: Independent Work □ Ship first significant AI-assisted feature □ Write tests for another developer's AI output □ Lead one code review session Building a Culture of Transparency One cultural shift that’s essential: normalize disclosure of AI assistance. We’re not trying to hide that we use AI—we’re trying to use it responsibly. Every commit message that includes AI work gets tagged: # Our .gitmessage template # feat/fix/docs: <description> [AI]? # # [AI] - Significant AI assistance (>50% generated) # [AI-minor] - Minor AI assistance (<50% generated) # [AI-review] - AI used for code review only # # Example: # feat: add Redis caching to user service [AI] # # AI generated the cache implementation and Redis client setup. # I designed the cache key structure and wrote all tests. # Manually verified cache invalidation logic works correctly. This transparency serves multiple purposes: Reviewers know to pay extra attention Future debuggers understand the code’s provenance No one feels shame",
      "summary": "37-minute read",
      "url": "https://diwank.space/field-notes-from-shipping-real-code-with-claude?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 4071,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 4071/7400 words (55.0%)"
    },
    {
      "title": "MCP vs API",
      "content": "The HTTP API Problem MCP: A Wire Protocol, Not Documentation Why Not Just Use OpenAPI? Five Fundamental Differences Runtime Discovery vs Static Specs Deterministic Execution vs LLM-Generated Calls Bidirectional Communication Single-Request Human Tasks Local-First by Design The Training Advantage They're Layers, Not Competitors Real-World Example The Bottom Line Bonus: MCP vs API video Bonus: Existing Reddit discussions Every week a new thread emerges on Reddit asking about the difference between MCP and API. I've tried summarizing everything that's been said about MCP vs API in a single post (and a single table). Aspect Traditional APIs (REST/GraphQL) Model Context Protocol (MCP) What it is Interface styles (REST, GraphQL) with optional spec formats (OpenAPI, GraphQL SDL) Standardized protocol with enforced message structure Designed for Human developers writing code AI agents making decisions Data location REST: Path, headers, query params, body (multiple formats) Single JSON input/output per tool Discovery Static docs, regenerate SDKs for changes 1 2 Runtime introspection ( tools/list ) Execution LLM generates HTTP requests (error-prone) LLM picks tool, deterministic code runs Direction Typically client-initiated; server-push exists but not standardized Bidirectional as first-class feature Local access Requires port, auth, CORS setup Native stdio support for desktop tools Training target Impractical at scale due to heterogeneity Single protocol enables model fine-tuning I am making several broad generalizations to keep the article length reasonable. I will continue to update this article with feedback from the community. If you have any suggestions, please email me at frank@glama.ai . The HTTP API Problem HTTP APIs suffer from combinatorial chaos. To send data to an endpoint, you might encode it in: URL path ( /users/123 ) Request headers ( X-User-Id: 123 ) Query parameters ( ?userId=123 ) Request body (JSON, XML, form-encoded, CSV) OpenAPI/Swagger documents these variations, but as a specification format, it describes existing patterns rather than enforcing consistency. Building automated tools to reliably use arbitrary APIs remains hard because HTTP wasn't designed for this—it was the only cross-platform, firewall-friendly transport universally available from browsers. MCP: A Wire Protocol, Not Documentation Model Context Protocol (MCP) isn't another API standard—it's a wire protocol that enforces consistency. While OpenAPI documents existing interfaces with their variations, MCP mandates specific patterns: JSON-RPC 2.0 transport, single input schema per tool, deterministic execution. Key architecture: Transport : stdio (local) or streamable HTTP Discovery : tools/list , resources/list expose capabilities at runtime Primitives : Tools (actions), Resources (read-only data), Prompts (templates) There is more than the above. Refer to the MCP specification for complete overview. Why Not Just Use OpenAPI? The most common question: \"Why not extend OpenAPI with AI-specific features?\" Three reasons: OpenAPI describes; MCP prescribes . You can't fix inconsistency by documenting it better—you need enforcement at the protocol level. Retrofitting fails at scale . OpenAPI would need to standardize transport, mandate single-location inputs, require specific schemas, add bidirectional primitives—essentially becoming a different protocol. The ecosystem problem . Even if OpenAPI added these features tomorrow, millions of existing APIs wouldn't adopt them. MCP starts fresh with AI-first principles. Five Fundamental Differences 1. Runtime Discovery vs Static Specs API : Ship new client code when endpoints change MCP : Agents query capabilities dynamically and adapt automatically // MCP discovery - works with any server client.request('tools/list') // Returns all available tools with schemas 2. Deterministic Execution vs LLM-Generated Calls API : LLM writes the HTTP request → hallucinated paths, wrong parameters MCP : LLM picks which tool → wrapped code executes deterministically This distinction is critical for production safety. With MCP, you can test, sanitize inputs, and handle errors in actual code, not hope the LLM formats requests correctly. 3. Bidirectional Communication API : Server-push exists (WebSockets, SSE, GraphQL subscriptions) but lacks standardization MCP : Bidirectional communication as first-class feature: Request LLM completions from server Ask users for input ( elicitation ) Push progress notifications 4. Single-Request Human Tasks REST APIs fragment human tasks across endpoints. Creating a calendar event might require: POST /events (create) GET /conflicts (check) POST /invitations (notify) MCP tools map to complete workflows. One tool, one human task. 5. Local-First by Design API : Requires HTTP server (port binding, CORS, auth headers) MCP : Can run as local process via stdio—no network layer needed Why this matters: When MCP servers run locally via stdio, they inherit the host process's permissions. This enables: Direct filesystem access (read/write files) Terminal command execution System-level operations A local HTTP server could provide the same capabilities. However, I think the fact that MCP led with stdio transport planted the idea that MCP servers are meant to be as local services, which is not how we typically think of APIs. The Training Advantage MCP's standardization creates a future opportunity: models could be trained on a single, consistent protocol rather than thousands of API variations. While models today use MCP through existing function-calling capabilities, the protocol's uniformity offers immediate practical benefits: Consistent patterns across all servers: Discovery: tools/list , resources/list , prompts/list Execution: tools/call with single JSON argument object Errors: Standard JSON-RPC format with numeric codes Reduced cognitive load for models: // Every MCP tool follows the same pattern: { \"method\": \"tools/call\", \"params\": { \"name\": \"github.search_prs\", \"arguments\": {\"query\": \"security\", \"state\": \"open\"} } } // Versus REST APIs with endless variations: // GET /api/v2/search?q=security&type=pr // POST /graphql {\"query\": \"{ search(query: \\\"security\\\") { ... } }\"} // GET /repos/owner/repo/pulls?state=open&search=security This standardization means models need to learn one calling convention instead of inferring patterns from documentation. As MCP adoption grows, future models could be specifically optimized for the protocol, similar to how models today are trained on function-calling formats. They're Layers, Not Competitors Most MCP servers wrap existing APIs: [AI Agent] ⟷ MCP Client ⟷ MCP Server ⟷ REST API ⟷ Service The mcp-github server translates repository/list into GitHub REST calls. You keep battle-tested infrastructure while adding AI-friendly ergonomics. Real-World Example Consider a task: \"Find all pull requests mentioning security issues and create a summary report.\" With OpenAPI/REST : LLM reads API docs, generates: GET /repos/{owner}/{repo}/pulls?state=all Hopes it formatted the request correctly Parses response, generates: GET /repos/{owner}/{repo}/pulls/{number} Repeats for each PR (rate limiting issues) Generates search queries for comments Assembles report With MCP : LLM calls: github.search_issues_and_prs({query: \"security\", type: \"pr\"}) Deterministic code handles pagination, rate limits, error retry Returns structured data LLM focuses on analysis, not API mechanics The Bottom Line HTTP APIs evolved to serve human developers and browser-based applications, not AI agents. MCP addresses AI-specific requirements from the ground up: runtime discovery, deterministic execution, and bidirectional communication. For AI-first applications, MCP provides structural advantages—local execution, server-initiated flows, and guaranteed tool reliability—that would require significant workarounds in traditional API architectures. The practical path forward involves using both: maintaining APIs for human developers while adding MCP for AI agent integration. Bonus: MCP vs API video During my research, I found this video to be one of the easiest to digest the differences between MCP and API. Bonus: Existing Reddit discussions During my research, I found these Reddit discussions to be helpful in understanding the differences between MCP and API. https://www.reddit.com/r/mcp/comments/1kyeu26/why_mcp_protocol_vs_openapi_docs/ https://www.reddit.com/r/mcp/comments/1iztbrc/whats_the_difefrence_of_using_an_api_vs_an_mcp/ https://www.reddit.com/r/mcp/comments/1jr8if3/i_cant_understand_the_hype/ Footnotes GraphQL offers schema introspection, but it lacks task-level descriptions or JSON-schema-style validation, so SDKs still regenerate for new fields. ↩ OpenAPI 3.1+ supports runtime discovery through the OpenAPI document endpoint. The key difference is that MCP mandates runtime discovery while OpenAPI makes it optional. ↩ Written by Frank Fiegel ( @ punkpeye )",
      "summary": "6-minute read",
      "url": "https://glama.ai/blog/2025-06-06-mcp-vs-api?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 1218,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Welcome to Campus. Here's Your ChatGPT",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~1600 words, got 435. URL: https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
      "summary": "8-minute read",
      "url": "https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 435,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 435/1600 words (27.2%)"
    },
    {
      "title": "The hidden time bomb in the tax code that's fueling mass tech layoffs",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
      "summary": "14-minute read",
      "url": "https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "magic namerefs",
      "content": "namerefs (introduced in bash 4.0) act as aliases for other variables var=meow declare -n ref=var echo $ref # prints meow ref=moo echo $var # prints moo they can also reference a specific element in an array, using declare -n ref='array[1234]' using this, i have been playing with a neat nameref trick: tmp=() declare -n var= ' tmp[tmp[0]=some_expression_here, 0] ' it uses the auxiliary array tmp to force an arithmetic context, in which it assigns the result of any arbitrary expression to an element of that array, then expands that same element. we can now create magic variables that evaluate any arbitrary expression. here's a basic counter: tmp=() x=0 declare -n counter= ' tmp[tmp[0]=x++,0] ' for i in {1..10} ; do echo $counter done # prints 0 1 2 3 4 5 6 7 8 9 here's an example that computes the fibonacci numbers: f=(0 1) declare -n fib= ' f[f[2]=f[0], f[0]+=f[1], f[1]=f[2], 0] ' for i in {1..10} ; do echo $fib done # prints 1 1 2 3 5 8 13 21 34 55 this is already very powerful, as it can do many magic things with numbers. but as it turns out, we can do even more: we can use dollar expansions too! here's a silly clock with magic variables that show the current date and time: # \\D{your-format-here} passes that format to strftime # but it only works in prompts like $PS1 # ${var@P} expands $var as if it was in your $PS1 date=( ' \\D{% ' {Y,m,d,H,M,S}}) # the formats we'll use months=(- jan feb mar apr may jun jul aug sep oct nov dec) numbers=({00..60}) tmp=() declare -n year= ' tmp[tmp=${date[0]@P},0] ' declare -n month= ' months[10#${date[1]@P}] ' declare -n day= ' numbers[10#${date[2]@P}] ' declare -n hour= ' numbers[10#${date[3]@P}] ' declare -n minute= ' numbers[10#${date[4]@P}] ' declare -n second= ' numbers[10#${date[5]@P}] ' while : ; do echo $year / $month / $day $hour : $minute : $second sleep 1 done # 2025/jun/06 09:54:13 # 2025/jun/06 09:54:14 # 2025/jun/06 09:54:15 # 2025/jun/06 09:54:16 this is probably one of the coolest things i've ever seen in bash. honestly i am a bit horrified that this works at all, but the resulting code is just so simple and elegant. and it feels like i'm just scratching the surface, there's so much potential. previous: recursive expansions",
      "summary": "2-minute read",
      "url": "https://gist.github.com/izabera/e4717562e20eb6cfb6e05f8019883efb?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 387,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "SaaS Is Just Vendor Lock-In with Better Branding",
      "content": "SaaS Is Just Vendor Lock-In with Better Branding Developers are told \"to focus on the product\" and let SaaS vendors handle the rest, but integrating third-party services, whether it's auth, queuing, file storage, or image optimization, comes at a cost. Not just in dollars but in time, friction, and mental overhead. There are five hidden taxes you pay every time you integrate a SaaS into your stack. 1. The Discovery Tax Before you can integrate anything, you first have to figure out what they're actually selling? What problems are they solving? Is it compatible with your stack? Is their price sane at your scale? a Are their docs clear and do they reveal any implementation weirdness? This unpaid research work is usually non-transferable. What you learn about \"Uploady\" or \"MegaQueue\" doesn't help you next time when you're evaluating something else. It's also subjective. It's marketing, and does the marketing message resonate with you? 2. The Sign-Up Tax You've decided on a service, and this is the moment when you hand over your email and credit card. Do they support usage-based pricing or only lock-in tiers? Can your team members access the dashboard, or do you have to pay more for that functionality? Despite only using the service the same amount! Can you even test the product without hitting a paywall? You're now on the hook, even if you haven't written a single line of code. 3. The Integration Tax Now the real work begins. You read the docs. You install the libraries You wire it into your framework. And figure out the edge cases that the docs don't mention, because docs are marketing! Often you're left fighting your own tooling. They're aiming for the lowest common denominator, and you're bleeding edge. Or the other way around! 4. The Local Development Tax You need the SaaS service to work locally. Does it even offer a local emulator? Can you stub it out in tests? Do you need to tunnel to the cloud just to test one feature? Now you've got branching configuration logic, one for production, one for staging, one for local… If you're lucky. 5. The Production Tax This is the part where you're \"done,\" except you're not. Can you use this in your staging environment? What about pull request previews? You need to securely manage the API keys. Monitoring, logging, and alerting Wondering why something worked in your laptop but fails in production? You've integrated the service, but now you're on the hook for its reliability in production. Conclusion The pitch of modern SaaS is \"don't reinvent the wheel.\" But every wheel you bolt on comes with some friction. It's not just a service: It's a contract. It's a dependency. It's a subtle architectural shift, and it comes with taxes. No matter what choice you make, it's always going to be vendor-locked in. Switching out something, even if it's open source and self-hosted, means that you're rewriting a lot of code. So, my argument is, don't make those decisions. Just pick a platform. The thing that matters is the software that you want to write, not the framework or the services that it runs on. Platforms like Cloudflare or Supabase shine. Where your database, queue, image service, and storage all live within the same platform and speak the same language. You avoid paying these taxes repeatedly. You simply pick the product that's already there. No context switching between vendors. No API key wrangling. No compatibility hacks or configuration forks. Just fast, local feeling integrations that work the same in dev and production. It feels like everything is running on the same machine, and in a way it kind of is. That's the hidden superpower of integrated platforms. They collapse the distance between your code and your services. And in doing so, they give you back the one thing no SaaS vendor can sell you: \"Flow.\"",
      "summary": "3-minute read",
      "url": "https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 646,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Reverse Engineering Cursor's LLM Client",
      "content": "Reverse Engineering Cursor's LLM Client June 5, 2025 · Viraj Mehta, Aaron Hill, Gabriel Bianconi What happens under the hood at Cursor? We wired TensorZero between Cursor and the LLMs to see every token fly by… and bend those API calls to our own will. TensorZero is an open-source framework that helps engineers optimize LLM applications with downstream feedback signals (e.g. production metrics, human feedback, user behavior), and we figured it would be interesting to see whether we could use TensorZero on the LLM application we use most heavily ourselves: Cursor. With our gateway between Cursor and the LLM providers, we can observe the LLM calls being made, run evaluations on individual inferences, use inference-time optimizations, and even experiment with and optimize the prompts and models that Cursor uses. Cursor is optimized for its population of users as a whole — beyond .cursorrules , is there room for improvement by diving deeper and tailoring it to individual users? What would it look like to be able to empirically experiment with and optimize Cursor for your individual usage patterns? Beyond optimization, Cursor still operates as a black box. Wouldn’t it be interesting to see what’s actually being sent to the models? Nearly all LLM optimization, evaluation, and experimentation techniques require data on what inferences were made and their real-world consequences. In this post, we’ll focus on the former and dive into how we set up TensorZero as a self-hosted proxy between Cursor and the LLMs that it calls. If you’d like to try it yourself, check out the example in our repository. We’ll follow up with a blog post on how we collect feedback and close the optimization loop. Wiring Things Together The first thing we noticed was that Cursor lets you override the OpenAI base URL and model names. Perfect. TensorZero exposes an OpenAI-compatible inference endpoint, so we can easily configure Cursor to call TensorZero instead of OpenAI. In the TensorZero configuration, we define a TensorZero function cursorzero so that we can automatically experiment with different models and prompts while storing provider-agnostic inference and feedback data in our database for observability and optimization. The First Roadblock: Cursor’s Servers Cursor was initially unable to connect to TensorZero running locally. It turns out that Cursor first sends a request to its own servers, where additional processing happens before making the LLM call, so it couldn’t connect to our gateway on localhost . (This also means that your credentials must be forwarded to Cursor’s servers, allowing them to collect data on your inferences and codebase.) As a test that our plan could work in the first place, we pointed Cursor to OpenRouter and realized we could use its models for the Ask, Agent, and Cmd+K interactions in Cursor. We were also still able to use the normal Cursor Tab completions, which rely on a proprietary model Cursor serves for inline completions (it is very good, so we’re glad to be able to keep it). The solution was to set up a reverse proxy to expose a public endpoint that would forward requests back to our machine. We used Ngrok to keep things simple. Since we were exposing a gateway with LLM credentials to the public Internet, we added Nginx in front of our gateway to authenticate requests. We set Cursor’s base URL to our Ngrok endpoint, added the credentials we set up in Nginx, turned off built-in models, and finally added our new TensorZero function under the model name tensorzero::function_name::cursorzero . In the end, the workflow looked like this: Cursor → Ngrok → Nginx (self-hosted) → TensorZero (self-hosted) → LLM providers But it didn’t work. The Second Roadblock: CORS The authentication process had failed. Nginx logs showed that there was an OPTIONS request hitting our endpoint, so we configured Nginx to return headers on OPTIONS requests and incrementally added headers we saw in responses from the OpenAI API. This is the initial verification request that comes from the local Cursor IDE. The CORS requirement likely comes from Electron. After the initial verification, all requests come from Cursor’s servers. Our Nginx Configuration to handle CORS headers # --- CORS helper macro --- set $CORS_ALLOW_ORIGIN $http_origin; # reflect the caller's origin set $CORS_ALLOW_HEADERS \"Authorization,Content-Type\"; location / { # --- pre-flight --- if ($request_method = OPTIONS) { add_header Access-Control-Allow-Origin $CORS_ALLOW_ORIGIN always; add_header Access-Control-Allow-Credentials \"true\" always; add_header Access-Control-Allow-Methods \"GET,POST,OPTIONS\" always; add_header Access-Control-Allow-Headers $CORS_ALLOW_HEADERS always; add_header Access-Control-Max-Age 86400 always; return 204; # 204 (No Content) is conventional for pre-flight } } You can find the entire codebase for “CursorZero” on GitHub . It finally worked! Finally: Observability for Cursor We could finally see everything coming in and out of Cursor — including its prompts. For example, here’s what we saw after asking Cursor “how do I get nginx to get the key from an environment variable in my .env?” System Prompt You are a an AI coding assistant, powered by tensorzero::function_name::cursorzero. You operate in Cursor You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide. Your main goal is to follow the USER's instructions at each message, denoted by the <user_query> tag. <communication> When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use ( and ) for inline math, [ and ] for block math. </communication> <search_and_reading> If you are unsure about the answer to the USER's request or how to satiate their request, you should gather more information. This can be done by asking the USER for more information. Bias towards not asking the user for help if you can find the answer yourself. </search_and_reading> <making_code_changes> The user is likely just asking questions and not looking for edits. Only suggest edits if you are certain that the user is looking for edits. When the user is asking for edits to their code, please output a simplified version of the code block that highlights the changes necessary and adds comments to indicate where unchanged code has been skipped. For example: ```language:path/to/file // ... existing code ... {{ edit_1 }} // ... existing code ... {{ edit_2 }} // ... existing code ... ``` The user can see the entire file, so they prefer to only read the updates to the code. Often this will mean that the start/end of the file will be skipped, but that's okay! Rewrite the entire file only if specifically requested. Always provide a brief explanation of the updates, unless the user specifically requests only the code. These edit codeblocks are also read by a less intelligent language model, colloquially called the apply model, to update the file. To help specify the edit to the apply model, you will be very careful when generating the codeblock to not introduce ambiguity. You will specify all unchanged regions (code and comments) of the file with \"// ... existing code ...\" comment markers. This will ensure the apply model will not delete existing unchanged code or comments when editing the file. You will not mention the apply model. </making_code_changes> <user_info> The user's OS version is darwin 24.3.0. The absolute path of the user's workspace is /Users/viraj/tensorzero/tensorzero/examples/cursorzero. The user's shell is /bin/zsh. </user_info> You MUST use the following format when citing code regions or blocks: ```12:15:app/components/Todo.tsx // ... existing code ... ``` This is the ONLY acceptable format for code citations. The format is ```startLine:endLine:filepath``` where startLine and endLine are line numbers. User Prompt <additional_data> Below are some potentially helpful/relevant pieces of information for figuring out to respond <current_file> Path: nginx/nginx.conf Line: 1 Line Content: `events {}` </current_file> <attached_files> <file_contents> ```path=nginx/nginx.conf, lines=1-46 events {} http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\"'; access_log /var/log/nginx/access.log main; map $http_authorization $is_authorized { default 0; \"~*^Bearer your-token-here$\" 1; } server { listen 80; # --- CORS helper macro --- set $CORS_ALLOW_ORIGIN \"$http_origin\"; # reflect the caller's origin set $CORS_ALLOW_HEADERS \"Authorization,Content-Type\"; location / { # --- pre-flight --- if ($request_method = OPTIONS) { add_header Access-Control-Allow-Origin \"$CORS_ALLOW_ORIGIN\" always; add_header Access-Control-Allow-Credentials \"true\" always; add_header Access-Control-Allow-Methods \"GET,POST,OPTIONS\" always; add_header Access-Control-Allow-Headers \"$CORS_ALLOW_HEADERS\" always; add_header Access-Control-Max-Age 86400 always; return 204; # 204 No Content is conventional for pre‑flight } # --- authenticated requests --- if ($is_authorized = 0) { return 401; } proxy_pass http://gateway:3000; proxy_set_header Host $host; # expose CORS headers on the real response too add_header Access-Control-Allow-Origin \"$CORS_ALLOW_ORIGIN\" always; add_header Access-Control-Allow-Credentials \"true\" always; } } } ``` </file_contents> <manually_added_selection> ```path=nginx/nginx.conf, lines=1-16 events {} http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\"'; access_log /var/log/nginx/access.log main; map $http_authorization $is_authorized { default 0; \"~*^Bearer your-token-here$\" 1; } server { listen 80; ``` </manually_added_selection> </attached_files> </additional_data> <user_query> how do I get nginx to get the key from an environment variable in my .env? </user_query> Response I see you're trying to use an environment variable for your authentication token in the Nginx configuration instead of hardcoding \"your-token-here\" in the `nginx.conf` file. Unfortunately, Nginx doesn't directly read environment variables from a `.env` file. However, there are a few approaches you can take: ### Option 1: Use Nginx's environment variable support with a template 1. Create a template file (e.g., `nginx.conf.template`) with a placeholder: ``` map $http_authorization $is_authorized { default 0; \"~*^Bearer ${API_TOKEN}$\" 1; } ``` 2. Then use a script to replace the placeholder with the actual environment variable before starting Nginx: ```bash #!/bin/bash # Load environment variables from .env source .env # Replace placeholder with actual token envsubst '${API_TOKEN}$' < /path/to/nginx.conf.template > /path/to/nginx.conf # Start Nginx nginx -g 'daemon off;' ``` ### Option 2: Use Docker environment variables (if using Docker) If you're running Nginx in Docker, you can pass environment variables to the container and use `envsubst`: ```dockerfile FROM nginx:alpine COPY nginx.conf.template /etc/nginx/templates/default.conf.template CMD [\"/bin/sh\", \"-c\", \"envsubst '${API_TOKEN}$' < /etc/nginx/templates/default.conf.template > /etc/nginx/conf.d/default.conf && nginx -g 'daemon off;'\"] ``` Then run with: ```bash docker run -e API_TOKEN=your-secret-token -p 80:80 your-nginx-image ``` ### Option 3: Use a separate authentication service Instead of hardcoding the token in Nginx, you could proxy authentication requests to a separate service that has access to environment variables: ``` location /auth { proxy_pass http://auth-service:3001; # This service can read environment variables } ``` For your specific case, I'd recommend Option 1 or 2 depending on your deployment setup. This way, you can keep your token in your `.env` file and have it securely injected into the Nginx configuration at startup time. Prompt Engineering Cursor This example paints a picture of what prompt engineering looks like for a state-of-the-art AI coding assistant. This prompt and others like it (there were different prompts used for asking questions, editing source code, and working in the shell) are all Cursor uses to answer questions and generate code. We find it remarkable that there isn’t more here and that all of software engineering has been internalized enough by the current generation of LLMs that you can get the brains behind Cursor with a 642-token system prompt. This must be due to extensive post-training efforts by the big labs. An Explicit AI Hierarchy? We find the following snippet particularly intriguing: These edit codeblocks are also read by a less intelligent language model, colloquially called the apply model, to update the file. To help specify the edit to the apply model, you will [...]. You will not mention the apply model. It’s common to mix different models to optimize cost and latency, but Cursor explains this hierarchy to the models themselves? Interesting. A/B Testing Models With TensorZero in place, we have the full Cursor experience with control over observability and experimentation of our LLM requests. We’ve been running CursorZero for days of heavy software engineering: it’s been stable and there has been no noticeable additional latency. We’re currently running an even random split of Claude 4.0 Sonnet, GPT-4.1, o4 Mini, and Gemini 2.5 Pro — and it feels as good as Cursor ever has. Try It Yourself Interested in analyzing your own usage of AI coding assistants? You can find instructions on GitHub to reproduce this work with Cursor and OpenAI Codex . TensorZero helps you understand — and optimize — your LLM agents, even if you didn’t build them yourself! But Wait… There’s More! This blog post demonstrates how we successfully reverse-engineered Cursor’s LLM client by setting up TensorZero as a self-hosted proxy service, enabling us to observe, analyze, and experiment with different LLM models while maintaining the full Cursor experience. In our next post of this series, we’ll explain how we’re evaluating real-world usage of AI coding assistants, along with the results of our internal A/B testing between models. Sneak peek: git hooks, tree-sitter, and more. Then, we’ll explore if this feedback signal might be able to improve Cursor by optimizing models and inferences through your individual usage patterns. Stay tuned by subscribing below. Start building today. Check out our GitHub and Quick Start .",
      "summary": "17-minute read",
      "url": "https://www.tensorzero.com/blog/reverse-engineering-cursors-llm-client/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 2185,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 2185/3400 words (64.3%)"
    },
    {
      "title": "Meta reportedly in talks to invest billions of dollars in Scale AI",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
      "summary": "1-minute read",
      "url": "https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Specification Engineering",
      "content": "Specification Engineering reading time: 2.90 mins published: 2025-03-18 updated: 2025-03-29 ... is a bet on better code gen and more complexity AI engineering wants to be declarative. Ultimately, the algorithms in language model attention heads which devs interleave with their Python and Typescript are fuzzy and inscrutable. Their natural abstractions are guarantees, predicates on co-images. We can’t know what they do, only what they will have done. This is fine when AI software is 99% Python logic and 1% AI. Most software accesses SQL databases, but that’s not a problem - the interaction is tightly scoped and managed. But what happens when the software is 99% AI and 1% Python, at least in terms of complexity/headspace? At that point, the software is poorly served by the imperative paradigm. There may be plenty of LoC, but as far as it’s owner is concerned, there’s nothing imperative about it. At that point, much of the actual logic of the program lives in … the heads of its developers. The prompts won’t speak for themselves - each maintainer will, as a byproduct of hours of whiteboarding and painful trial-and-error, have reams and reams of knowledge regarding how different AI components interrelate with one another that simply cannot be safely deduced from the code itself, possibly even by a superintelligence. Storing critical system information solely in human minds - and doing so more often as time goes on and AI becomes a bigger part of software - is not a good idea. The I/O bandwidth is low, the information degrades quickly, and collaboration scales poorly. Its a structural trend directly counter to the massive productivity gains the rest of software is seeing - and it’s holding AI software development back. Evals can sometimes help introduce some structure and legibility, but they’re too fragmented - the requirements your engineers care about are distributed across hundreds of test cases you will never read and likely struggle to version and update. As time goes on, teams and engineers will want AI systems like Synth to help them - and, to be most effective in controlling and intervening on the software, those systems will need a legible and durable source of truth. Finding the right abstractions will take time, but now is the time to start. Every abstraction is leaky, and so directly maintaining imperative Python in AI software will be a necessity for the foreseeable future. But, great engineering teams will use processes and tools to ensure that the system specification is syncronized and takes precedence. PRs and prompt updates can be compiled up into spec diffs, and rejected if they introduce breaking changes. Syncronization in the other direction is where the abstraction starts paying for itself. Add a requirement -> AI spools up 1k LoC and Synth stress-tests two new prompts and a sub-agent, with 5 new evals to boot. Evals just become a way to check guarantees and create impetus for the compiler to update prompts/code/LoRAs. Naturally, syncronization will sometimes go both ways. Adding a better model might require simpler code with fewer prompts, depending on how preferences are outlined, and so we might go models -> evals -> spec -> code -> evals -> spec. Suddenly equilibrium becomes a more apt description than compilation. But don’t let that scare you away. Declarative specs with guarantees aren’t new . They’ve been used as long as software’s been written. Engineering teams benefit greatly from clearly communicating system level guarantees, and maintained them even back when it took precious human-hours to do so. What is new is intelligence that helps us consistently and cheaply transpile between them and software in a git commit hook. Let’s use it.",
      "summary": "3-minute read",
      "url": "https://www.joshuapurtell.com/posts/spec_eng/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 610,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Maintaining an Android app is a lot of work",
      "content": "There was recent news about 47% decline in the number of apps on Google Play Store. As a hobby Android developer, who has been developing MusicSync , a Google Play Music + Podcast replacement for the last five years, I thought I would share my experience of maintaining an Android app. And why this reduction in the number of apps is not surprising to me. I have several side-projects that run on a backend server with a limited web UI, and it is much less effort to maintain them. However, maintaining an Android app as a side-project is a more involved affair. And here are some of the problems I have faced. Java vs Kotlin # Kotlin is clearly the preferred language of development if you are starting a new Android project in 2025. But what if you are maintaining a hobby project written in Java? You will start seeing incompatibility when your dependencies are re-written in Kotlin. If you depend on a library that uses Kotlin’s coroutines or relies on Kotlin’s suspend functions , then you will have to work around it, or rewrite your app in Kotlin as well! Jetpack Compose, an official Google UI library for Android is entirely unusable from Java. I would imagine that if you started with Kotlin first then a big chunk of StackOverflow questions written for Java audiences require you translate them to corresponding Kotlin code as well To their credit, Android documentation still gives code samples in both Java and Kotlin. Google makes breaking changes to its libraries # Google has a habit of making breaking changes to its Android libraries. Here’s a list of some of the libraries that I have used in my app and the issues I have faced. Media 3 # Android ships with MediaPlayer . Google recommends its open-source library ExoPlayer . ExoPlayer V1 was last released in 2017. It was replaced with backward-incompatible ExoPlayer V2 which was last released in July 2024. And now, it has now been replaced with backward-incompatible media3 . The Google provided migration script is far from being complete. Further, media3 does not follow semantic versioning, minor version upgrades has resulted in breaking API changes. Google Auth library # Google’s own Auth library had a bug and sign-in was broken for API 26 and lower for months . Java 1 2 3 4 5 6 7 8 9 10 java.lang.NoSuchMethodError: No virtual method getAndSetObject (Ljava / lang / Object;JLjava / lang / Object;)Ljava / lang / Object; in class Lsun / misc / Unsafe; or its super classes (declaration of 'sun.misc.Unsafe' appears in / system / framework / core - libart.jar) E at com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper.gasWaiters(AbstractFuture.java:1394) E at com.google.common.util.concurrent.AbstractFuture.releaseWaiters(AbstractFuture.java:1110) E at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1000) E at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:783) E at com.google.auth.oauth2.OAuth2Credentials$RefreshTask.access$400(OAuth2Credentials.java:600) E at com.google.auth.oauth2.OAuth2Credentials$RefreshTask$1.onSuccess(OAuth2Credentials.java:617) ... Dropping support for older Android versions # Google Ads library v24 dropped support for Android API 21. According to official Google statistics, API 21 is used by 0.1% (~4 million) users. The rationale behind this has been left unexplained. Upgrades for the sake of it # Material 2 was deprecated for Material 3. No clear migration guide was provided. I tried to upgrade and some components like Sliders won’t look good. Why? I don’t know, and I was never able to figure out the mystic. It does not help that most documentation now refers to Jetpack Compose which I cannot use! So, for the near term, Java-based codebase are likely stuck with Material 2. The UI design guidelines for Android evolve unpredictably # Bottom bar, a featured popular on iOS was discouraged and then became a standard feature in Material design. Back and up buttons used to behave differently and now they are supposed to behave the same . I only learnt about it last year when I posted about it on Reddit. You might think that you can just use Material Design components and be done with it. But migrating from one version of Material Design to another is not trivial either. And before you migrate from Material 1 to Material 2, Google deprecates it for Material 3. Google makes breaking changes to Android platform # Every major release of Android makes breaking changes that requires developer effort Toasts use to work for quick notifications, now, after API 31, it only works if the app is foreground. How to know if you app in foreground? You have to use ActivityLifecycleCallbacks for that and write ton of code and even then there are confusions about onStart vs onResume . Displaying notifications didn’t require permissions, now after API 33, it requires POST_NOTIFICATIONS . Storage permissions were either all or none, now API 33 onwards , they can be fine-grained at the level of audio, video, and images. Background code execution restrictions keeps changing subtly in every release. Media notifications were changed in a backward-incompatible in API 33 onwards. This long thread explains the pain of a lot of developers. Crucial third-party libraries have been deprecated # Several popular third-party have been deprecated or are no longer maintained. Picasso # Picasso was great for image loading and has been deprecated . It has been replaced with coil but the upgrade is not trivial. Glide # Glide an alternative to Picasso was last released in Sep 2023. OkHttp # OkHttp which even Android uses internally for implementing HttpURLConnection has not seen a stable release since Oct 2023, the last stable release was 4.12.0 and even the last alpha release was in April 2024. OkHttp 4.12.0 does not support Happy Eyeballs which is a major issue with IPv6 networks. EventBus # EventBus was the de-facto event passing library for Android. And it is unmaintained now. RateThisApp # RateThisApp was good to get app ratings, and then it was abandoned. I don’t blame the maintainers here. If you use an open-source library, you have to be prepared for the fact that it may not be maintained. I am just pointing out, how some of the obvious boilerplate tasks that one requires for building an Android app are suddenly in a limbo. Two different versioning schemes for everything # Android has two versioning schemes , Android API version is for developers and Android version for marketing. For example, Android 11 is API 30, Android 12 is API 31 as well as 32(!), Android 13 is API 33, Android 14 is API 34. The developer documents would reference one scheme or the other or sometimes both! And you are supposed to memorize the mappings while trying to debug issues using GitHub issues or StackOverflow. It just adds unnecessary friction and confusion. Forced upgrades # There are multiple versions in an Android app, all tightly coupled with each other. minSdkVersion and targetSdkVersion of the app Java sourceCompatibility and targetCompatibility version of dependencies version of Android build tool chain version of Gradle version of Android Studio You might think that all updates are optional, but they aren’t Gradle and Android Studio must be upgraded together for version-compatibility Upgrading Java sourceCompatibility and targetCompatibility requires upgrading Gradle (and hence, Android Studio) Upgrading Android build tool chain requires upgrading minSdkVersion and targetSdkVersion Upgrade Android build tool chain requires upgrading Gradle version Also, if you want to stay on an old library like Exoplayer V2, sooner or later, it will become incompatible with other dependencies, and you will be forced to upgrade to media3! You see how you are forced to upgrade almost everything or nothing? And what if you decide to not upgrade any of these? Well, your app will get delisted if the minSdkVersion is too old. Conclusion # Compared to server-side development, Android development requires a bit more efforts to maintain. So, if you are planning to build an Android app as a hobby, keep the ongoing maintenance cost in mind. Update # After this article ended up on the front page of Hacker News , Two new items came out during those discussions that are worth mentioning. App published for the first time after 2021, have to hand over their private signing keys to Google Play Store Further, multiple people suggested that I should use F-droid to publish apps. The problem is that F-droid not only have very little reach but also it cannot solve for backward-incompatible changes to the underlying platform, abandoned libraries, and backward-incompatible changes to Android libraries.",
      "summary": "7-minute read",
      "url": "https://ashishb.net/programming/maintaining-android-app/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 1380,
      "content_extraction_status": "success",
      "failure_reason": null
    }
  ]
}