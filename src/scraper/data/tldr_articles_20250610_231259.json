{
  "metadata": {
    "extraction_timestamp": "2025-06-10T23:12:59.758656",
    "scraper_version": "1.0",
    "extraction_statistics": {
      "total_articles": 89,
      "success_count": 41,
      "partial_count": 9,
      "failed_count": 39,
      "success_rate": 46.1,
      "partial_rate": 10.1,
      "failure_rate": 43.8,
      "failed_articles": [
        {
          "title": "Apple WWDC 2025: the 13 biggest announcements",
          "url": "https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Doctors Could Hack the Nervous System With Ultrasound",
          "url": "https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "AI's metrics question",
          "url": "https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
          "reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
        },
        {
          "title": "Silicon Valley's quest to remove friction from our lives",
          "url": "https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "A bit more on Twitter/X's new encrypted messaging",
          "url": "https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Everything Apple Plans to Show at Its iOS 26-Focused WWDC 2025 Event",
          "url": "https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Microsoft and Asus announce two Xbox Ally handhelds with new Xbox full-screen experience",
          "url": "https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "BYD's Five-Minute Charging Puts China in the Lead for EVs",
          "url": "https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Welcome to Campus. Here's Your ChatGPT",
          "url": "https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 435/1600 words (27.2%)"
        },
        {
          "title": "The hidden time bomb in the tax code that's fueling mass tech layoffs",
          "url": "https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
          "reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
        },
        {
          "title": "Meta reportedly in talks to invest billions of dollars in Scale AI",
          "url": "https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Longevity Is Now a Factor When Picking an Embryo for IVF",
          "url": "https://www.wsj.com/health/embryo-ivf-screening-longevity-2b1f096a?st=WfueaT&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "What a developer needs to know about SCIM",
          "url": "https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim?utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 256/3800 words (6.7%)"
        },
        {
          "title": "X Is Testing a Way To Take Community Notes to the Next Level",
          "url": "https://www.adweek.com/media/x-is-testing-a-way-to-take-community-notes-to-the-next-level/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "One thing Tesla and Comma.ai overlooked in self-driving",
          "url": "https://deepdream.substack.com/p/one-thing-tesla-and-commaai-overlooked?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "LLMs are mirrors of operator skill",
          "url": "https://ghuntley.com/mirrors/?utm_source=tldrnewsletter",
          "reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
        },
        {
          "title": "23andMe's Former CEO Pushes Purchase Price Nearly $50 Million Higher",
          "url": "https://www.wsj.com/articles/23andmes-former-ceo-pushes-purchase-price-nearly-50-million-higher-7ec98421?st=HxjmWW&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Reddit Sues Anthropic, Alleges Unauthorized Use of Site's Data",
          "url": "https://www.wsj.com/tech/ai/reddit-lawsuit-anthropic-ai-3b9624dd?st=zWM5Ru&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Musk Says SpaceX Revenue Will Near $16 Billion in 2025",
          "url": "https://www.wsj.com/business/spacex-revenue-to-exceed-nasas-budget-in-2026-elon-musk-says-3321ea31?st=4dsszH&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Human Brain Cells on a Chip for Sale",
          "url": "https://spectrum.ieee.org/biological-computer-for-sale?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Why GUIs are built at least 2.5 times",
          "url": "https://patricia.no/2025/05/30/why_lean_software_dev_is_wrong.html?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Meta and Yandex are de-anonymizing Android users' web browsing identifiers",
          "url": "https://arstechnica.com/security/2025/06/meta-and-yandex-are-de-anonymizing-android-users-web-browsing-identifiers/?utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 299/4200 words (7.1%)"
        },
        {
          "title": "Does AI Progress Have a Speed Limit?",
          "url": "https://asteriskmag.com/issues/10/does-ai-progress-have-a-speed-limit?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Google's NotebookLM now lets you share your notebook — and AI podcasts — publicly",
          "url": "https://www.theverge.com/news/678915/google-notebooklm-share-public-link?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Meta Prioritizing Ultralight Headset With Puck For 2026 Over Traditional Quest 4",
          "url": "https://www.uploadvr.com/meta-prioritizing-puffin-for-2026-pushing-out-quest-4-to-2027/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Stop Over-thinking AI Subscriptions",
          "url": "https://steipete.me/posts/2025/stop-overthinking-ai-subscriptions?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "How Much Should the World's Richest Man Get Paid?",
          "url": "https://www.wsj.com/business/elon-musk-tesla-pay-package-fcd86d99?st=2vtDJs&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Meta Aims to Fully Automate Ad Creation Using AI",
          "url": "https://www.wsj.com/tech/ai/meta-aims-to-fully-automate-ad-creation-using-ai-7d82e249?st=jA6Fqa&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "9 Lessons From Cursor's System Prompt",
          "url": "https://byteatatime.dev/posts/cursor-prompt-analysis/?utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 2591/10400 words (24.9%)"
        },
        {
          "title": "Google's Pixel 10 phones will reportedly launch on August 13th",
          "url": "https://www.theverge.com/news/678088/google-pixel-10-release-date-launch-august?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Of Course Workers Want a Four-Day Week. Companies Should Too",
          "url": "https://www.wsj.com/lifestyle/workplace/of-course-workers-want-a-four-day-week-companies-should-too-0837a0a1?st=nmPfYi&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "What happened in the 2010s?",
          "url": "https://www.strangeloopcanon.com/p/what-happened-in-the-2010s?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Airbnb CEO Brian Chesky wants to build the everything app",
          "url": "https://www.theverge.com/decoder-podcast-with-nilay-patel/677324/airbnb-ceo-brian-chesky-services-redesign-app-future-travel?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "It's Waymo's World. We're All Just Riding in It",
          "url": "https://www.wsj.com/tech/waymo-cars-self-driving-robotaxi-tesla-uber-0777f570?st=p2QtzY&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "For Some Recent Graduates, the A.I. Job Apocalypse May Already Be Here",
          "url": "https://www.nytimes.com/2025/05/30/technology/ai-jobs-college-graduates.html?unlocked_article_code=1.Lk8.juhQ.YSBBq29gTIhd&smid=url-share&utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 461/1800 words (25.6%)"
        },
        {
          "title": "The U.S. Plan to Hobble China Tech Isn't Working",
          "url": "https://www.wsj.com/tech/the-u-s-plan-to-hobble-china-tech-isnt-working-56d1a512?st=EKbL7D&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Judge Examines Steps to Limit Google's Reach in AI Arms Race",
          "url": "https://www.wsj.com/tech/google-search-ai-antitrust-trial-57ec6fdb?st=HmfzFC&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "The Illusion of Causality in Charts",
          "url": "https://filwd.substack.com/p/the-illusion-of-causality-in-charts?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Claude 4 - From Hallucination to Creation?",
          "url": "https://omarabid.com/claude-4?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        }
      ],
      "partial_articles": [
        {
          "title": "The SignalFire State of Talent Report - 2025",
          "url": "https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 2308/3400 words (67.9%)",
          "word_count": 2308
        },
        {
          "title": "Field Notes From Shipping Real Code With Claude",
          "url": "https://diwank.space/field-notes-from-shipping-real-code-with-claude?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 4071/7400 words (55.0%)",
          "word_count": 4071
        },
        {
          "title": "Reverse Engineering Cursor's LLM Client",
          "url": "https://www.tensorzero.com/blog/reverse-engineering-cursors-llm-client/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 2185/3400 words (64.3%)",
          "word_count": 2185
        },
        {
          "title": "Amazon ‘testing humanoid robots to deliver packages'",
          "url": "https://www.theguardian.com/technology/2025/jun/05/amazon-testing-humanoid-robots-to-deliver-packages?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 390/600 words (65.0%)",
          "word_count": 390
        },
        {
          "title": "AMC Networks Inks Deal With AI Company Runway",
          "url": "https://www.hollywoodreporter.com/business/business-news/amc-networks-runway-ai-deal-1236255108/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 320/600 words (53.3%)",
          "word_count": 320
        },
        {
          "title": "Epic Games' MetaHuman creation tool launches out of early access",
          "url": "https://venturebeat.com/gaming-business/epic-games-metahuman-creation-tool-launches-out-of-early-access/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 227/400 words (56.8%)",
          "word_count": 227
        },
        {
          "title": "Typing 118 WPM Broke My Brain in the Right Ways (A Year-Long Journey)",
          "url": "https://balaji-amg.surge.sh/blog/typing-118-wpm-brain-rewiring?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 442/1200 words (36.8%)",
          "word_count": 442
        },
        {
          "title": "Digg's founders explain how they're building a site for humans in the AI era",
          "url": "https://techcrunch.com/2025/06/02/diggs-founders-explain-how-theyre-building-a-site-for-humans-in-the-ai-era/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 483/1600 words (30.2%)",
          "word_count": 483
        },
        {
          "title": "Systems Correctness Practices at Amazon Web Services",
          "url": "https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 3023/4400 words (68.7%)",
          "word_count": 3023
        }
      ]
    }
  },
  "articles": [
    {
      "title": "Apple WWDC 2025: the 13 biggest announcements",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "OpenAI hits $10 billion in annual recurring revenue fueled by ChatGPT growth",
      "content": "Key Points OpenAI has hit $10 billion in annual recurring revenue, according to a company spokesperson. The milestone comes roughly two and a half years after the company launched its popular ChatGPT chatbot. The figure includes revenue from the company's consumer products, ChatGPT business products and its API, an OpenAI spokesperson says. OpenAI CEO Sam Altman appears on screen during a talk with Microsoft Chair and CEO Satya Nadella at the Microsoft Build 2025 conference in Seattle on May 19, 2025. Jason Redmond | AFP | Getty Images OpenAI has hit $10 billion in annual recurring revenue, or ARR, less than three years after launching its popular ChatGPT chatbot. The figure includes sales from the company's consumer products; ChatGPT business products; and its application programming interface, or API. It excludes licensing revenue from Microsoft and large one-time deals, according to an OpenAI spokesperson. For all of last year, OpenAI had around $5.5 billion in ARR. Reaching its meteoric growth rates requires a substantial amount of cash. The San Francisco-based startup lost about $5 billion last year. OpenAI is also targeting $125 billion in revenue by 2029, according to a person familiar with the matter who asked not to be named because the details are confidential. The Information first reported on OpenAI's revenue ambitions. The new revenue metrics give some context to OpenAI's monster valuation. OpenAI closed a $40 billion funding round in March, marking the largest private tech deal on record. At today's metrics, OpenAI is valued at about 30 times revenue, which highlights the hyper growth expectations by some of its largest investors. OpenAI is backed by Japan's SoftBank , Microsoft , Coatue, Altimeter, Thrive and others. OpenAI burst onto the scene with the release of the consumer version of ChatGPT in late 2022, and began launching business products the following year. As of late March, OpenAI said it supports 500 million weekly active users. The company announced earlier this month that it has three million paying business users, up from the two million it reported in February. WATCH: OpenAI paying business users of ChatGPT hits 3 million, up from 2 million in February watch now VIDEO 1:59 01:59 OpenAI paying business users of ChatGPT hits 3 million, up from 2 million in February The Exchange FBI says Palm Springs bombing suspects used AI chat program to help plan attack Annie Palmer Nvidia CEO Jensen Huang hammers chip controls that 'effectively closed' China market Kif Leswing Telegram CEO announces $300 million partnership with Elon Musk's xAI and Grok Ashley Capoot Read More",
      "summary": "2-minute read",
      "url": "https://www.cnbc.com/2025/06/09/openai-hits-10-billion-in-annualized-revenue-fueled-by-chatgpt-growth.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 423,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Doctors Could Hack the Nervous System With Ultrasound",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
      "summary": "12-minute read",
      "url": "https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "XRobotics' countertop robots are cooking up 25,000 pizzas a month",
      "content": "XRobotics thinks it has cracked the code on getting pizza restaurants to adopt robotics. The San Francisco-based robotics company built a countertop robot called xPizza Cube, which is roughly the size of a stackable washing machine and uses machine learning to apply sauce, cheese, and pepperoni to pizza dough. The machines, which lease for $1,300 a month for three years, can make up to 100 pizzas an hour and be retrofitted to work with pies of different sizes and styles, like Detroit and Chicago deep dish. “This saves like almost 70, sometimes 80% of the time for the staff,” Denis Rodionov, the co-founder and CEO of XRobotics, told TechCrunch. “It is just repeatable work. If you have a pepperoni pizza, you need to place 50 slices of pepperoni one by one.” XRobotics is not the only company that has tried to introduce robotics into the restaurant industry — nor the only one focused on pizza. Zume is the most notable pizza robotics company — if that can be considered its own category. The company raised more than $420 million in venture capital for its robotic pizza trucks, before pivoting to focus on sustainable packaging in 2020 and shuttering entirely in 2023. Rodionov argues that they’ve been successful where other companies haven’t because they aren’t trying to fully transform the pizza-making process, as Zume was, but rather build technology to help existing pizza makers save on time and labor. Because they are building assistive technology, as opposed to replacement tech, Rodionov said they’ve been able to keep their device small enough to fit in existing kitchens and priced at a level that pizzerias — from mom-and-pop shops to large chains, both of which the company counts as customers — could afford. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The company found this out the hard way. XRobotics launched in 2019 and introduced the first version of the technology in 2021 . Its first robot was significantly larger and could work with more than 20 toppings, and ran into the same problems as their competitors. “We did a real pilot in the restaurant with our huge machine,” Rodionov said. “We learned a lot from that, and we figured out we needed a very small, compact solution. It was a bit scary. All the numbers, all the feelings, all the gut said you need to do this, not this . And we just followed the gut and said, ‘Yeah, we would go and make a smaller version,’ and it was tremendous success.” XRobotics launched their current model in 2023. The company declined to share how many customers it has. It said its robots are producing 25,000 pizzas per month, but how many customers that translates to is hard to calculate. The startup also recently raised a $2.5 million seed round led by FinSight Ventures with participation from SOSV, MANA Ventures, and Republic Capital. Rodionov said the company will use the capital to produce more units and install more robots for customers. XRobotics is committed to the pizza industry, at least for now, Rodionov said, considering the sheer size of the market — there are more than 73,000 pizza chains in the U.S. alone . The company plans to expand to Mexico and Canada next. “I love pizza, my co-founder too,” Rodionov said. “We have tested probably any pizza in San Francisco. Also, we test pizza in New York and Chicago.” Rodionov added that Detroit-style pizza, known for its square shape and crispy cheese crust, is his favorite.",
      "summary": "4-minute read",
      "url": "https://techcrunch.com/2025/06/09/xrobotics-countertop-robots-are-cooking-up-25000-pizzas-a-month/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 658,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple Announces Foundation Models Framework for Developers to Leverage AI",
      "content": "Apple Announces Foundation Models Framework for Developers to Leverage AI Monday June 9, 2025 10:10 am PDT by Tim Hardwick Apple at WWDC today announced Foundation Models Framework, a new API allowing third-party developers to leverage the large language models at the heart of Apple Intelligence and build it into their apps. With the Foundation Models Framework, developers can integrate Apple's on-device models directly into apps, allowing them to build on Apple Intelligence. \"Last year, we took the first steps on a journey to bring users intelligence that's helpful, relevant, easy to use, and right where users need it, all while protecting their privacy. Now, the models that power Apple Intelligence are becoming more capable and efficient, and we're integrating features in even more places across each of our operating systems,\" said Craig Federighi, Apple's senior vice president of Software Engineering. \"We're also taking the huge step of giving developers direct access to the on-device foundation model powering Apple Intelligence, allowing them to tap into intelligence that is powerful, fast, built with privacy, and available even when users are offline. We think this will ignite a whole new wave of intelligent experiences in the apps users rely on every day. We can't wait to see what developers create.\" The Foundation Models framework lets developers build AI-powered features that work offline, protect privacy, and incur no inference costs. For example, an education app can generate quizzes from user notes on-device, and an outdoors app can offer offline natural language search. Apple says the framework is available for testing starting today through the Apple Developer Program at developer.apple.com, and a public beta will be available through the Apple Beta Software Program next month at beta.apple.com. It includes built-in features like guided generation and tool calling for easy integration of generative capabilities into existing apps. Tag: WWDC 2025 [ 6 comments ] Get weekly top MacRumors stories in your inbox. Popular Stories Monday June 9, 2025 9:00 am PDT by Jordan Golson Apple's Worldwide Developers Conference (WWDC) starts today with the traditional keynote kicking things off at 10:00 a.m. Pacific Time. MacRumors is on hand for the event and we'll be sharing details and our thoughts throughout the day. We're expecting to see a number of software-related announcements led by a design revamp across Apple's platforms that will also see the numbering of all of... • 1211 comments Friday June 6, 2025 7:27 am PDT by Joe Rossignol Apple is planning to announce several new features for the Messages and Phone apps on iOS 26, according to Bloomberg's Mark Gurman. In a lengthy report outlining his WWDC 2025 expectations today, Gurman said that the two main changes in the Messages app will be the ability to create polls, as well as the option to set a background image within a conversation. 9to5Mac was first to report... • 50 comments Monday June 9, 2025 10:13 am PDT by Hartley Charlton Apple today announced a complete redesign of all of its major software platforms called \"Liquid Glass.\" Announced simultaneously for iOS, iPadOS, macOS, watchOS, tvOS, visionOS, and CarPlay, Liquid Glass forms a new universal design language for the first time. At its WWDC 2025 keynote address, Apple's software chief Craig Federighi said \"Apple Silicon has become dramatically more powerful... • 229 comments Friday June 6, 2025 6:17 am PDT by Joe Rossignol While the so-called \"iPhone 17 Air\" is not expected to launch until September, there are already plenty of rumors about the ultra-thin device. Overall, the iPhone 17 Air sounds like a mixed bag. While the device is expected to have an impressively thin and light design, rumors indicate it will have some compromises compared to iPhone 17 Pro models, including worse battery life, only a single ... • 77 comments Monday June 9, 2025 11:22 am PDT by Joe Rossignol Apple today announced that iPadOS 26 will be compatible with the iPad models listed below. iPadOS 26 features a new Liquid Glass design, a menu bar, improved app windowing, and more. iPadOS 26 supports the following iPad models:iPad Pro (M4) iPad Pro 12.9-inch (3rd generation and later) iPad Pro 11-inch (1st generation and later) iPad Air (M2 and later) iPad Air (3rd generation and... • 97 comments Monday June 9, 2025 5:21 pm PDT by Juli Clover At today's WWDC 2025 keynote event, Apple unveiled a new design that will inform the next decade of iOS, iPadOS, and macOS development, so needless to say, it was a busy day. Apple also unveiled a ton of new features for the iPhone, an overhauled Spotlight interface for the Mac, and a ton of updates that make the iPad more like a Mac than ever before.",
      "summary": "2-minute read",
      "url": "https://www.macrumors.com/2025/06/09/foundation-models-framework/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 780,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "LLMs are cheap",
      "content": "LLMs are cheap Posted on 2025-06-02 in General This post is making a point - generative AI is relatively cheap - that might seem so obvious it doesn't need making. I'm mostly writing it because I've repeatedly had the same discussion in the past six months where people claim the opposite. Not only is the misconception still around, but it's not even getting less frequent. This is mainly written to have a document I can point people at, the next time it repeats. It seems to be a common, if not a majority, belief that Large Language Models (in the colloquial sense of \"things that are like ChatGPT\") are very expensive to operate. This then leads to a ton of innumerate analyses about how AI companies must be obviously doomed, as well as a myopic view on how consumer AI businesses can/will be monetized. It's an understandable mistake, since inference was indeed very expensive at the start of the AI boom, and those costs were talked about a lot. But inference has gotten cheaper even faster than models have gotten better, and nobody has an intuition for something becoming 1000x cheaper in two years. It just doesn't happen. It doesn't help that the common pricing model (\"$ per million tokens\") is very hard to visualize. So let's compare LLMs to web search. I'm choosing search as the comparison since it's in the same vicinity and since it's something everyone uses and nobody pays for, not because I'm suggesting that ungrounded generative AI is a good substitute for search. (It should also go without saying that these are just my personal opinions.) What is the price of a web search? Here's the public API pricing for some companies operating their own web search infrastructure, retrieved on 2025-05-02: The Gemini API pricing lists a \"Grounding with Google Search\" feature at $35/1k queries. I believe that's the best number we can get for Google, they don't publish prices for a \"raw\" search result API. The Bing Search API is priced at $15/1k queries at the cheapest tier. Brave has a price of $5/1k searches at the cheapest tier. Though there's something very strange about their pricing structure, with the unit pricing increasing as the quota increases, which is the opposite of what you'd expect. The tier with real quota is priced at $9/1k searches. So there's a range of prices, but not a horribly wide one, and with the engines you'd expect to be of higher quality also having higher prices. What is the price of LLMs in a similar domain? To make a reasonable comparison between those search prices and LLM prices, we need two numbers: How many tokens are output per query? What's the price per token? I picked a few arbitrary queries from my search history, and phrased them as questions, and ran them on Gemini 2.5 Flash (thinking mode off) in AI Studio: [When was the term LLM first used?] -> 361 tokens, 2.5 seconds [What are the top javascript game engines?] -> 1145 tokens, 7.6 seconds [What are the typical carry-on bag size limits in europe?] -> 506 tokens, 3.4 seconds [List the 10 largest power outages in history] -> 583 tokens, 3.7 seconds Note that I'm not judging the quality of the answers here. The purpose is just to get rough numbers for how large typical responses are. A 500-1000 token range seems like a reasonable estimate. What's the price of a token? The pricing is sometimes different for input and output tokens. Input tokens tend to be cheaper, and our inputs are very short compared to the outputs, so for simplicity let's consider all the tokens to be outputs. Here's the pricing of some relevant models, retrieved on 2025-05-02: Model Price / 1M tokens Gemma 3 27B $0.20 ( source ) Qwen3 30B A3B $0.30 ( source ) Gemini 2.0 Flash $0.40 ( source ) GPT-4.1 nano $0.40 ( source ) Gemini 2.5 Flash Preview $0.60 ( source ) Deepseek V3 $1.10 ( source ) GPT-4.1 mini $1.60 ( source ) Deepseek R1 $2.19 ( source ) Claude 3.5 Haiku $4.00 ( source ) GPT-4.1 $8.00 ( source ) Gemini 2.5 Pro Preview $10.00 ( source ) Claude 3.7 Sonnet $15.00 ( source ) o3 $40.00 ( source ) If we assume the average query uses 1k tokens, these prices would be directly comparable to the prices per 1k search queries. That's convenient. The low end of that spectrum is at least an order of magnitude cheaper than even the cheapest search API, and even the models at the low end are pretty capable. The high end is about on par with the highest end of search pricing. To compare a midrange pair on quality, the Bing Search vs. a Gemini 2.5 Flash comparison shows the LLM being 1/25th the price. Note that many of the above models have cheaper pricing in exchange for more flexible scheduling (Anthropic, Google and OpenAI give a 50% discount for batch requests, Deepseek is 50%-75% cheaper during off-peak hours). I've not included those cheaper options in the table to keep things comparable, but the presence of those cheaper tiers is worth keeping in mind when thinking about the next section... Objection! I know some people are going to have objections to this back-of-the-envelope calculation, and a lot of them will be totally legit concerns. I'll try to address some of them preemptively. Slightly different assumptions can easily lead to clawing back 10% here and 50% there. But I don't see how to bridge a 25x gap just for breaking even, let alone making the AI significantly more expensive. If you want to play around with different assumptions, there's a little calculator widget below. Surely the typical LLM response is longer than that - I already picked the upper end of what the (very light) testing suggested as a reasonable range for the type of question that I'd use web search for. There's a lot of use cases where the inputs and outputs are going to be much longer (e.g. coding), but then you'd need to also switch the comparison to something in that same domain as well. The LLM API prices must be subsidized to grab market share -- i.e. the prices might be low, but the costs are high - I don't think they are, for a few reasons. I'd instead assume APIs are typically profitable on a unit basis. I have not found any credible analysis suggesting otherwise. First, there's not that much motive to gain API market share with unsustainably cheap prices. Any gains would be temporary, since there's no long-term lock-in, and better models are released weekly. Data from paid API queries will also typically not be used for training or tuning the models, so getting access to more data wouldn't explain it. Note that it's not just that you'd be losing money on each of these queries for no benefit, you're losing the compute that could be spent on training, research, or more useful types of inference. Second, some of those models have been released with open weights and API access is also available from third-party providers who would have no motive to subsidize inference. (Or the number in the table isn't even first party hosting -- I sure can't figure out what the Vertex AI pricing for Gemma 3 is). The pricing of those third-party hosted APIs appears competitive with first-party hosted APIs. For example, the Artificial Analysis summary on Deepseek R1 hosting . Third, Deepseek released actual numbers on their inference efficiency in February. Those numbers suggest that their normal R1 API pricing has about 80% margins when considering the GPU costs, though not any other serving costs. Fourth, there are a bunch of first-principles analyses on the cost structure of models with various architectures should be. Those are of course mathematical models, but those costs line up pretty well with the observed end-user pricing of models whose architecture is known. See the references section for links. The search API prices amortize building and updating the search index, LLM inference is based on just the cost of inference - This seems pretty likely to be true, actually? But the effect can't really be that large for a popular model: e.g. the allegedly leaked OpenAI financials claimed $4B/year spent on inference vs. $3B/year on training. Given the crazy growth of inference volumes (e.g. Google recently claimed a 50x increase in token volumes in the last year ) the training costs are getting amortized much more effectively. The search API prices must have higher margins than LLM inference - It's possible. I certainly don't know what the margins of any Search API providers are, though it seems fair to assume they're pretty robust. But, well, see the point above about Deepseek's releasd numbers on the R1 profit margins. Also, it seems quite plausible that some Search providers would accept lower margins, since at least Microsoft execs have testified under oath that they'd be willing to pay more for the iOS query stream than their revenue, just to get more usage data. Web search returns results 20x-100x faster than an LLM finishes the query, how could it be more expensive? - Search latency can be improved by parallelizing the problem, while LLM inference is (for now) serial in nature. The task of predicting a single token can be parallelized, but the you can't predict all the output tokens at once. But OpenAI made a loss, and they don't expect to make profit for years! - That's because a huge proportion of their usage is not monetized at all, despite the usage pattern being ideal for it. OpenAI reportedly made a loss of $5B in 2024. They also reportedly have 500M MAUs. To reach break-even, they'd just need to monetize (e.g. with ads) those free users for an average of $10/year, or $1/month. A $1 ARPU for a service like this would be pitifully low. If the reported numbers are true, OpenAI doesn't actually have high costs for a consumer service that popular, which is what you'd expect to see if the high cost of inference was the problem. They just have a very low per-user revenue, by choice. Sensitivity analysis If you want to play around with different assumptions, here's a calculator: Open in new tab Why does this matter? I mean, you're right to ask that. Nothing really matters and eventually we'll all be dead. But it is interesting how many people have built their mental model for the near future on a premise that was true for only a brief moment. Some things that will come as a surprise to them even assuming all progress stops right now: There's an argument advanced by some people about how low prices mean it'll be impossible for AI companies to ever recoup model training costs. The thinking seems to be that it's just the prices that have been going down, but not the costs, and the low prices must be an unprofitable race to the bottom for what little demand there is. What's happening and will continue to happen instead is that as costs go down, the prices go down too, and demand increases as new uses become viable. For an example, look at the OpenRouter API traffic volumes , both in aggregate and in the relative share of cheaper models. This post was mainly about APIs, but consumer usage will have exactly the same cost structure, just a different monetization structure. And given how low the unit costs must be, advertising isn't merely viable but lucrative. From this it follows that the financials of frontier AI labs are a lot better than some innumerate pundits would have you believe. They're making a loss because they're not under pressure to be profitable, and aren't actively trying to monetize consumer traffic yet. This could well be a land grab unlike APIs, since unpaid consumer queries may be used for training while paid API queries typically are not. Even the subscription pricing might be there mainly for demand management rather than trying to run a profit. The real cost problem isn't going to be with the LLMs themselves, it's with all the backend services that AI agents will want to access if even a rudimentary form of the agentic vision actually materializes. Running the AI is already cheap, will keep getting cheaper, and will always have a monetization model of some sort since it's what the end user is interacting with. Neither of those is true for the end-user services that have been turned into AI backends without their consent. An AI trying to, I don't know, book concert tickets whenever a band I like plays in my town will probably be phenomenally expensive to its third-party backends (e.g. scraping ticket sites). Those sites will be uncompensated for the expense while also removing their actual revenue streams. I don't really know how that plays out. Obviously many service owners will try to make unauthorized scraping harder, but that's a very hard problem to solve on the web. Maybe some of them give up on the web entirely, and move to mobile where they can at least get device attestations. Some might just give up on the open web, and require all usage to be signed in, with account creation being gated on something scarce. Some might become unviable and close up shop entirely. If/when that happens, what's the play on the AI agent side? Will they choose an escalating adversarial arms race with increasingly dodgy tactics, or will they eventually decide that it's better to pay for the services they use? The former seems unsustainable. If the latter, then it feels like the core engineering challenge becomes one of building data provider backends optimized specifically for AI use, with the goal of scaling to massive volumes and cheaper unit prices, with the trade-off being higher latency, lower reliability and lower quality. That could be quite interesting from a systems perspective. (Yes, I'm aware of MCP , but it's a solution to an orthogonal issue.) But one thing I'm confident won't be happening is that it's the AIs that turn out to be too expensive to run. Additional reading Below are some additional references that were not worked into the main narrative (this article was long-winded enough already). Inference economics of language models (2025) - A mathematical model for estimating the cost structure, latency/cost tradeoffs, optimal cluster size, and optimal batching based on the LLM architecture. LLM Inference Economics from First Principles - (2025) A very detailed cost-per-token computation on the cost structure of one specific model, LLama 3.3 70B. Observations About LLM Inference Pricing - (2025) Analysis of the economics driven by pricing data rather than first-principles cost structure; concludes that proprietary models have very significant markups. Large Language Models Search Architecture And Cost - (2023) Analysis on the cost of integrating LLMs into search; the LLM cost data is no longer very relevant due to the age of the article (GPT-3.5) but it uses a different way of estimating the search cost structure. If you liked this and want to be notified of new posts, follow me on",
      "summary": "16-minute read",
      "url": "https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 2530,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "One Man Armies",
      "content": "One Man Armies In an overdecorated office somewhere in California, there is someone being paid so much money they could take six months of their salary, move to Brazil, and never work another day in their life — and yet this person is getting a remarkably little amount of work done every day. Just down the street, in a similar-looking office, there is a similar-looking person who is producing astonishing work at a breakneck pace. The variation in human output is kind of insane. What some may consider impossible without a team and $5M can sometimes be built by a single person who cares a lot and works hard. It’s easy to understimate just how much a one person army can achieve. So here are some of our favorite one-person (or mostly one-person) projects. We’re not including things that you wouldn't be surprised one person made, like a music album or a novel, though those are also very impressive. Stardew Valley was made by Eric Barone , who put in 70 hours a week for 4.5 years before releasing the game for the first time. He still updates the game with loads of new content. Linux was created by Linus Torvalds . He is not responsible for all of Linux and did not do everything single handedly, but he was its origin (which was a lot of work). Minecraft, before it sold to Microsoft, was developed by Markus Persson . It took him 2 years! The Art of Computer Programming is a massive tome of a lot of knowledge about computer programming and algorithms and math. It’s used by all sorts of people (from scientists to mathematicians to linguists) and it is not yet done. The one person behind this book is Donald Knuth . The Sagrada Familia was designed by Antonio Gaudí , who spent more than 40 years of his life on it. The first scientific map of the Atlantic Ocean floor, which discovered the rift valley in the Mid-Atlantic ridge and was foundational evidence for the plate tectonics theory, was created solo by Marie Tharp. Bruce Heezen, a colleague, collected the raw data as women were not allowed to work on ships at the time. The Coral Castle, a kind of weird-but-cool garden of structures in Florida, was built singlehandedly by a Latvian immigrant named Edward Leedskalnin . When asked how he moved such heavy rocks on his own for his structures, he said he knew the secrets of those who built the Pyramids. The first version of a computer program was written in the 1800s by Ada Lovelace . The Cathedral of Justo in Spain was designed and built almost exclusively by Justo Gallego Martinez . He worked on it for the better part of his life, all the way up until he died. Rollercoaster Tycoon, a game you should definitely play, was made by Chris Sawyer . Kenshi, a complex RPG game, was made in 12 years by Chris Hunt . The Theory of General Relativity, as you probably know, was created by this guy . jQuery was made by one person: John Resig . Fun fact, he works at Khan Academy as of this writing. Christianity, though it later spread in many other ways, was pioneered by Jesus of Nazareth . Islam, though it also later spread in many ways, was pioneered by Muhammad ibn Abdullah . Light is a series of seven operas, one for each day of the week. It takes nearly 30 hours to perform. It was composed over decades by Karlheinz Stockhausen . 24 Preludes, Op. 28 by Frédéric Chopin is a project that includes one prelude for each key. He wrote it entirely on his own. Many of these pieces were masterpieces and pushed the boundaries of piano at the time. Beethoven, Liszt, Rachmaninov, Mozart, Bartók, Bach, Scriabin, Debussy, and the list goes on — many of the most influential composers also put together most of their ambitious projects solo. You can take away whatever you want from this list. One takeaway is that it is extremely impressive. If the Theory of General Relativity, entire cathedrals, 29-hour operas, and billion-dollar video games can be made by a single person, your own grand and seemingly unrealistic ideas may not be too ambtious, either. And, if you care enough about them, they might be worth trying. It sounds a bit like a feel-good platitude—but it may also be true. If you are looking for a list of ideas to try, you could read this. * * * Enjoy these essays? Enter your email and we'll let you know when we post a new one. No spam, just essays. Or, if you have any feedback,",
      "summary": "4-minute read",
      "url": "https://quarter--mile.com/One-Man-Armies?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 783,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "The SignalFire State of Talent Report - 2025",
      "content": "’s talent gap widens: new grad hiring plummets, Texas loses startup talent, and Anthropic's 80% retention dominance. Over the last couple of years, we’ve seen companies rewrite the playbook for hiring AI and tech talent. In a landscape defined by fierce competition and rapid change, our latest tech talent report zeroes in on the bold moves, early trends, and strategic pivots reshaping how companies attract and retain top-tier talent in 2025 and beyond. At SignalFire, our Beacon AI platform tracks over 650 million professionals and 80 million organizations to give us a front-row seat to the talent transformations reshaping the industry. Our latest report reveals some sharp shifts: - Entry-level hiring is collapsing - A generational hiring shift is leaving new graduates behind - Elite AI labs are locking in top talent, and retaining them - Geographic power centers are evolving fast While headlines continue to highlight labor market shifts amidst political change and intense AI rivalry among tech giants, our data reveals a more nuanced story. This isn’t just about layoffs or remote work. It’s about a fundamental reset in how, where, and who companies are hiring to build the next generation of technology. #1 Tech's lost generation? New grad hiring drops 50% compared to pre-pandemic levels The tech world has long been synonymous with innovation, breakneck growth, and boundless opportunities. The door to tech once swung wide open for new grads. Today, it’s barely cracked. The industry's obsession with hiring bright-eyed grads right out of college is colliding with new realities: smaller funding rounds, shrinking teams, fewer new grad programs, and the rise of AI. Everyone took a hit in 2023, but while hiring bounced back in 2024 for mid- and senior-level roles, the cut keeps getting deeper for new grads: Big Tech: New grads now account for just 7% of hires, with new hires down 25% from 2023 and over 50% from pre-pandemic levels in 2019. Startups: New grads make up under 6% of hires, with new hires down 11% from 2023 and over 30% from pre-pandemic levels in 2019. ‎As budgets tighten and AI capabilities increase, companies are reducing their investment in new grad opportunities. The latest data from the Federal Reserve Bank of New York shows that the unemployment rate for new college grads has risen 30% since bottoming out in September 2022, versus about 18% for all workers. The perception gap isn’t helping— 55% of employers say Gen Z struggles with teamwork, and 37% of managers said they’d rather use AI than hire a Gen Z employee. Even top computer science grads aren’t spared. As demand for junior roles declines, even highly credentialed engineering grads are struggling to break into tech, especially at the Big Tech companies. The share of new graduates landing roles at the Magnificent Seven (Alphabet, Amazon, Apple, Meta, Microsoft, NVIDIA and Tesla) has dropped by more than half since 2022. ‎The experience paradox It’s not just a hiring slowdown, it’s a shift in expectations. Today’s tech employers aren’t looking for potential, they’re looking for proof. That’s left new grads stuck in a Catch-22: you need experience to get the job, but you need the job to get experience. In a world of leaner teams and tighter budgets, there’s little room for investment in training. As the pipeline for new talent shrinks at breakneck speed, it’s creating fierce competition for the few entry-level roles that remain. The cruel irony? Companies are posting junior roles but filling them with senior individual contributors (ICs)—a phenomenon known as the experience paradox . Series A squeeze or AI hype: What’s really stalling Gen Z hiring? AI gets a lot of blame for wiping out junior roles post-2022, but the real story is more nuanced, as you can see from the 200+ comments on this Hacker News post and this recent article from The Atlantic . Yes, automation is already replacing some routine tasks , but the bigger driver may be the end of the “free money madness” driven by low interest rates that we saw in 2020-2022, along with the overhiring and inflation it led to. Now, with tighter budgets and shorter runways, companies are hiring leaner and later. Carta data shows that Series A tech startups are 20% smaller than they were in 2020. This shift isn’t just about hiring less—it’s a hiring reset. As AI tools take over more routine, entry-level tasks, companies are prioritizing roles that deliver high-leverage technical output. Big Tech is doubling down on machine learning and data engineering, while non-technical functions like recruiting, product, and sales keep shrinking, making it especially tough for Gen Z and early-career talent to break in. But despite the hype, AI hasn’t wiped out entire job categories—yet. So far, the fallout has hit new grads hardest, while demand for experienced engineers is still rising. As AI capabilities continue to grow, the real differentiator will be talent, and no one’s playing the game harder than the top AI labs . #2 AI Labs: Anthropic sets the pace in the talent race November 2022 didn’t just mark the launch of ChatGPT—it kicked off the AI talent race. We analyzed retention across top AI labs, and one clear leader emerged: Anthropic. An impressive 80% of employees hired at least 2 years ago at Anthropic were still at the company at the end of their second year—a figure that stands out in an industry known for high turnover. DeepMind follows closely at 78%, while OpenAI’s retention trails at 67% but remains on par with large FAANG companies like Meta (64%). With outsized salary packages at DeepMind and even Senate scrutiny over Big Tech talent poaching, the AI talent war is real, and the spoils go to those who can attract and keep the best minds. Anthropic’s winning formula and where they’re hiring from: Anthropic’s 80% retention isn’t just impressive—it’s a strategic advantage. In a field where turnover means lost time and money, keeping top talent consistently is a game-changer. And it’s not just about keeping talent, it’s about winning it. Talent poaching with precision: Anthropic is siphoning top talent from two of its biggest rivals: OpenAI and DeepMind. Engineers are 8 times more likely to leave OpenAI for Anthropic than the reverse. From DeepMind, the ratio is nearly 11:1 in Anthropic’s favor. Some of that’s expected—Anthropic is the hot new startup, while DeepMind’s larger, tenured team is ripe for movement. But the scale of the shift is striking. Beyond the salary: While Big Tech leans on big paychecks and brand cachet, Anthropic’s edge is its unique culture that embraces unconventional thinkers and gives employees true autonomy to drive impact. Flexible work options, no title politics or forced management tracks—just clear paths for career growth. Employees say Anthropic embraces intellectual discourse and researcher autonomy, making it a magnet for AI talent stifled by bureaucracy elsewhere. Claude is quickly emerging as a favorite among developers by several informal measures , and that kind of affinity can influence career decisions. Engineers often gravitate toward companies whose products they admire and use, and that perceived product resonance may be giving Anthropic an edge in recruiting. It’s not just rival AI labs feeling the pull— Big Tech has become a prime hunting ground, too . Google, Meta, Microsoft, Amazon, and Stripe serve as primary talent pools for AI labs, and Anthropic has been particularly successful at poaching senior researchers and engineers from these companies.‎ #3 Hot spots, cooling zones, and America’s new tech hubs The tech talent map continues to shift. While San Francisco and New York remain dominant, rising hubs like Toronto, Miami, and San Diego are gaining ground. Meanwhile, Texas cities like Austin are losing momentum, prompting founders to rethink hiring and compensation strategies. More companies are embracing hub-and-spoke models and tailoring compensation philosophies to ensure they secure the right talent mix across diverse locations. Key trends shaping the new geographic playbook Hiring is slowing in most metros, but we’re seeing a reconsolidation of tech jobs around major tech hubs like the Bay Area, Seattle, and NYC. Here are the key geographic trends we saw in 2024: 1. Silicon Valley and New York City hold strong: - High talent, high cost: SF and NYC still anchor the AI ecosystem, with over 65% of AI engineers based in these two metros. Despite rising housing costs, shrinking salary advantages, and remote work flexibility, SF and NY continue to attract more tech talent. 2. The sunshine surge - Miami and San Diego are rising fast: These two cities are attracting tech talent not with massive budgets, but with sun, lifestyle, and lower living costs. - Miami’s mix of tax perks and quality of life has fueled a 12% jump in AI roles, according to recent hiring data. - San Diego saw a 7% rise in Big Tech roles, even as startups in the region lost 3.5% of their workforce in 2024, suggesting talent is being poached upward. In 2024, San Diego County startups raised $5.7 billion in venture capital, marking one of the region's best-performing years on record. 3. Texas cools off: Are Austin and Houston losing their luster? Once the darlings of tech growth, Austin and Houston have been losing startup talent. Lagging infrastructure, a cultural mismatch, fluctuating housing costs, and a renewed emphasis on hybrid RTO policies are motivating startup employees to live closer to traditional hubs. Last year, Austin, which was a post-pandemic growth leader, saw a 6% drop in headcount at VC-backed startups. Houston's drop is even steeper at 10.9%. This isn’t just a story about regional reshuffling—it’s a signal of something bigger. The media’s return-to-office narrative is overstated, and companies are rethinking what really matters. The new model? Proximity over presence. For many tech companies, it’s not about clocking in five days a week—it’s about being close enough for hybrid schedules and anchor days. The result: a surge in in-state hiring as employers strike a new balance between flexibility and face time. #4 Hits and misses from 2024 and predictions for 2025 Some of our predictions from last year’s report held strong while others veered off course. Here’s what we got right—and what we predict is next. 2024: What we saw coming: Fractional work stuck around: Fractional roles are thriving—even at the top. CMOs, CFOs, and CTOs are increasingly working as consultants. Whether this trend holds in a stronger market remains to be seen, but for now, this model seems to be working for companies and execs alike. Cybersecurity kept growing: AI-powered threats are on the rise , and the demand for cybersecurity talent has continued to grow as predicted. Compensation is up, roles are harder to fill, and hiring urgency is higher than ever. Remote work didn’t die—it evolved: The RTO debate rages on, but the reality is more nuanced. Companies are increasingly adopting hybrid models while talent continues to demand flexibility. We won’t know the long-term balance until the next economic cycle resets the supply-demand equation. Our predictions for 2025: The rise of the generalist engineer: Specialists won the last decade. Generalists may win the next. With tools like Copilot, Replit, and Cursor maturing, engineers don’t need deep ML expertise to build with AI. Companies will prioritize flexible, collaborative, generalist engineers who can move fast and collaborate effectively with powerful tools—no PhD required. The year of the equity advisor: With lean startups hesitant to hire for both junior and C-suite roles, founders will tap seasoned experts as equity advisors. Carta data shows these roles are more affordable than before, offering startups a low-cost way to gain experience and mentorship without inflating their burn rate. New jobs—not just disappearing ones: While headlines warn of jobs lost to AI, we see another shift: new roles emerging. Expect to see titles like AI governance lead, AI ethics and privacy specialists, agentic AI engineers, and non-human security ops specialists become commonplace. It’ll take time to scale, but these are some of the roles new grads should be paying attention to. People, not just technology, will shape 2025 and beyond The past year made one thing clear: technology alone doesn’t build the future, people do. Anthropic’s retention edge and the reshaping of talent hubs prove that the real advantage lies in how you hire, grow, and keep great talent. What it means for the road ahead: - For new grads: The training wheels are gone. With fewer entry-level roles, the path forward will rely on bootcamps, open-source, freelancing, and creative projects. It’s not enough to just master the latest AI tools; learn to fix their flaws—debugging messy, machine-generated code may be your superpower. - For employers: AI might reduce the short-term need for junior hires, but skipping them entirely risks breaking the long-term talent pipeline. The industry’s future depends on equipping the next generation with skills that grow alongside the evolving technology landscape. ( Read our article on how to build a multi-generational company here .) ________________________ Methodology Note: This report uses data from our proprietary Beacon AI platform, an intelligence engine that tracks 650+ million individuals and 80+ million organizations. We analyze millions of data points on hiring trends, geographic movements, and more to spot emerging talent and help our portfolio companies build teams and products faster. Here's the approach we used to analyze the data for this report: - “Big tech” represents the top 15 technology companies by market cap. - “Startups” represents companies funded by the Top 100 VC firms that closed a Seed through Series C round in the previous 4 years. - “Top computer science graduates” represents graduates from the top 20 engineering programs in the U.S. according to the U.S. News’ Best Undergraduate Engineering Programs Rankings. - For the AI Labs retention analysis, we excluded some newer AI labs (like DeepSeek and xAI/Grok) because they had not been operating over the whole time period we looked at (2023-2024). We focused on entities with distinct",
      "summary": "17-minute read",
      "url": "https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 2308,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 2308/3400 words (67.9%)"
    },
    {
      "title": "AI's metrics question",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
    },
    {
      "title": "The Code Review Frustration",
      "content": "Vibe code isn't meant to be reviewed: how to stay in control of codebase and not lose vibe code productivity boost 76 views > Disclaimer: The views and opinions expressed in this post are those of the author and do not necessarily reflect the official position of Monadical. Any content provided is for informational purposes only. ## The Code Review Frustration Another day, another slop. Chasing 10x productivity, you run several Claude code agents simultaneously and push code with blazing fast ferocity. No juniors left on the team — they just couldn't catch up. This morning, walking in the park with his dog, your team lead wrote and deployed a 100-story-point system just by casually talking on his phone to a remote agent. Meanwhile, you're stuck with a code review: ![](https://docs.monadical.com/uploads/042fee53-f1f2-40ba-9e3a-0e65fed1ee4a.png) \"It's fucking vibe code, I don't care!\" — this sentiment is just the tip of the iceberg of a current industry problem: <center><big>Treating all code the same when it fundamentally isn't</big></center> ## The Ownership Trap Here's the brutal reality: **the moment you start treating AI-generated code as \"precious,\" you lose AI's biggest superpower.** Once you've spent time reviewing, fixing, and improving that generated code, you become invested. You're going to be extra careful about AI \"breaking\" it. And it's ok, some code should be like this! But many times you just want to vibe and have your 10x productivity dream come true. Treating vibe code as precious is the productivity killer nobody talks about. You get the initial speed boost, then gradually slide back to normal development velocity as your codebase fills up with \"improved vibe code\" that you're reluctant to throw away. And you aren't ready to regenerate it from scratch anymore — an LLM skill that it excels at sometimes when it's stuck with a loop of never ending edits. Meanwhile, every code review becomes a battle between two conflicting mental models: - Reviewer: \"This could be cleaner\" (treating it like human code) - Author: \"It works, who cares\" (treating it like disposable vibe code) **The industry needs a way to keep these two types of code separate.** ## The Modular Solution: Giving Form to Chaos The solution is neither to abandon AI coding nor to accept messy codebases. It's to **architect the separation explicitly.** Think of it like this: Human code provides the \"form\" or \"mold\" that vibe code must fill. Just like the inpainting/outpainting feature in image generation. The human code contains your domain knowledge, business logic, and architectural decisions. The vibe code is just the implementation details that make it work. When AI code is clearly separated and constrained by interfaces, tests, and clear boundaries, you can also regenerate it fearlessly while keeping your valuable insights intact. ### The Overseer Package Approach #### The High-Level View Before scaring managers and leads with implementation details, here's the conceptual framework: 1. **Interface packages** - Define contracts, data shapes, and the most important tests (human-written) 2. **Implementation packages** - Fulfill those constraints (Vibe-generated, marked as @vibe-coded in README or in files) 3. **Clear dependency direction** - Implementation depends on interfaces, never the reverse 4. **Regeneration freedom** - Any @vibe-coded package can be deleted and rewritten without fear This creates a \"constraint sandwich\" - your domain knowledge stays protected while AI handles the tedious implementation work. > Technical implementation example awaits you in one of the last paragraphs. ## Two Flavors of Review: Classic vs YOLO With the modular approach and vibe code as \"second class citizen,\" we can now reframe code review practices: ### **Classic review: High standards, educational, expertise-building** Continue your nitpicky craftsmanship, talk about loops vs. combinators, and document your findings into project code style for LLMs to conform to. Share knowledge, discover new details about the system. ### **YOLO review**: \"Does it work? Does it pass tests? **Doesn't it sneak around the overseer package requirements**? Does it look safe enough? Ship it.\" <center><big>Clear separation of code \"types\" eliminates misunderstanding and friction</big></center> ## The Control Paradox Solved There are deeper psychological and sociological nth-order benefits to this approach. ### Professional Confidence When someone asks about a feature, you want to give an answer. \"I don't know, I vibed it\" destroys professional credibility. \"Cursor can't find the answer today, try again tomorrow\" makes you incompetent. With explicit separation, you can confidently say: \"The business logic is in the interface packages - here's exactly how it works. The implementation details are auto-generated, but the core logic is solid.\" ### Competitive Advantage While others choose between \"fast and messy\" or \"slow and clean,\" you get both. Your company's competitors using 100% vibe coding will hit complexity walls. Your company's competitors avoiding AI will be slower. You'll maintain AI productivity gains while building systems that actually scale. ### Better \"Boomer Business\" Adoption I believe that this approach could tip the scale for businesses who are still indecisive about vibe coding due to reliability and security concerns. ## Looking Forward: The Tooling Evolution I strongly believe that in the near future, the distinction between vibe code and human code will be admitted by industry and integrated into existing tools. ### Git-Level Integration Git commits automatically tagged as vibed. GitHub PRs showing clear visual distinction between human and AI contributions, up to the code line level. ### AI Agent Constraints Future coding agents will have an option to respect \"human code zones\" - like content-aware inpainting for image generation, but for code. Agents could regenerate entire implementations, not only files or packages, but code line-wise, leaving human guidance code untouched. ### IDE Evolution Syntax highlighting that dims unchecked vibe code while emphasizing human code. Folding options that hide implementation details. Search that prioritizes guidance, domain, architectural code. ### Corporate Adoption This separation makes AI coding auditable and controllable - exactly what the bloody enterprise needs. CTOs can require that all business logic lives in human-controlled packages while allowing rapid development in implementation packages. **So the idea of vibe code separation isn't just about individual productivity. It's about making AI coding enterprise-ready.** ## Technical Implementation in TypeScript One of the ways to split vibe and human code using current tooling that I found is a per-package approach. It's easy to do with a monorepo, but another structural or file-naming convention could work well too. For the case of monorepo, I used the \"-interface\" packages that contain concentrated domain knowledge and shape-setting code (tests, interfaces). I used dependency injection to draw more explicit frontiers between modules. ```ts export type GenerateAndSyncTasksDeps = { taskmaster: { generateTasks: ReturnType<GenerateTasksF>; }; tasktracker: { syncTasks: ReturnType<SyncTasksF>; }; }; export const generateAndSyncTasks = (di: GenerateAndSyncTasksDeps) => async (prd: PrdText) => { const tasks = await di.taskmaster.generateTasks(prd); return await di.tasktracker.syncTasks(tasks.tasks); }; ``` This is the \"entry point\" of a module that receives a PRD document, uses https://github.com/eyaltoledano/claude-task-master to generate tasks, and then syncs them to a task tracker. You can tell the coding agent to pick up from there, but it won't have enough guidance yet. Therefore, \"we need to go deeper.\" That's an example of how I defined the interface for taskmaster.generateTask, in its own package: ```ts export type GenerateTasksDeps = { savePrd: (path: NonEmptyString, prd: PrdText) => Promise<AsyncDisposable>; cli: { generate: ( prdPath: NonEmptyString, tasksJsonPath: NonEmptyString ) => Promise<TasksFileContent>; }; readTasksJson: (tasksJsonPath: NonEmptyString) => Promise<TasksFileContent>; }; export type GenerateTasksF = ( deps: GenerateTasksDeps ) => ( prd: PrdText, current: Option.Option<TasksFileContent> ) => Promise<TasksFileContent>; export const generateTasks: GenerateTasksF = (deps) => async (prd, current) => { if (Option.isSome(current)) { throw new Error(\"panic! PRD update not implemented\"); } const prdPath = castNonEmptyString(\"scripts/prd.txt\"); // not obvious: taskmaster CLI wants the prd first saved in file system await using _letFileGo = await deps.savePrd(prdPath, prd); const outputPath = castNonEmptyString(\"tasks/tasks.json\"); await deps.cli.generate(prdPath, outputPath); // don't clean up here // we read file system after CLI ran to return parsed tasks.json return await deps.readTasksJson(outputPath); }; ``` Past this point, it's already possible to tell Claude Code to generate the `GenerateTasksDeps` providing code that calls the CLI, saves and reads from the file system. Important details that we want to be preserved - \"PRD file is temporary and we want to have it in the file system before calling CLI,\" \"we also want to read the result of CLI call from the file system\" are well-preserved as strong contextual harness for LLM code. Data shape definitions are also a great candidate to use as controlling code: ```ts export const TaskFileContent = Schema.Struct({ id: TaskId, title: Schema.NonEmptyString, description: Schema.String, status: TaskStatus, dependencies: Schema.Array(TaskId), priority: Schema.optional(Schema.String), details: Schema.String, testStrategy: Schema.String, subtasks: Schema.Array(SubtaskFileContent), }); ``` Interfaces too: ```ts export interface TasksService { list: (filters?: { project?: ProjectId; status?: StatusId; user_story?: UserStoryId; }) => Promise<readonly TaskDetail[]>; create: (task: CreateTaskRequest) => Promise<TaskDetail>; get: (id: TaskId) => Promise<TaskDetail>; update: (id: TaskId, task: UpdateTaskRequest) => Promise<TaskDetail>; delete: (id: TaskId) => Promise<void>; } ``` And of course, unit tests are a great candidate for putting into controlling packages, especially [property-based tests](https://monadical.com/posts/property-based-testing-for-temporal-graph-storage.html). Also, you can put there all the code that you *could* 100% vibe but better *don't* - that you're supposed to know, if not by heart, then at least \"at some point when you wrote/reviewed it.\" Tell the agent to conform to those interfaces, test and shape-setting functions, writing the \"-implementation\" package counterpart to your \"-interface.\" An example system query: https://github.com/Monadical-SAS/taiga-taskmaster/blob/master/.llm-docs/PACKAGE_ORGANISATION_AND_CODE_SEPARATION_STANDARDS.md To me, it worked handsomely. The agent was very strong at regenerating code anew if I didn't like something and wanted to add more context to the \"control packages.\" And it never loses any context because of strict conformance to controlling code. And for the PR process, it now becomes clear which code is worth more attention and which you can just glance over, which unlocks much more of vibe power without compromising quality. Igor Loskutov is a Slop Enchanter of Monadical Recent posts Vibe code isn't meant to be reviewed * The Scraping-With-Cookies Dilemma Conversations are the New Oil Don't Give Big Tech Your Papaya View more posts... Back to top Let's transform your organization. BOOK A FREE AI STRATEGY CALL",
      "summary": "7-minute read",
      "url": "https://monadical.com/posts/vibe-code-how-to-stay-in-control.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 1660,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Silicon Valley's quest to remove friction from our lives",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
      "summary": "20-minute read",
      "url": "https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "A bit more on Twitter/X's new encrypted messaging",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
      "summary": "24-minute read",
      "url": "https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "crypto",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Everything Apple Plans to Show at Its iOS 26-Focused WWDC 2025 Event",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
      "summary": "10-minute read",
      "url": "https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Microsoft and Asus announce two Xbox Ally handhelds with new Xbox full-screen experience",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Breakthrough in search for HIV cure leaves researchers ‘overwhelmed'",
      "content": "A cure for HIV could be a step closer after researchers found a new way to force the virus out of hiding inside human cells. The virus’s ability to conceal itself inside certain white blood cells has been one of the main challenges for scientists looking for a cure. It means there is a reservoir of the HIV in the body, capable of reactivation, that neither the immune system nor drugs can tackle. Now researchers from the Peter Doherty Institute for Infection and Immunity in Melbourne, have demonstrated a way to make the virus visible, paving the way to fully clear it from the body. It is based on mRNA technology, which came to prominence during the Covid-19 pandemic when it was used in vaccines made by Moderna and Pfizer/BioNTech. In a paper published in Nature Communications , the researchers have shown for the first time that mRNA can be delivered into the cells where HIV is hiding, by encasing it in a tiny, specially formulated fat bubble. The mRNA then instructs the cells to reveal the virus. Globally, there are almost 40 million people living with HIV, who must take medication for the rest of their lives in order to suppress the virus and ensure they do not develop symptoms or transmit it. For many it remains deadly, with UNAids figures suggesting one person died of HIV every minute in 2023. It was “previously thought impossible” to deliver mRNA to the type of white blood cell that is home to HIV, said Dr Paula Cevaal, research fellow at the Doherty Institute and co-first author of the study, because those cells did not take up the fat bubbles, or lipid nanoparticles (LNPs), used to carry it. The team have developed a new type of LNP that those cells will accept, known as LNP X. She said: “Our hope is that this new nanoparticle design could be a new pathway to an HIV cure.” When a colleague first presented test results at the lab’s weekly meeting, Cevaal said, they seemed too good to be true. “We sent her back into the lab to repeat it, and she came back the next week with results that were equally good. So we had to believe it. And of course, since then, we’ve repeated it many, many, many more times. “We were overwhelmed by how [much of a] night and day difference it was – from not working before, and then all of a sudden it was working. And all of us were just sitting gasping like, ‘wow’.” Further research will be needed to determine whether revealing the virus is enough to allow the body’s immune system to deal with it, or whether the technology will need to be combined with other therapies to eliminate HIV from the body. The study is laboratory based and was carried out in cells donated by HIV patients. The path to using the technology as part of a cure for patients is long, and would require successful tests in animals followed by safety trials in humans, likely to take years, before efficacy trials could even begin. “In the field of biomedicine, many things eventually don’t make it into the clinic – that is the unfortunate truth; I don’t want to paint a prettier picture than what is the reality,” stressed Cevaal. “But in terms of specifically the field of HIV cure, we have never seen anything close to as good as what we are seeing, in terms of how well we are able to reveal this virus. “So from that point of view, we’re very hopeful that we are also able to see this type of response in an animal, and that we could eventually do this in humans.” Dr Michael Roche of the University of Melbourne and co-senior author of the research, said the discovery could have broader implications beyond HIV, with the relevant white blood cells also involved in other diseases including cancers. Dr Jonathan Stoye, a retrovirologist and emeritus scientist at the Francis Crick Institute, who was not involved in the study, said the approach taken by the Melbourne team appeared be a major advance on existing strategies to force the virus out of hiding, but further studies would be needed to determine how best to kill it after that. He added: “Ultimately, one big unknown remains. Do you need to eliminate the entire reservoir for success or just the major part? If just 10% of the latent reservoir survives will that be sufficient to seed new infection? Only time will tell. “However, that does not detract from the significance of the current study, which represents a major potential advance in delivery of mRNA for therapeutic purposes to blood cells.” Prof Tomáš Hanke of the Jenner Institute, University of Oxford, disputed the idea that getting RNA into white blood cells had been a significant challenge. He said the hope that all cells in the body where HIV was hiding could be reached in this way was “merely a dream”.",
      "summary": "2-minute read",
      "url": "https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 834,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "BYD's Five-Minute Charging Puts China in the Lead for EVs",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Field Notes From Shipping Real Code With Claude",
      "content": "June 7, 2025 Field Notes From Shipping Real Code With Claude Vibe Coding Isn’t Just a Vibe Shimmering Substance - Jackson Pollock Note : This post comes with a NotebookLM podcast ( 1 linked at the bottom), and three generated audio recordings. You can read the conversation I had with ChatGPT while preparing drafts of this post. Comments and discussion on the related HN post . Audio Think of this post as your field guide to a new way of building software. By the time you finish reading, you’ll understand not just the how but the why behind AI-assisted development that actually works. Here’s What You’re Going to Learn First, we’ll explore how to genuinely achieve a 10x productivity boost—not through magic, but through deliberate practices that amplify AI’s strengths while compensating for its weaknesses. Next, I’ll walk you through the infrastructure we use at Julep to ship production code daily with Claude’s help. You’ll see our CLAUDE.md templates, our commit strategies, and guardrails. Most importantly, you’ll understand why writing your own tests remains absolutely sacred, even (especially) in the age of AI. This single principle will save you from many a midnight debugging sessions. This is the main insight: Good development practices aren’t just nice-to-haves—they’re the difference between AI that amplifies your capabilities versus your chaos. The research bears this out. 2 Teams using rigorous practices deploy 46 times more frequently and are 440 times faster from commit to deployment. This effect is even more pronounced when you add capable AI assistants into the mix. Why This Post Exists: From Meme to Method Let me take you back to when this all started. 3 Andrej Karpathy 4 tweeted about “vibe-coding”—this idea of letting AI write your code while you just vibe. The developer community had a good laugh. It sounded like the ultimate developer fantasy: kick back, sip coffee, let the machines do the work. The birth of “vibe coding” Then Anthropic released Sonnet 3.7 and Claude Code , and something unexpected happened. The joke stopped being funny because it started being… possible? Of course, our trusty friend Cursor had been around awhile but this new interface finally felt like true vibe coding . At Julep , we build AI workflow orchestration. Our backend has years of accumulated decisions, patterns, and occasional technical debt. We have taken the utmost care to keep code quality high, and ample documentation for ourselves. However, the sheer size, and historical context of why different parts of the code are organized the way they are takes weeks for a good engineer to grok. Without proper guardrails when using Claude, you’re basically playing whack-a-mole with an overeager intern. Understanding Vibe-Coding ‘pls fix’ 5 Steve Yegge brilliantly coined the term CHOP —Chat-Oriented Programming in a slightly-dramatic-titled post “The death of the junior developer” . It’s a perfect, and no-bs description of what it’s like to code with Claude. Think of traditional coding like sculpting marble. You start with a blank block and carefully chisel away, line by line, function by function. Every stroke is deliberate, every decision yours. It’s satisfying but slow. Vibe-coding is more like conducting an orchestra. You’re not playing every instrument—you’re directing, shaping, guiding. The AI provides the raw musical talent, but without your vision, it’s just noise. There are three distinct postures you can take when vibe-coding, each suited to different phases in the development cycle: AI as First-Drafter : Here, AI generates initial implementations while you focus on architecture and design. It’s like having a junior developer who can type at the speed of thought but needs constant guidance. Perfect for boilerplate, CRUD operations, and standard patterns. AI as Pair-Programmer : This is the sweet spot for most development. You’re actively collaborating, bouncing ideas back and forth. The AI suggests approaches, you refine them. You sketch the outline, AI fills in details. It’s like pair programming with someone who has read every programming book ever written but has never actually shipped code. AI as Validator : Sometimes you write code and want a sanity check. AI reviews for bugs, suggests improvements, spots patterns you might have missed. Think of it as an incredibly well-read code reviewer who never gets tired or cranky. Instead of crafting every line, you’re reviewing, refining, directing. But—and this cannot be overstated—you remain the architect. Claude is your intern with encyclopedic knowledge but zero context about your specific system, your users, your business logic. The Three Modes of Vibe-Coding: A Practical Framework After months of experimentation and more than a few production incidents, I’ve settled on three distinct modes of operation. Each has its own rhythm, its own guardrails, and its own use cases. Mode 1: The Playground Lighter Fluid When to use it : Weekend hacks, personal scripts, proof-of-concepts, and those “I wonder if…” moments that make programming fun. In Playground Mode , you embrace the chaos. Claude writes 80-90% of the code while you provide just enough steering to keep things on track. It’s liberating and slightly terrifying. Pro Tip: check out claude-composer for going full-YOLO mode. Here’s what Playground Mode looks like: You have an idea for a script to analyze your Spotify history. You open Claude, describe what you want in plain English, and watch as it generates a complete solution. No CLAUDE.md file, no careful prompting—just raw, unfiltered AI-written code. The beauty of Playground Mode is its speed. You can go from idea to working prototype in minutes. The danger is that this cowboy coding style is absolutely inappropriate for anything that matters. Use it for experiments, never for production. Trust me, while the amazing folks preaching otherwise, good engineering principles still matter, now more than ever . Mode 2: Pair Programming Compiling When to use it : Projects under ~5,000 lines of code , side projects with real users, demos (you don’t want to break), or well-scoped small services in larger systems. This is where vibe-coding starts to shine. You need structure, but not so much that it slows you down. The key innovation here is the CLAUDE.md file—custom documentation that Claude automatically reads when invoked. From Anthropic’s Best practices for Claude Code : CLAUDE.md is a special file that Claude automatically pulls into context when starting a conversation: Common bash commands Core files and utility functions Code style guidelines Testing instructions Repository etiquette (e.g., branch naming, merge vs. rebase, etc.) Other information you want Claude to remember Instead of repeatedly explaining your project’s conventions, you document them once. Here’s a real example from a recent side project: ## Project: Analytics Dashboard This is a Next.js dashboard for visualizing user analytic: ### Architecture Decisions - Server Components by default, Client Components only when necessary - tRPC for type-safe API calls - Prisma for database access with explicit select statements - Tailwind for styling (no custom CSS files) ### Code Style - Formatting: Prettier with 100-char lines - Imports: sorted with simple-import-sort - Components: Pascal case, co-located with their tests - Hooks: always prefix with 'use' ### Patterns to Follow - Data fetching happens in Server Components - Client Components receive data as props - Use Zod schemas for all external data - Error boundaries around every data display component ### What NOT to Do - Don't use useEffect for data fetching - Don't create global state without explicit approval - Don't bypass TypeScript with 'any' types Audio With this context, Claude becomes remarkably effective. It’s like the difference between explaining your project to a new hire every single day versus having them read the onboarding docs once. But Pair Programming Mode requires more than just documentation. You need to actively guide the AI with what I call “anchor comments”—breadcrumbs that prevent Claude from wandering into the wilderness: // AIDEV- NOTE: This component uses virtual scrolling for performance // See: https://tanstack.com/virtual/latest // Don't convert to regular mapping—we handle 10k+ items export function DataTable ( { items }: DataTableProps ) { // Claude, when you edit this, maintain the virtual scrolling ... } These comments serve a dual purpose: they guide the AI and document your code for humans. It’s documentation that pays dividends in both directions. The key distinction between such “anchor comments” and regular comments: these are written , maintained , and meant to be used by Claude itself. Here’s an actual snippet from our project’s CLAUDE.md : ## Anchor comments Add specially formatted comments throughout the codebase, where appropriate, for yourself as inline knowledge that can be easily `grep` ped for. ### Guidelines: - Use `AIDEV-NOTE:` , `AIDEV-TODO:` , or `AIDEV-QUESTION:` (all-caps prefix) for comments aimed at AI and developers. - Keep them concise (≤ 120 chars). - **Important:** Before scanning files, always first try to **locate existing anchors** `AIDEV-*` in relevant subdirectories. - **Update relevant anchors** when modifying associated code. - **Do not remove `AIDEV-NOTE`s** without explicit human instruction. Example: # AIDEV-NOTE: perf-hot-path; avoid extra allocations (see ADR-24) async def render _feed(...): ... Mode 3: Production/Monorepo Scale RTFM When to use it : Large codebases, systems with real users, anything where bugs cost money or reputation. Claude can generate tremendous amounts of code, but integrating it into a complex system requires careful orchestration. Let me start with a big caveat: vibe coding at this scale does NOT scale very well, yet. I definitely do see these systems getting significantly better at handling larger codebases but , for them to be effective, significant effort is needed to help them navigate, understand, and safely hack on them without getting lost in a maze. Generally speaking, it’s better to section them into individual services, and 6 sub modules when possible. As a universal principle, good engineering practices apply to large-scale projects, vibe coded or not. For example, at production scale, boundaries become critical. Every integration point needs explicit documentation: # AIDEV- NOTE: API Contract Boundary - v2.3.1 # ANY changes require version bump and migration plan # See: docs/api-versioning.md @router.get( \"/users/{user_id}/feed\" ) async def get_user_feed ( user_id: UUID ) -> FeedResponse: # Claude: the response shape here is sacred # Changes break real apps in production ... Without these boundaries, Claude will happily “improve” your API and break every client in production. Bottom line: larger projects should definitely start adopting vibe coding in parts, and adopt methodologies that enhance that experience but, don’t expect to land large features reliably just yet. (as of June 7, 2025 / AI epoch ) Infrastructure: The Foundation of Sustainable AI Development CLAUDE.md : Your Single Source of Truth Let me be absolutely clear about this: CLAUDE.md is not optional documentation. Every minute you spend updating it saves an hour of cleanup later. Think of CLAUDE.md as a constitution for your codebase. It establishes the fundamental laws that govern how code should be written, how systems interact, and what patterns to follow or avoid. Organizations that invest in developing the skills and capabilities of their teams get better outcomes—and your CLAUDE.md is that investment crystallized into documentation. Here’s an abridged version of our production CLAUDE.md structure, refined over thousands of AI-assisted commits: # `CLAUDE.md` - Julep Backend Service ## The Golden Rule When unsure about implementation details, ALWAYS ask the developer. ## Project Context Julep enables developers to build stateful AI agents using declarative workflows. ## Critical Architecture Decisions ### Why Temporal? We use Temporal for workflow orchestration because: 1. Workflows can run for days/weeks with perfect reliability 2. Automatic recovery from any failure point ### Why PostgreSQL + pgvector? 1. ACID compliance for workflow state (can't lose user data) 2. Vector similarity search for agent memory ### Why TypeSpec? Single source of truth for API definitions: - OpenAPI specs - TypeScript/Python clients - Validation schemas ## Code Style and Patterns ### Anchor comments Add specially formatted comments throughout the codebase, where appropriate, for yourself as inline knowledge that can be easily `grep` ped for. ### Guidelines: - Use `AIDEV-NOTE:` , `AIDEV-TODO:` , or `AIDEV-QUESTION:` (all-caps prefix) for comments aimed at AI and developers. - **Important:** Before scanning files, always first try to **grep for existing anchors** `AIDEV-*` in relevant subdirectories. - **Update relevant anchors** when modifying associated code. - **Do not remove `AIDEV-NOTE`s** without explicit human instruction. - Make sure to add relevant anchor comments, whenever a file or piece of code is: * too complex, or * very important, or * confusing, or * could have a bug ## Domain Glossary (Claude, learn these!) - **Agent** : AI entity with memory, tools, and defined behavior - **Task** : Workflow definition composed of steps (NOT a Celery task) - **Execution** : Running instance of a task - **Tool** : Function an agent can call (browser, API, etc.) - **Session** : Conversation context with memory - **Entry** : Single interaction within a session ## What AI Must NEVER Do 1. **Never modify test files** - Tests encode human intent 2. **Never change API contracts** - Breaks real applications 3. **Never alter migration files** - Data loss risk 4. **Never commit secrets** - Use environment variables 5. **Never assume business logic** - Always ask 6. **Never remove AIDEV- comments** - They're there for a reason Remember: We optimize for maintainability over cleverness. When in doubt, choose the boring solution. This document becomes the shared context between you and Claude. It’s like having a senior developer whispering guidance in Claude’s ear throughout the coding session. Anchor Comments: Breadcrumbs at Scale As your codebase grows, CLAUDE.md alone isn’t enough. You need inline guidance—what I call anchor comments. These serve as local context that prevents AI from making locally bad decisions. Think of your codebase as a city and anchor comments as street signs. Without them, even smart visitors get lost. Here’s how we use them effectively: # AIDEV- NOTE: Critical performance path - this serves 100k req/sec # DO NOT add database queries here def get_user_feed ( user_id: UUID, cached_data: FeedCache ) -> List[FeedItem]: # We need to avoid mutating the cached data items = cached_data.items[:] # AIDEV- TODO: Implement pagination (ticket: FEED-123) # Need cursor-based pagination for infinite scroll # AIDEV-QUESTION: Why do we filter private items here instead of in cache? # AIDEV-ANSWER: Historical context: Privacy rules can change between cache updates filtered = [item for item in items if user_has_access(user_id, item)] return filtered These comments create a narrative that helps both AI and humans understand not just what the code does, but why it does it that way. Git Workflows for AI Development One of the most underappreciated aspects of AI-assisted development is how it changes your git workflow. You’re now generating code at a pace that can quickly pollute your git history if you’re not careful. It really only applies to very large codebases because it is not a very straightforward tool, but I recommend using git worktrees to create isolated environments for AI experiments: # Create an AI playground without polluting main git worktree add ../ai-experiments/cool-feature -b ai/cool-feature # Let Claude go wild in the isolated worktree cd ../ai-experiments/cool-feature # ... lots of experimental commits ... # Cherry-pick the good stuff back to main cd ../main-repo git cherry-pick abc123 # Just the commits that worked # Clean up when done git worktree remove ../ai-experiments/cool-feature Pro tip : Read about how to use worktrees , and check out the nifty wt tool. This approach gives you the best of both worlds: Claude can experiment freely while your main branch history stays clean and meaningful. For commit messages, we’ve standardized on tagging AI-assisted commits: feat: implement user feed caching [AI] - Add Redis-based cache for user feeds - Implement cache warming on user login - Add metrics for cache hit rate AI-assisted: core logic generated, tests human-written This transparency helps during code review—reviewers know to pay extra attention to AI-generated code. The Sacred Rule: Humans Write Tests Now we come to the most important principle in AI-assisted development. It’s so important that I’m going to repeat it in multiple ways until it’s burned into your memory: Never. Let. AI. Write. Your. Tests. Tests are not just code that verifies other code works. Tests are executable specifications. They encode your actual intentions, your edge cases, your understanding of the problem domain. High performers excel at both speed and stability—there’s no trade-off. Tests are how you achieve both. Beware… Let me illustrate why this matters with an example. Let’s say we asked Claude to implement a rate limiter: class RateLimiter : def __init__ ( self, max_requests: int , window_seconds: int ): self.max_requests = max_requests self.window_seconds = window_seconds self.requests = defaultdict( list ) def is_allowed ( self, user_id: str ) -> bool: now = time.time() user_requests = self.requests[user_id] # Clean old requests self.requests[user_id] = [ req_time for req_time in user_requests if now - req_time < self.window_seconds ] if len (self.requests[user_id]) < self.max_requests: self.requests[user_id].append(now) return True return False Looks reasonable, right? Claude even helpfully generated tests: def test_rate_limiter (): limiter = RateLimiter(max_requests= 3 , window_seconds= 60 ) assert limiter.is_allowed( \"user1\" ) == True assert limiter.is_allowed( \"user1\" ) == True assert limiter.is_allowed( \"user1\" ) == True assert limiter.is_allowed( \"user1\" ) == False # Limit reached But here’s what Claude’s tests missed—what only a human who understands the business requirements would test: Claude’s implementation has a memory leak. Users who hit the API once and never return leave their data in memory forever. The AI-generated tests check the happy path but miss this critical production concern. Vibe coding at its best This is why humans write tests. We understand the context, the production environment, the edge cases that matter. At Julep, our rule is absolute: ## Testing Discipline | What | AI CAN Do | AI MUST NOT Do | |------|-----------|----------------| | Implementation | Generate business logic | Touch test files | | Test Planning | Suggest test scenarios | Write test code | | Debugging | Analyze test failures | Modify test expectations | If an AI tool touches a test file, the PR gets rejected. No exceptions. Your tests are your specification. They’re your safety net. They’re the encoded wisdom of every bug you’ve fixed and every edge case you’ve discovered. Guard them zealously. Scaling Without Drowning: Token Economics and Context Management One of the most counterintuitive lessons in AI-assisted development is that being stingy with context to save tokens actually costs you more. It’s like trying to save money on gas by only filling your tank halfway—you just end up making more trips to the gas station. Token budgets matter. Provide focused prompts, reduce diff length, and avoid large-file bloat by summarizing intent in advance. But “focused” doesn’t mean “minimal”—it means “relevant and complete.” Let me show you the false economy of starved prompts: Starved Prompt Attempt: \"Add caching to the user endpoint\" Claude’s Response: Implements caching… but: Uses in-memory cache (won’t work with multiple servers) No cache invalidation strategy No metrics or monitoring No consideration of cache stampede Result: 3 more rounds of fixes, 4x the tokens spent . Proper Context-Rich Prompt: Add Redis caching to the GET /users/{id} endpoint. Context: - This endpoint serves 50k requests/minute - We run 12 API servers behind a load balancer - User data changes infrequently (few times per day) - We already have Redis at cache.redis.internal:6379 - Use our standard cache key pattern: \"user:v1:{id}\" - Include cache hit/miss metrics (we use Prometheus) - Implement cache-aside pattern with 1 hour TTL - Handle cache stampede with probabilistic early expiration See our caching guide: docs/patterns/caching.md The lesson? Front-load context to avoid iteration cycles. Think of tokens like investing in good tools—the upfront cost pays for itself many times over. In fact, I recommend that all projects should routinely ask Claude to look through the codebase changes, and add context to CLAUDE.md Fresh Sessions and Mental Models Here’s another counterintuitive practice: use fresh Claude sessions for distinct tasks. It’s tempting to keep one long-running conversation, but this leads to context pollution. Think of it like this: you wouldn’t use the same cutting board for vegetables after cutting raw chicken. Similarly, don’t use the same Claude session for database migrations after discussing frontend styling. The context bleeds through in subtle ways. Our rule: One task, one session. When the task is done, start fresh. This keeps Claude’s “mental model” clean and focused. Audio Case Study: Shipping Structured Errors in Production Let me walk you through a real refactoring we did at Julep that showcases production-scale vibe-coding. We needed to replace our ad-hoc error handling with a structured error hierarchy across 500+ endpoints. The Human Decisions (The Why): First, we had to decide on our error taxonomy. This is pure architectural work—Claude can’t make these decisions because they involve understanding our business, our users, and our operational needs: # SPEC.md - Error Hierarchy Design (Human-Written) ## Error Philosophy - Client errors (4xx) must include actionable feedback - System errors (5xx) must include trace IDs for debugging - All errors must be JSON-serializable - Error codes must be stable (clients depend on them) ## Hierarchy BaseError ├── ClientError (4xx) │ ├── ValidationError │ │ ├── SchemaValidationError - Request doesn't match schema │ │ ├── BusinessRuleError - Valid schema, invalid business logic │ │ └── RateLimitError - Too many requests │ └── AuthError │ ├── AuthenticationError - Who are you? │ └── AuthorizationError - You can't do that └── SystemError (5xx) ├── DatabaseError - Connection, timeout, deadlock ├── ExternalServiceError - APIs, webhooks failing └── InfrastructureError - Disk full, OOM, etc. ## Error Response Format { \"error\": { \"code\": \"VALIDATION_FAILED\", // Stable code for clients \"message\": \"Email already exists\", // Human-readable \"details\": { ... }, // Structured data \"trace_id\": \"abc-123-def\" // For debugging } } The AI Execution (The How): With the specification clear, we unleashed Claude on the mechanical refactoring: ### Prompt to Claude: Refactor our error handling to match SPEC.md. Current state: - raise ValueError(\"Invalid email\") - return {\"error\": \"Something went wrong\"}, 500 Target state: - Use error hierarchy from SPEC.md - Include proper error codes - Add trace _id to all 5xx errors Start with the auth module. Show me the plan before implementing. Claude’s plan was solid: 1. Create error hierarchy in `common/errors.py` 2. Create error response formatter 3. Update each module systematically 4. Add error handling middleware Claude was able to handle the tedious work of finding and updating 500+ error sites, while we focused on reviewing: # Before (Claude found these patterns): if not user: raise Exception( \"User not found\" ) # After (Claude's refactoring): if not user: raise AuthenticationError( message= \"User not found\" , code= \"USER_NOT_FOUND\" , details={ \"identifier\" : email} ) Combined with our carefully written CLAUDE.md file, meticulous docs, regularly updated anchor comments, and clear instructions, results: Time: 4 hours instead of 2 days Coverage: All 500+ error sites updated Leadership and Culture in the AI Era Your role as a senior engineer has fundamentally shifted. You’re no longer just writing code—you’re curating knowledge, setting boundaries, and teaching both humans and AI systems how to work effectively. Lean management and continuous delivery practices help improve software delivery performance, which in turn improves organizational performance—and this includes how you manage AI collaboration. The New Onboarding Checklist When new developers join our team, they get two onboarding tracks: one for humans, one for working with AI. Here’s our combined checklist: Week 1: Foundation □ Read team `CLAUDE.md` files (start with root, then service-specific) □ Set up development environment □ Make first PR (human-written, no AI) Week 2: Guided AI Collaboration □ Set up Claude with team templates □ Complete \"toy problem\" with AI assistance □ Practice prompt patterns □ Create first AI-assisted PR (with supervision) Week 3: Independent Work □ Ship first significant AI-assisted feature □ Write tests for another developer's AI output □ Lead one code review session Building a Culture of Transparency One cultural shift that’s essential: normalize disclosure of AI assistance. We’re not trying to hide that we use AI—we’re trying to use it responsibly. Every commit message that includes AI work gets tagged: # Our .gitmessage template # feat/fix/docs: <description> [AI]? # # [AI] - Significant AI assistance (>50% generated) # [AI-minor] - Minor AI assistance (<50% generated) # [AI-review] - AI used for code review only # # Example: # feat: add Redis caching to user service [AI] # # AI generated the cache implementation and Redis client setup. # I designed the cache key structure and wrote all tests. # Manually verified cache invalidation logic works correctly. This transparency serves multiple purposes: Reviewers know to pay extra attention Future debuggers understand the code’s provenance No one feels shame",
      "summary": "37-minute read",
      "url": "https://diwank.space/field-notes-from-shipping-real-code-with-claude?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 4071,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 4071/7400 words (55.0%)"
    },
    {
      "title": "MCP vs API",
      "content": "The HTTP API Problem MCP: A Wire Protocol, Not Documentation Why Not Just Use OpenAPI? Five Fundamental Differences Runtime Discovery vs Static Specs Deterministic Execution vs LLM-Generated Calls Bidirectional Communication Single-Request Human Tasks Local-First by Design The Training Advantage They're Layers, Not Competitors Real-World Example The Bottom Line Bonus: MCP vs API video Bonus: Existing Reddit discussions Every week a new thread emerges on Reddit asking about the difference between MCP and API. I've tried summarizing everything that's been said about MCP vs API in a single post (and a single table). Aspect Traditional APIs (REST/GraphQL) Model Context Protocol (MCP) What it is Interface styles (REST, GraphQL) with optional spec formats (OpenAPI, GraphQL SDL) Standardized protocol with enforced message structure Designed for Human developers writing code AI agents making decisions Data location REST: Path, headers, query params, body (multiple formats) Single JSON input/output per tool Discovery Static docs, regenerate SDKs for changes 1 2 Runtime introspection ( tools/list ) Execution LLM generates HTTP requests (error-prone) LLM picks tool, deterministic code runs Direction Typically client-initiated; server-push exists but not standardized Bidirectional as first-class feature Local access Requires port, auth, CORS setup Native stdio support for desktop tools Training target Impractical at scale due to heterogeneity Single protocol enables model fine-tuning I am making several broad generalizations to keep the article length reasonable. I will continue to update this article with feedback from the community. If you have any suggestions, please email me at frank@glama.ai . The HTTP API Problem HTTP APIs suffer from combinatorial chaos. To send data to an endpoint, you might encode it in: URL path ( /users/123 ) Request headers ( X-User-Id: 123 ) Query parameters ( ?userId=123 ) Request body (JSON, XML, form-encoded, CSV) OpenAPI/Swagger documents these variations, but as a specification format, it describes existing patterns rather than enforcing consistency. Building automated tools to reliably use arbitrary APIs remains hard because HTTP wasn't designed for this—it was the only cross-platform, firewall-friendly transport universally available from browsers. MCP: A Wire Protocol, Not Documentation Model Context Protocol (MCP) isn't another API standard—it's a wire protocol that enforces consistency. While OpenAPI documents existing interfaces with their variations, MCP mandates specific patterns: JSON-RPC 2.0 transport, single input schema per tool, deterministic execution. Key architecture: Transport : stdio (local) or streamable HTTP Discovery : tools/list , resources/list expose capabilities at runtime Primitives : Tools (actions), Resources (read-only data), Prompts (templates) There is more than the above. Refer to the MCP specification for complete overview. Why Not Just Use OpenAPI? The most common question: \"Why not extend OpenAPI with AI-specific features?\" Three reasons: OpenAPI describes; MCP prescribes . You can't fix inconsistency by documenting it better—you need enforcement at the protocol level. Retrofitting fails at scale . OpenAPI would need to standardize transport, mandate single-location inputs, require specific schemas, add bidirectional primitives—essentially becoming a different protocol. The ecosystem problem . Even if OpenAPI added these features tomorrow, millions of existing APIs wouldn't adopt them. MCP starts fresh with AI-first principles. Five Fundamental Differences 1. Runtime Discovery vs Static Specs API : Ship new client code when endpoints change MCP : Agents query capabilities dynamically and adapt automatically // MCP discovery - works with any server client.request('tools/list') // Returns all available tools with schemas 2. Deterministic Execution vs LLM-Generated Calls API : LLM writes the HTTP request → hallucinated paths, wrong parameters MCP : LLM picks which tool → wrapped code executes deterministically This distinction is critical for production safety. With MCP, you can test, sanitize inputs, and handle errors in actual code, not hope the LLM formats requests correctly. 3. Bidirectional Communication API : Server-push exists (WebSockets, SSE, GraphQL subscriptions) but lacks standardization MCP : Bidirectional communication as first-class feature: Request LLM completions from server Ask users for input ( elicitation ) Push progress notifications 4. Single-Request Human Tasks REST APIs fragment human tasks across endpoints. Creating a calendar event might require: POST /events (create) GET /conflicts (check) POST /invitations (notify) MCP tools map to complete workflows. One tool, one human task. 5. Local-First by Design API : Requires HTTP server (port binding, CORS, auth headers) MCP : Can run as local process via stdio—no network layer needed Why this matters: When MCP servers run locally via stdio, they inherit the host process's permissions. This enables: Direct filesystem access (read/write files) Terminal command execution System-level operations A local HTTP server could provide the same capabilities. However, I think the fact that MCP led with stdio transport planted the idea that MCP servers are meant to be as local services, which is not how we typically think of APIs. The Training Advantage MCP's standardization creates a future opportunity: models could be trained on a single, consistent protocol rather than thousands of API variations. While models today use MCP through existing function-calling capabilities, the protocol's uniformity offers immediate practical benefits: Consistent patterns across all servers: Discovery: tools/list , resources/list , prompts/list Execution: tools/call with single JSON argument object Errors: Standard JSON-RPC format with numeric codes Reduced cognitive load for models: // Every MCP tool follows the same pattern: { \"method\": \"tools/call\", \"params\": { \"name\": \"github.search_prs\", \"arguments\": {\"query\": \"security\", \"state\": \"open\"} } } // Versus REST APIs with endless variations: // GET /api/v2/search?q=security&type=pr // POST /graphql {\"query\": \"{ search(query: \\\"security\\\") { ... } }\"} // GET /repos/owner/repo/pulls?state=open&search=security This standardization means models need to learn one calling convention instead of inferring patterns from documentation. As MCP adoption grows, future models could be specifically optimized for the protocol, similar to how models today are trained on function-calling formats. They're Layers, Not Competitors Most MCP servers wrap existing APIs: [AI Agent] ⟷ MCP Client ⟷ MCP Server ⟷ REST API ⟷ Service The mcp-github server translates repository/list into GitHub REST calls. You keep battle-tested infrastructure while adding AI-friendly ergonomics. Real-World Example Consider a task: \"Find all pull requests mentioning security issues and create a summary report.\" With OpenAPI/REST : LLM reads API docs, generates: GET /repos/{owner}/{repo}/pulls?state=all Hopes it formatted the request correctly Parses response, generates: GET /repos/{owner}/{repo}/pulls/{number} Repeats for each PR (rate limiting issues) Generates search queries for comments Assembles report With MCP : LLM calls: github.search_issues_and_prs({query: \"security\", type: \"pr\"}) Deterministic code handles pagination, rate limits, error retry Returns structured data LLM focuses on analysis, not API mechanics The Bottom Line HTTP APIs evolved to serve human developers and browser-based applications, not AI agents. MCP addresses AI-specific requirements from the ground up: runtime discovery, deterministic execution, and bidirectional communication. For AI-first applications, MCP provides structural advantages—local execution, server-initiated flows, and guaranteed tool reliability—that would require significant workarounds in traditional API architectures. The practical path forward involves using both: maintaining APIs for human developers while adding MCP for AI agent integration. Bonus: MCP vs API video During my research, I found this video to be one of the easiest to digest the differences between MCP and API. Bonus: Existing Reddit discussions During my research, I found these Reddit discussions to be helpful in understanding the differences between MCP and API. https://www.reddit.com/r/mcp/comments/1kyeu26/why_mcp_protocol_vs_openapi_docs/ https://www.reddit.com/r/mcp/comments/1iztbrc/whats_the_difefrence_of_using_an_api_vs_an_mcp/ https://www.reddit.com/r/mcp/comments/1jr8if3/i_cant_understand_the_hype/ Footnotes GraphQL offers schema introspection, but it lacks task-level descriptions or JSON-schema-style validation, so SDKs still regenerate for new fields. ↩ OpenAPI 3.1+ supports runtime discovery through the OpenAPI document endpoint. The key difference is that MCP mandates runtime discovery while OpenAPI makes it optional. ↩ Written by Frank Fiegel ( @ punkpeye )",
      "summary": "6-minute read",
      "url": "https://glama.ai/blog/2025-06-06-mcp-vs-api?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 1218,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Welcome to Campus. Here's Your ChatGPT",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~1600 words, got 435. URL: https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
      "summary": "8-minute read",
      "url": "https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 435,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 435/1600 words (27.2%)"
    },
    {
      "title": "The hidden time bomb in the tax code that's fueling mass tech layoffs",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
      "summary": "14-minute read",
      "url": "https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
    },
    {
      "title": "magic namerefs",
      "content": "namerefs (introduced in bash 4.0) act as aliases for other variables var=meow declare -n ref=var echo $ref # prints meow ref=moo echo $var # prints moo they can also reference a specific element in an array, using declare -n ref='array[1234]' using this, i have been playing with a neat nameref trick: tmp=() declare -n var= ' tmp[tmp[0]=some_expression_here, 0] ' it uses the auxiliary array tmp to force an arithmetic context, in which it assigns the result of any arbitrary expression to an element of that array, then expands that same element. we can now create magic variables that evaluate any arbitrary expression. here's a basic counter: tmp=() x=0 declare -n counter= ' tmp[tmp[0]=x++,0] ' for i in {1..10} ; do echo $counter done # prints 0 1 2 3 4 5 6 7 8 9 here's an example that computes the fibonacci numbers: f=(0 1) declare -n fib= ' f[f[2]=f[0], f[0]+=f[1], f[1]=f[2], 0] ' for i in {1..10} ; do echo $fib done # prints 1 1 2 3 5 8 13 21 34 55 this is already very powerful, as it can do many magic things with numbers. but as it turns out, we can do even more: we can use dollar expansions too! here's a silly clock with magic variables that show the current date and time: # \\D{your-format-here} passes that format to strftime # but it only works in prompts like $PS1 # ${var@P} expands $var as if it was in your $PS1 date=( ' \\D{% ' {Y,m,d,H,M,S}}) # the formats we'll use months=(- jan feb mar apr may jun jul aug sep oct nov dec) numbers=({00..60}) tmp=() declare -n year= ' tmp[tmp=${date[0]@P},0] ' declare -n month= ' months[10#${date[1]@P}] ' declare -n day= ' numbers[10#${date[2]@P}] ' declare -n hour= ' numbers[10#${date[3]@P}] ' declare -n minute= ' numbers[10#${date[4]@P}] ' declare -n second= ' numbers[10#${date[5]@P}] ' while : ; do echo $year / $month / $day $hour : $minute : $second sleep 1 done # 2025/jun/06 09:54:13 # 2025/jun/06 09:54:14 # 2025/jun/06 09:54:15 # 2025/jun/06 09:54:16 this is probably one of the coolest things i've ever seen in bash. honestly i am a bit horrified that this works at all, but the resulting code is just so simple and elegant. and it feels like i'm just scratching the surface, there's so much potential. previous: recursive expansions",
      "summary": "2-minute read",
      "url": "https://gist.github.com/izabera/e4717562e20eb6cfb6e05f8019883efb?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 387,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "SaaS Is Just Vendor Lock-In with Better Branding",
      "content": "SaaS Is Just Vendor Lock-In with Better Branding Developers are told \"to focus on the product\" and let SaaS vendors handle the rest, but integrating third-party services, whether it's auth, queuing, file storage, or image optimization, comes at a cost. Not just in dollars but in time, friction, and mental overhead. There are five hidden taxes you pay every time you integrate a SaaS into your stack. 1. The Discovery Tax Before you can integrate anything, you first have to figure out what they're actually selling? What problems are they solving? Is it compatible with your stack? Is their price sane at your scale? a Are their docs clear and do they reveal any implementation weirdness? This unpaid research work is usually non-transferable. What you learn about \"Uploady\" or \"MegaQueue\" doesn't help you next time when you're evaluating something else. It's also subjective. It's marketing, and does the marketing message resonate with you? 2. The Sign-Up Tax You've decided on a service, and this is the moment when you hand over your email and credit card. Do they support usage-based pricing or only lock-in tiers? Can your team members access the dashboard, or do you have to pay more for that functionality? Despite only using the service the same amount! Can you even test the product without hitting a paywall? You're now on the hook, even if you haven't written a single line of code. 3. The Integration Tax Now the real work begins. You read the docs. You install the libraries You wire it into your framework. And figure out the edge cases that the docs don't mention, because docs are marketing! Often you're left fighting your own tooling. They're aiming for the lowest common denominator, and you're bleeding edge. Or the other way around! 4. The Local Development Tax You need the SaaS service to work locally. Does it even offer a local emulator? Can you stub it out in tests? Do you need to tunnel to the cloud just to test one feature? Now you've got branching configuration logic, one for production, one for staging, one for local… If you're lucky. 5. The Production Tax This is the part where you're \"done,\" except you're not. Can you use this in your staging environment? What about pull request previews? You need to securely manage the API keys. Monitoring, logging, and alerting Wondering why something worked in your laptop but fails in production? You've integrated the service, but now you're on the hook for its reliability in production. Conclusion The pitch of modern SaaS is \"don't reinvent the wheel.\" But every wheel you bolt on comes with some friction. It's not just a service: It's a contract. It's a dependency. It's a subtle architectural shift, and it comes with taxes. No matter what choice you make, it's always going to be vendor-locked in. Switching out something, even if it's open source and self-hosted, means that you're rewriting a lot of code. So, my argument is, don't make those decisions. Just pick a platform. The thing that matters is the software that you want to write, not the framework or the services that it runs on. Platforms like Cloudflare or Supabase shine. Where your database, queue, image service, and storage all live within the same platform and speak the same language. You avoid paying these taxes repeatedly. You simply pick the product that's already there. No context switching between vendors. No API key wrangling. No compatibility hacks or configuration forks. Just fast, local feeling integrations that work the same in dev and production. It feels like everything is running on the same machine, and in a way it kind of is. That's the hidden superpower of integrated platforms. They collapse the distance between your code and your services. And in doing so, they give you back the one thing no SaaS vendor can sell you: \"Flow.\"",
      "summary": "3-minute read",
      "url": "https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 646,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Reverse Engineering Cursor's LLM Client",
      "content": "Reverse Engineering Cursor's LLM Client June 5, 2025 · Viraj Mehta, Aaron Hill, Gabriel Bianconi What happens under the hood at Cursor? We wired TensorZero between Cursor and the LLMs to see every token fly by… and bend those API calls to our own will. TensorZero is an open-source framework that helps engineers optimize LLM applications with downstream feedback signals (e.g. production metrics, human feedback, user behavior), and we figured it would be interesting to see whether we could use TensorZero on the LLM application we use most heavily ourselves: Cursor. With our gateway between Cursor and the LLM providers, we can observe the LLM calls being made, run evaluations on individual inferences, use inference-time optimizations, and even experiment with and optimize the prompts and models that Cursor uses. Cursor is optimized for its population of users as a whole — beyond .cursorrules , is there room for improvement by diving deeper and tailoring it to individual users? What would it look like to be able to empirically experiment with and optimize Cursor for your individual usage patterns? Beyond optimization, Cursor still operates as a black box. Wouldn’t it be interesting to see what’s actually being sent to the models? Nearly all LLM optimization, evaluation, and experimentation techniques require data on what inferences were made and their real-world consequences. In this post, we’ll focus on the former and dive into how we set up TensorZero as a self-hosted proxy between Cursor and the LLMs that it calls. If you’d like to try it yourself, check out the example in our repository. We’ll follow up with a blog post on how we collect feedback and close the optimization loop. Wiring Things Together The first thing we noticed was that Cursor lets you override the OpenAI base URL and model names. Perfect. TensorZero exposes an OpenAI-compatible inference endpoint, so we can easily configure Cursor to call TensorZero instead of OpenAI. In the TensorZero configuration, we define a TensorZero function cursorzero so that we can automatically experiment with different models and prompts while storing provider-agnostic inference and feedback data in our database for observability and optimization. The First Roadblock: Cursor’s Servers Cursor was initially unable to connect to TensorZero running locally. It turns out that Cursor first sends a request to its own servers, where additional processing happens before making the LLM call, so it couldn’t connect to our gateway on localhost . (This also means that your credentials must be forwarded to Cursor’s servers, allowing them to collect data on your inferences and codebase.) As a test that our plan could work in the first place, we pointed Cursor to OpenRouter and realized we could use its models for the Ask, Agent, and Cmd+K interactions in Cursor. We were also still able to use the normal Cursor Tab completions, which rely on a proprietary model Cursor serves for inline completions (it is very good, so we’re glad to be able to keep it). The solution was to set up a reverse proxy to expose a public endpoint that would forward requests back to our machine. We used Ngrok to keep things simple. Since we were exposing a gateway with LLM credentials to the public Internet, we added Nginx in front of our gateway to authenticate requests. We set Cursor’s base URL to our Ngrok endpoint, added the credentials we set up in Nginx, turned off built-in models, and finally added our new TensorZero function under the model name tensorzero::function_name::cursorzero . In the end, the workflow looked like this: Cursor → Ngrok → Nginx (self-hosted) → TensorZero (self-hosted) → LLM providers But it didn’t work. The Second Roadblock: CORS The authentication process had failed. Nginx logs showed that there was an OPTIONS request hitting our endpoint, so we configured Nginx to return headers on OPTIONS requests and incrementally added headers we saw in responses from the OpenAI API. This is the initial verification request that comes from the local Cursor IDE. The CORS requirement likely comes from Electron. After the initial verification, all requests come from Cursor’s servers. Our Nginx Configuration to handle CORS headers # --- CORS helper macro --- set $CORS_ALLOW_ORIGIN $http_origin; # reflect the caller's origin set $CORS_ALLOW_HEADERS \"Authorization,Content-Type\"; location / { # --- pre-flight --- if ($request_method = OPTIONS) { add_header Access-Control-Allow-Origin $CORS_ALLOW_ORIGIN always; add_header Access-Control-Allow-Credentials \"true\" always; add_header Access-Control-Allow-Methods \"GET,POST,OPTIONS\" always; add_header Access-Control-Allow-Headers $CORS_ALLOW_HEADERS always; add_header Access-Control-Max-Age 86400 always; return 204; # 204 (No Content) is conventional for pre-flight } } You can find the entire codebase for “CursorZero” on GitHub . It finally worked! Finally: Observability for Cursor We could finally see everything coming in and out of Cursor — including its prompts. For example, here’s what we saw after asking Cursor “how do I get nginx to get the key from an environment variable in my .env?” System Prompt You are a an AI coding assistant, powered by tensorzero::function_name::cursorzero. You operate in Cursor You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide. Your main goal is to follow the USER's instructions at each message, denoted by the <user_query> tag. <communication> When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use ( and ) for inline math, [ and ] for block math. </communication> <search_and_reading> If you are unsure about the answer to the USER's request or how to satiate their request, you should gather more information. This can be done by asking the USER for more information. Bias towards not asking the user for help if you can find the answer yourself. </search_and_reading> <making_code_changes> The user is likely just asking questions and not looking for edits. Only suggest edits if you are certain that the user is looking for edits. When the user is asking for edits to their code, please output a simplified version of the code block that highlights the changes necessary and adds comments to indicate where unchanged code has been skipped. For example: ```language:path/to/file // ... existing code ... {{ edit_1 }} // ... existing code ... {{ edit_2 }} // ... existing code ... ``` The user can see the entire file, so they prefer to only read the updates to the code. Often this will mean that the start/end of the file will be skipped, but that's okay! Rewrite the entire file only if specifically requested. Always provide a brief explanation of the updates, unless the user specifically requests only the code. These edit codeblocks are also read by a less intelligent language model, colloquially called the apply model, to update the file. To help specify the edit to the apply model, you will be very careful when generating the codeblock to not introduce ambiguity. You will specify all unchanged regions (code and comments) of the file with \"// ... existing code ...\" comment markers. This will ensure the apply model will not delete existing unchanged code or comments when editing the file. You will not mention the apply model. </making_code_changes> <user_info> The user's OS version is darwin 24.3.0. The absolute path of the user's workspace is /Users/viraj/tensorzero/tensorzero/examples/cursorzero. The user's shell is /bin/zsh. </user_info> You MUST use the following format when citing code regions or blocks: ```12:15:app/components/Todo.tsx // ... existing code ... ``` This is the ONLY acceptable format for code citations. The format is ```startLine:endLine:filepath``` where startLine and endLine are line numbers. User Prompt <additional_data> Below are some potentially helpful/relevant pieces of information for figuring out to respond <current_file> Path: nginx/nginx.conf Line: 1 Line Content: `events {}` </current_file> <attached_files> <file_contents> ```path=nginx/nginx.conf, lines=1-46 events {} http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\"'; access_log /var/log/nginx/access.log main; map $http_authorization $is_authorized { default 0; \"~*^Bearer your-token-here$\" 1; } server { listen 80; # --- CORS helper macro --- set $CORS_ALLOW_ORIGIN \"$http_origin\"; # reflect the caller's origin set $CORS_ALLOW_HEADERS \"Authorization,Content-Type\"; location / { # --- pre-flight --- if ($request_method = OPTIONS) { add_header Access-Control-Allow-Origin \"$CORS_ALLOW_ORIGIN\" always; add_header Access-Control-Allow-Credentials \"true\" always; add_header Access-Control-Allow-Methods \"GET,POST,OPTIONS\" always; add_header Access-Control-Allow-Headers \"$CORS_ALLOW_HEADERS\" always; add_header Access-Control-Max-Age 86400 always; return 204; # 204 No Content is conventional for pre‑flight } # --- authenticated requests --- if ($is_authorized = 0) { return 401; } proxy_pass http://gateway:3000; proxy_set_header Host $host; # expose CORS headers on the real response too add_header Access-Control-Allow-Origin \"$CORS_ALLOW_ORIGIN\" always; add_header Access-Control-Allow-Credentials \"true\" always; } } } ``` </file_contents> <manually_added_selection> ```path=nginx/nginx.conf, lines=1-16 events {} http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\"'; access_log /var/log/nginx/access.log main; map $http_authorization $is_authorized { default 0; \"~*^Bearer your-token-here$\" 1; } server { listen 80; ``` </manually_added_selection> </attached_files> </additional_data> <user_query> how do I get nginx to get the key from an environment variable in my .env? </user_query> Response I see you're trying to use an environment variable for your authentication token in the Nginx configuration instead of hardcoding \"your-token-here\" in the `nginx.conf` file. Unfortunately, Nginx doesn't directly read environment variables from a `.env` file. However, there are a few approaches you can take: ### Option 1: Use Nginx's environment variable support with a template 1. Create a template file (e.g., `nginx.conf.template`) with a placeholder: ``` map $http_authorization $is_authorized { default 0; \"~*^Bearer ${API_TOKEN}$\" 1; } ``` 2. Then use a script to replace the placeholder with the actual environment variable before starting Nginx: ```bash #!/bin/bash # Load environment variables from .env source .env # Replace placeholder with actual token envsubst '${API_TOKEN}$' < /path/to/nginx.conf.template > /path/to/nginx.conf # Start Nginx nginx -g 'daemon off;' ``` ### Option 2: Use Docker environment variables (if using Docker) If you're running Nginx in Docker, you can pass environment variables to the container and use `envsubst`: ```dockerfile FROM nginx:alpine COPY nginx.conf.template /etc/nginx/templates/default.conf.template CMD [\"/bin/sh\", \"-c\", \"envsubst '${API_TOKEN}$' < /etc/nginx/templates/default.conf.template > /etc/nginx/conf.d/default.conf && nginx -g 'daemon off;'\"] ``` Then run with: ```bash docker run -e API_TOKEN=your-secret-token -p 80:80 your-nginx-image ``` ### Option 3: Use a separate authentication service Instead of hardcoding the token in Nginx, you could proxy authentication requests to a separate service that has access to environment variables: ``` location /auth { proxy_pass http://auth-service:3001; # This service can read environment variables } ``` For your specific case, I'd recommend Option 1 or 2 depending on your deployment setup. This way, you can keep your token in your `.env` file and have it securely injected into the Nginx configuration at startup time. Prompt Engineering Cursor This example paints a picture of what prompt engineering looks like for a state-of-the-art AI coding assistant. This prompt and others like it (there were different prompts used for asking questions, editing source code, and working in the shell) are all Cursor uses to answer questions and generate code. We find it remarkable that there isn’t more here and that all of software engineering has been internalized enough by the current generation of LLMs that you can get the brains behind Cursor with a 642-token system prompt. This must be due to extensive post-training efforts by the big labs. An Explicit AI Hierarchy? We find the following snippet particularly intriguing: These edit codeblocks are also read by a less intelligent language model, colloquially called the apply model, to update the file. To help specify the edit to the apply model, you will [...]. You will not mention the apply model. It’s common to mix different models to optimize cost and latency, but Cursor explains this hierarchy to the models themselves? Interesting. A/B Testing Models With TensorZero in place, we have the full Cursor experience with control over observability and experimentation of our LLM requests. We’ve been running CursorZero for days of heavy software engineering: it’s been stable and there has been no noticeable additional latency. We’re currently running an even random split of Claude 4.0 Sonnet, GPT-4.1, o4 Mini, and Gemini 2.5 Pro — and it feels as good as Cursor ever has. Try It Yourself Interested in analyzing your own usage of AI coding assistants? You can find instructions on GitHub to reproduce this work with Cursor and OpenAI Codex . TensorZero helps you understand — and optimize — your LLM agents, even if you didn’t build them yourself! But Wait… There’s More! This blog post demonstrates how we successfully reverse-engineered Cursor’s LLM client by setting up TensorZero as a self-hosted proxy service, enabling us to observe, analyze, and experiment with different LLM models while maintaining the full Cursor experience. In our next post of this series, we’ll explain how we’re evaluating real-world usage of AI coding assistants, along with the results of our internal A/B testing between models. Sneak peek: git hooks, tree-sitter, and more. Then, we’ll explore if this feedback signal might be able to improve Cursor by optimizing models and inferences through your individual usage patterns. Stay tuned by subscribing below. Start building today. Check out our GitHub and Quick Start .",
      "summary": "17-minute read",
      "url": "https://www.tensorzero.com/blog/reverse-engineering-cursors-llm-client/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 2185,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 2185/3400 words (64.3%)"
    },
    {
      "title": "Meta reportedly in talks to invest billions of dollars in Scale AI",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
      "summary": "1-minute read",
      "url": "https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Specification Engineering",
      "content": "Specification Engineering reading time: 2.90 mins published: 2025-03-18 updated: 2025-03-29 ... is a bet on better code gen and more complexity AI engineering wants to be declarative. Ultimately, the algorithms in language model attention heads which devs interleave with their Python and Typescript are fuzzy and inscrutable. Their natural abstractions are guarantees, predicates on co-images. We can’t know what they do, only what they will have done. This is fine when AI software is 99% Python logic and 1% AI. Most software accesses SQL databases, but that’s not a problem - the interaction is tightly scoped and managed. But what happens when the software is 99% AI and 1% Python, at least in terms of complexity/headspace? At that point, the software is poorly served by the imperative paradigm. There may be plenty of LoC, but as far as it’s owner is concerned, there’s nothing imperative about it. At that point, much of the actual logic of the program lives in … the heads of its developers. The prompts won’t speak for themselves - each maintainer will, as a byproduct of hours of whiteboarding and painful trial-and-error, have reams and reams of knowledge regarding how different AI components interrelate with one another that simply cannot be safely deduced from the code itself, possibly even by a superintelligence. Storing critical system information solely in human minds - and doing so more often as time goes on and AI becomes a bigger part of software - is not a good idea. The I/O bandwidth is low, the information degrades quickly, and collaboration scales poorly. Its a structural trend directly counter to the massive productivity gains the rest of software is seeing - and it’s holding AI software development back. Evals can sometimes help introduce some structure and legibility, but they’re too fragmented - the requirements your engineers care about are distributed across hundreds of test cases you will never read and likely struggle to version and update. As time goes on, teams and engineers will want AI systems like Synth to help them - and, to be most effective in controlling and intervening on the software, those systems will need a legible and durable source of truth. Finding the right abstractions will take time, but now is the time to start. Every abstraction is leaky, and so directly maintaining imperative Python in AI software will be a necessity for the foreseeable future. But, great engineering teams will use processes and tools to ensure that the system specification is syncronized and takes precedence. PRs and prompt updates can be compiled up into spec diffs, and rejected if they introduce breaking changes. Syncronization in the other direction is where the abstraction starts paying for itself. Add a requirement -> AI spools up 1k LoC and Synth stress-tests two new prompts and a sub-agent, with 5 new evals to boot. Evals just become a way to check guarantees and create impetus for the compiler to update prompts/code/LoRAs. Naturally, syncronization will sometimes go both ways. Adding a better model might require simpler code with fewer prompts, depending on how preferences are outlined, and so we might go models -> evals -> spec -> code -> evals -> spec. Suddenly equilibrium becomes a more apt description than compilation. But don’t let that scare you away. Declarative specs with guarantees aren’t new . They’ve been used as long as software’s been written. Engineering teams benefit greatly from clearly communicating system level guarantees, and maintained them even back when it took precious human-hours to do so. What is new is intelligence that helps us consistently and cheaply transpile between them and software in a git commit hook. Let’s use it.",
      "summary": "3-minute read",
      "url": "https://www.joshuapurtell.com/posts/spec_eng/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 610,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Maintaining an Android app is a lot of work",
      "content": "There was recent news about 47% decline in the number of apps on Google Play Store. As a hobby Android developer, who has been developing MusicSync , a Google Play Music + Podcast replacement for the last five years, I thought I would share my experience of maintaining an Android app. And why this reduction in the number of apps is not surprising to me. I have several side-projects that run on a backend server with a limited web UI, and it is much less effort to maintain them. However, maintaining an Android app as a side-project is a more involved affair. And here are some of the problems I have faced. Java vs Kotlin # Kotlin is clearly the preferred language of development if you are starting a new Android project in 2025. But what if you are maintaining a hobby project written in Java? You will start seeing incompatibility when your dependencies are re-written in Kotlin. If you depend on a library that uses Kotlin’s coroutines or relies on Kotlin’s suspend functions , then you will have to work around it, or rewrite your app in Kotlin as well! Jetpack Compose, an official Google UI library for Android is entirely unusable from Java. I would imagine that if you started with Kotlin first then a big chunk of StackOverflow questions written for Java audiences require you translate them to corresponding Kotlin code as well To their credit, Android documentation still gives code samples in both Java and Kotlin. Google makes breaking changes to its libraries # Google has a habit of making breaking changes to its Android libraries. Here’s a list of some of the libraries that I have used in my app and the issues I have faced. Media 3 # Android ships with MediaPlayer . Google recommends its open-source library ExoPlayer . ExoPlayer V1 was last released in 2017. It was replaced with backward-incompatible ExoPlayer V2 which was last released in July 2024. And now, it has now been replaced with backward-incompatible media3 . The Google provided migration script is far from being complete. Further, media3 does not follow semantic versioning, minor version upgrades has resulted in breaking API changes. Google Auth library # Google’s own Auth library had a bug and sign-in was broken for API 26 and lower for months . Java 1 2 3 4 5 6 7 8 9 10 java.lang.NoSuchMethodError: No virtual method getAndSetObject (Ljava / lang / Object;JLjava / lang / Object;)Ljava / lang / Object; in class Lsun / misc / Unsafe; or its super classes (declaration of 'sun.misc.Unsafe' appears in / system / framework / core - libart.jar) E at com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper.gasWaiters(AbstractFuture.java:1394) E at com.google.common.util.concurrent.AbstractFuture.releaseWaiters(AbstractFuture.java:1110) E at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1000) E at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:783) E at com.google.auth.oauth2.OAuth2Credentials$RefreshTask.access$400(OAuth2Credentials.java:600) E at com.google.auth.oauth2.OAuth2Credentials$RefreshTask$1.onSuccess(OAuth2Credentials.java:617) ... Dropping support for older Android versions # Google Ads library v24 dropped support for Android API 21. According to official Google statistics, API 21 is used by 0.1% (~4 million) users. The rationale behind this has been left unexplained. Upgrades for the sake of it # Material 2 was deprecated for Material 3. No clear migration guide was provided. I tried to upgrade and some components like Sliders won’t look good. Why? I don’t know, and I was never able to figure out the mystic. It does not help that most documentation now refers to Jetpack Compose which I cannot use! So, for the near term, Java-based codebase are likely stuck with Material 2. The UI design guidelines for Android evolve unpredictably # Bottom bar, a featured popular on iOS was discouraged and then became a standard feature in Material design. Back and up buttons used to behave differently and now they are supposed to behave the same . I only learnt about it last year when I posted about it on Reddit. You might think that you can just use Material Design components and be done with it. But migrating from one version of Material Design to another is not trivial either. And before you migrate from Material 1 to Material 2, Google deprecates it for Material 3. Google makes breaking changes to Android platform # Every major release of Android makes breaking changes that requires developer effort Toasts use to work for quick notifications, now, after API 31, it only works if the app is foreground. How to know if you app in foreground? You have to use ActivityLifecycleCallbacks for that and write ton of code and even then there are confusions about onStart vs onResume . Displaying notifications didn’t require permissions, now after API 33, it requires POST_NOTIFICATIONS . Storage permissions were either all or none, now API 33 onwards , they can be fine-grained at the level of audio, video, and images. Background code execution restrictions keeps changing subtly in every release. Media notifications were changed in a backward-incompatible in API 33 onwards. This long thread explains the pain of a lot of developers. Crucial third-party libraries have been deprecated # Several popular third-party have been deprecated or are no longer maintained. Picasso # Picasso was great for image loading and has been deprecated . It has been replaced with coil but the upgrade is not trivial. Glide # Glide an alternative to Picasso was last released in Sep 2023. OkHttp # OkHttp which even Android uses internally for implementing HttpURLConnection has not seen a stable release since Oct 2023, the last stable release was 4.12.0 and even the last alpha release was in April 2024. OkHttp 4.12.0 does not support Happy Eyeballs which is a major issue with IPv6 networks. EventBus # EventBus was the de-facto event passing library for Android. And it is unmaintained now. RateThisApp # RateThisApp was good to get app ratings, and then it was abandoned. I don’t blame the maintainers here. If you use an open-source library, you have to be prepared for the fact that it may not be maintained. I am just pointing out, how some of the obvious boilerplate tasks that one requires for building an Android app are suddenly in a limbo. Two different versioning schemes for everything # Android has two versioning schemes , Android API version is for developers and Android version for marketing. For example, Android 11 is API 30, Android 12 is API 31 as well as 32(!), Android 13 is API 33, Android 14 is API 34. The developer documents would reference one scheme or the other or sometimes both! And you are supposed to memorize the mappings while trying to debug issues using GitHub issues or StackOverflow. It just adds unnecessary friction and confusion. Forced upgrades # There are multiple versions in an Android app, all tightly coupled with each other. minSdkVersion and targetSdkVersion of the app Java sourceCompatibility and targetCompatibility version of dependencies version of Android build tool chain version of Gradle version of Android Studio You might think that all updates are optional, but they aren’t Gradle and Android Studio must be upgraded together for version-compatibility Upgrading Java sourceCompatibility and targetCompatibility requires upgrading Gradle (and hence, Android Studio) Upgrading Android build tool chain requires upgrading minSdkVersion and targetSdkVersion Upgrade Android build tool chain requires upgrading Gradle version Also, if you want to stay on an old library like Exoplayer V2, sooner or later, it will become incompatible with other dependencies, and you will be forced to upgrade to media3! You see how you are forced to upgrade almost everything or nothing? And what if you decide to not upgrade any of these? Well, your app will get delisted if the minSdkVersion is too old. Conclusion # Compared to server-side development, Android development requires a bit more efforts to maintain. So, if you are planning to build an Android app as a hobby, keep the ongoing maintenance cost in mind. Update # After this article ended up on the front page of Hacker News , Two new items came out during those discussions that are worth mentioning. App published for the first time after 2021, have to hand over their private signing keys to Google Play Store Further, multiple people suggested that I should use F-droid to publish apps. The problem is that F-droid not only have very little reach but also it cannot solve for backward-incompatible changes to the underlying platform, abandoned libraries, and backward-incompatible changes to Android libraries.",
      "summary": "7-minute read",
      "url": "https://ashishb.net/programming/maintaining-android-app/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 1380,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Google starts testing ‘Search Live' in AI Mode",
      "content": "Skip to main content Apps & Updates Google AI Mode Google starts testing ‘Search Live’ in AI Mode Abner Li | Jun 5 2025 - 7:35 pm PT After previewing at I/O 2025 , Google is beginning to test AI Mode’s new “Search Live” experience. Powered by Project Astra (just like Gemini Live ), it lets you have a real-time conversation with Google. If rolled out to you, the Google app will show a waveform badged by a sparkle underneath the Search bar. (That is curiously the same icon used by Gemini Live. As such, this must be Google’s icon for “Live” conversational experiences.) It replaces the left Google Lens shortcut that immediately opened your gallery/screenshots. Another way to launch Search Live is from the new circular button to the right of the text field in AI Mode conversations. The fullscreen interface has a light or dark background with the new ‘G’ logo in the top-left corner. There’s a curved waveform in the Google colors, while pill-shaped buttons let you “Mute” and get a “Transcript.” Currently, that second button just opens the AI Mode text chat (ending the Live conversation) instead of showing you real-time captions . Tap the three-dot overflow menu for Voice settings, with four options available: Cosmo, Neso, Terra, and Cassini. After you ask a question, Search Live will surface sites used to inform the answer with a scrollable carousel. Google can ask you clarifying questions to refine your query, while you’re free to ask follow-ups. You can exit the Google app and continue your conversation in the background.(The iOS app makes use of Live Activities.) As of today, Search Live’s camera capability that lets you stream video is not yet available. It’s similar to how Gemini Live first rolled out the voice experience before getting camera sharing. Search Live is currently rolling out to the Google app Android and iOS (we have it on the latter platform). Our account is enrolled in Search Labs . Add 9to5Google to your Google News feed. You’re reading 9to5Google — experts who break news about Google and its surrounding ecosystem, day after day. Be sure to check out our homepage for all the latest news, and follow 9to5Google on",
      "summary": "2-minute read",
      "url": "https://9to5google.com/2025/06/05/google-search-live-testing/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 369,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Amazon ‘testing humanoid robots to deliver packages'",
      "content": "Amazon is reportedly developing software for humanoid robots that could perform the role of delivery workers and “spring out” of its vans. The $2tn (£1.47tn) technology company is building a “humanoid park” in the US to test the robots, said the tech news site the Information, citing a person who had been involved in the project. The Information reported that the robots could eventually take the jobs of delivery workers. It is developing the artificial intelligence software that would power the robots but will use hardware developed by other companies. The indoor obstacle course being used for the tests at an Amazon office in San Francisco is about the size of a coffee shop, the report said, with the company hoping the robots will be able to travel in Amazon’s Rivian vans and make deliveries from them. Even with a human driver behind the wheel, a robot could theoretically speed up drop-off times by visiting one address while the human employee delivers to another. Amazon also has an interest in self-driving vehicles through its Zoox unit. Amazon has more than 20,000 Rivians in the US and has placed one of the vans in the humanoid testing zone, according to the report. Once it has tested the robots in the humanoid park, it plans to take them on “field trips” in the real world where they will attempt to deliver packages to homes. Amazon has already conducted trials with humanoid robots, deploying devices developed by the US company Agility Robotics in its warehouses. The chief executive of Agility, Peggy Johnson, told the Guardian last year that the company’s Digit robot allowed employees to hand off work to humanoids and become a “robot manager”. Last year Amazon was given permission to test-fly drones beyond a human controller’s line of sight in the UK, paving the way for using the technology in home delivery . Prof Subramanian Ramamoorthy, the chair of robot learning and autonomy at the University of Edinburgh, said Amazon had a respected robotics team and its reported focus on “last mile” delivery was not a surprise. The humanoid robot hardware capable of carrying out such a task is becoming available and the field is developing rapidly, he said. However, he added, the challenge was to achieve reliable performance outside highly constrained environments such as the reported “humanoid park”. skip past",
      "summary": "3-minute read",
      "url": "https://www.theguardian.com/technology/2025/jun/05/amazon-testing-humanoid-robots-to-deliver-packages?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 390,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 390/600 words (65.0%)"
    },
    {
      "title": "Walmart and Wing expand drone delivery to five more US cities",
      "content": "Wing, the on-demand drone delivery company owned by Alphabet, is spreading its commercial wings with help from Walmart. The two companies announced Thursday plans to roll out drone delivery to more than 100 Walmart stores in five new cities: Atlanta, Charlotte, Houston, Orlando, and Tampa. Walmart is also adding Wing drone deliveries to its existing market in the Dallas-Fort Worth area. The expansion signals Walmart’s growing confidence in drone delivery. Greg Cathey, who is senior vice president of Walmart’s U.S. Transformation and Innovation department, said drone delivery would remain a key part of its “commitment to redefining retail.” “We’re pushing the boundaries of convenience to better serve our customers, making shopping faster and easier than ever before,” Cathey said in a blog posted Thursday. The expansion also marks a turning point for Wing, from Alphabet X graduate to commercial enterprise. Wing partnered with Walmart in 2023 and launched a pilot program to test on-demand drone delivery at two stores in the Dallas metro area that reached about 60,000 homes. It has since grown to 18 Walmart Supercenters in Dallas-Fort Worth. The expansion announced Thursday is nearly a five-fold increase of Wing’s operations with Walmart. “We’re decidedly out of the pilot and trial phase and into scaling up this business,” Wing CEO Adam Woodworth told TechCrunch in a recent interview. “We’ve always been the type of company that wants to do something well and stay focused. And so this is the next big bite at the apple. It’s a much bigger bite than we’ve taken before.” Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW Woodworth said the pilot program in Dallas-Fort Worth, and specifically how it scaled, helped form Wing’s drone delivery strategy in the retail sector. “We figured out how the expansion worked out and looked in DFW, and now we’re sort of copy-pasting that across more markets,” he added. Woodworth wouldn’t say whether Wing was profitable yet or when it would be. But he did say the company is focused on how to scale its deliveries while keeping its expenses in check. Wing’s hypothesis is to build a business centered on small, lightweight, automated, low-cost airplanes — aka drones. There are fixed operational costs tied to those physical assets such as flight operations and training. The crux, and what Wing is trying to navigate, is how to scale the number of drones and flights without adding even more personnel. “The more places you can be operating, the more you can be flying, the more you can defray those costs. This is a meaningful step in that direction,” he said, adding that Wing is trying to keep its resources flat as the scale continues to go up. Wing is also pushing into the restaurant food delivery sector through its partnership with DoorDash . The two companies paired up in 2022 to launch drone deliveries in Australia and have since worked together in Dallas-Forth Worth and more recently in Charlotte .",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/05/walmart-and-wing-expand-drone-delivery-to-five-more-u-s-cities/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 563,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Longevity Is Now a Factor When Picking an Embryo for IVF",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/health/embryo-ivf-screening-longevity-2b1f096a?st=WfueaT&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.wsj.com/health/embryo-ivf-screening-longevity-2b1f096a?st=WfueaT&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "science",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Google rolling out upgraded Gemini 2.5 Pro preview",
      "content": "Skip to main content Apps & Updates Gemini Google rolling out upgraded Gemini 2.5 Pro preview Abner Li | Jun 5 2025 - 9:01 am PT Google today released an “ upgraded preview ” of Gemini 2.5 Pro that will soon be generally available in the coming weeks. This model update builds off 2.5 Pro’s I/O Edition (05-06) last month that brought coding upgrades. That coding lead continues with 06-05 today across benchmarks like AIDER Polyglot, while there’s “top-tier performance on GPQA and Humanity’s Last Exam (HLE), highly challenging benchmarks that evaluate a model’s math, science, knowledge and reasoning capabilities.” Meanwhile, Google notes how 06-05 today has a “24-point Elo score jump on LMArena” at 1470 today, while there’s a “35-point Elo jump to lead on WebDevArena at 1443.” With the last few model upgrades, some people noticed performance declines outside of coding, especially compared to 03-25 . Google is now addressing that feedback with improved “style and structure,” so that Gemini 2.5 Pro “can be more creative with better-formatted responses.” This Gemini 2.5 Pro model update is still in preview, but you can expect general availability and a stable version “in a couple of weeks.” It is rolling out now to the Gemini API via Google AI Studio and Vertex AI, with developers able to control cost and latency with thinking budgets (as introduced with 2.5 Flash ). The Gemini app is also getting this 2.5 Pro preview upgrade today. L-R: 06-05, 05-06, 03-25 More on Gemini: Gemini app doubles recently introduced 2.5 Pro query limit Google rolls out Gemini Live camera and screen sharing to everyone Gemini app ‘Pro’ users getting Veo 3 trial on Android & iOS, AI Ultra now in 73 countries Add 9to5Google to your Google News feed. You’re reading 9to5Google — experts who break news about Google and its surrounding ecosystem, day after day. Be sure to check out our homepage for all the latest news, and follow 9to5Google on",
      "summary": "1-minute read",
      "url": "https://9to5google.com/2025/06/05/gemini-2-5-pro-preview-06-05/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 326,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "AMC Networks Inks Deal With AI Company Runway",
      "content": "By Steven Zeitchik Plus Icon Steven Zeitchik Senior Editor, Technology and Politics View All June 4, 2025 6:00am Scott M. Gimple, David Zabel, Greg Nicotero and Norman Reedus speak onstage at AMC and AMC+ Present The Walking Dead: Daryl Dixon at Jacob Javits Center on October 12, 2023 in New York City. The company is placing its chips on AI in a new deal with upstart Runway Photo by Jason Mendez/Getty Images for AMC AMC Networks is the latest entertainment entity — and the first cable player — to formally make a deal with emerging Hollywood AI player Runway, THR has learned. AMC will use the New York firm’s tech both to generate marketing images and help pre-visualize shows that have yet to be produced. “It’s kind of a natural transition for every entertainment and media company; they need to think through what AI means for them,” Runway co-founder Cristóbal Valenzuela told THR when asked about the AMC partnership. “This is the most important tech of their lifetime — it’s a make-or-break moment.” Related Stories Movies A New York Startup Just Threw a Splashy Event to Hail the Future of AI Movies TV Taiwanese Drama Series 'Islanders' Set for Global Launch on Amazon With AMC and BBC America in its portfolio along with streaming services Shudder, Acorn TV and AMC+, AMC Networks has a notable range of platforms, in addition of course to bringing to the world Peak TV mainstays like Mad Men , The Walking Dead and Breaking Bad to the world. But with cord-cutting long a fact of life and even far larger streamers struggling for mindshare, the company has been hit hard. In its most recent earnings report, profits at AMC Networks nearly halved to $63 million, and the company is believed to be looking for ways to produce content less expensively without suffering a drop in quality. Still, AMC’s streaming services boasts more than ten million",
      "summary": "3-minute read",
      "url": "https://www.hollywoodreporter.com/business/business-news/amc-networks-runway-ai-deal-1236255108/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 320,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 320/600 words (53.3%)"
    },
    {
      "title": "Google's Chrome Browser Gets 'Highest Score Ever' on Speedometer Performance Test",
      "content": "Google's Chrome Browser Gets 'Highest Score Ever' on Speedometer Performance Test Thursday June 5, 2025 3:02 pm PDT by Juli Clover Google today announced that its Chrome browser has received the \"highest ever score\" on the Speedometer 3 benchmarking test. Speedometer 3 is designed to measure browser performance, and it was created collaboratively by Google, Apple, Intel, Microsoft, and Mozilla. The test is designed to measure web responsiveness through several workloads like HTML parsing, JavaScript and JSON processing, pixel rendering, CSS application, and more. Optimizations that Google has implemented over the last year have brought a 10 percent improvement in performance since August 2024, which Google says leads to better browser experiences for end users. The team heavily optimized memory layouts of many internal data structures across DOM, CSS, layout, and painting components. Blink now avoids a lot of useless churn on system memory by keeping state where it belongs with respect to access patterns, maximizing utilization of CPU caches. Where internal memory was already relying on garbage collection in Oilpan, e.g. DOM, the usage was expanded by converting types from using malloc to Oilpan. This generally speeds up the affected areas as it packs memory nicely in Oilpan's backend. On an M4 MacBook Pro with macOS 15, Chrome 139 achieved a score of 52.35 on the benchmarking test. More detail on the optimizations that were added can be found in Google's blog post . Apple has not recently shared its maximum Speedometer 3 test results for Safari so there isn't a direct comparison available, and it is worth noting that Google appears to have used Speedometer 3, and not the newer Speedometer 3.1 test. [ 81 comments ] Get weekly top MacRumors stories in your inbox. Popular Stories Friday June 6, 2025 7:27 am PDT by Joe Rossignol Apple is planning to announce several new features for the Messages and Phone apps on iOS 26, according to Bloomberg's Mark Gurman. In a lengthy report outlining his WWDC 2025 expectations today, Gurman said that the two main changes in the Messages app will be the ability to create polls, as well as the option to set a background image within a conversation. 9to5Mac was first to report... • 47 comments Monday June 9, 2025 9:00 am PDT by Jordan Golson Apple's Worldwide Developers Conference (WWDC) starts today with the traditional keynote kicking things off at 10:00 a.m. Pacific Time. MacRumors is on hand for the event and we'll be sharing details and our thoughts throughout the day. We're expecting to see a number of software-related announcements led by a design revamp across Apple's platforms that will also see the numbering of all of... • 1212 comments Friday June 6, 2025 6:17 am PDT by Joe Rossignol While the so-called \"iPhone 17 Air\" is not expected to launch until September, there are already plenty of rumors about the ultra-thin device. Overall, the iPhone 17 Air sounds like a mixed bag. While the device is expected to have an impressively thin and light design, rumors indicate it will have some compromises compared to iPhone 17 Pro models, including worse battery life, only a single ... • 77 comments Monday June 9, 2025 10:13 am PDT by Hartley Charlton Apple today announced a complete redesign of all of its major software platforms called \"Liquid Glass.\" Announced simultaneously for iOS, iPadOS, macOS, watchOS, tvOS, visionOS, and CarPlay, Liquid Glass forms a new universal design language for the first time. At its WWDC 2025 keynote address, Apple's software chief Craig Federighi said \"Apple Silicon has become dramatically more powerful... • 191 comments Saturday June 7, 2025 5:27 am PDT by Joe Rossignol macOS 26 will drop support for several older Intel-based Mac models currently compatible with macOS Sequoia, according to a private account on X with a proven track record of leaking information about Apple's software platforms. macOS 26 will be compatible with the following Mac models, the account said:MacBook Air (M1 and later) MacBook Pro (2019 and later) iMac (2020 and later) Mac... • 172 comments Friday June 6, 2025 8:32 am PDT by Joe Rossignol In a lengthy report outlining his WWDC 2025 expectations today, Bloomberg's Mark Gurman shared more details about iOS 26's rumored new design. According to Gurman, iOS 26 will feature a \"digital glass\" design inspired by visionOS, the operating system for Apple's Vision Pro headset. That is a well-known rumor by now, but he goes on to provide some more specific details, as listed below:There ... • 64 comments Top Rated Comments DHagan4755 4 days ago at 03:03 pm But it consumes energy like a drunken sailor compared to Safari. Score: 25 Votes ( Like | Disagree ) jontalk5 4 days ago at 03:46 pm I switched from Chrome to Firefox when I upgraded to the Mac Mini M1 and find it extremely fast and have it on my phone as well..I also moved from Google to Duck Duck Go for search engine and see little if any difference from Google AND its not impacting my YT either Score: 15 Votes ( Like | Disagree ) gravage 4 days ago at 04:22 pm What score does it get on adblocking? That's all I care about and that's why I don't use it. Score: 15 Votes ( Like | Disagree ) SnowPenguin 4 days ago at 03:06 pm Nice, since us Brave users will likely get that performance boost as well, but with better privacy. Score: 13 Votes ( Like | Disagree ) justanotherdave 4 days ago at 04:42 pm Don’t care how fast it is, nothing will get me to use that vacuum cleaner data mining software. Score: 13 Votes ( Like | Disagree ) Six0Four 4 days ago at 03:11 pm I try Safari and Firefox like every year but I still keep coming back to Chrome. Just works 99% perfect. Safari on iPad tho since extensions will work. Score: 9 Votes ( Like | Disagree ) Read All Comments Next Article WWDC 2025: All the Rumors About visionOS 26",
      "summary": "2-minute read",
      "url": "https://www.macrumors.com/2025/06/05/chrome-speedometer-improvements/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 1002,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "The founder of DeviantArt is making a $22,000 display for digital art",
      "content": "Angelo Sotira started the online digital art platform DeviantArt when he was just a teenager, growing a formative community for millions of artists in the 2000s. Twenty-five years later, Sotira wants to change digital art again, but with a focus on the way it’s displayed. On Thursday, Sotira revealed his new venture, Layer , a screen specifically designed to showcase digital art in the best quality possible. “The way that the canvas needs to perform and behave in your life is quite different than other types of displays,” Sotira told TechCrunch. “It needs to blend into beautiful environments.” The closest point of reference that the average consumer would have for a product like this is Samsung’s The Frame TV, which looks like a painting hung on the wall when it’s not turned on. But Layer takes that kind of feel to an even more premium level — unlike The Frame, Layer is not a consumer product, and it’s not trying to emulate static paintings or photographs. “They’re $22,000, so that kind of tells you a lot about who that’s for,” Sotira said. “We spared no expense and we spared no effort. We made no compromise in producing what is actually, in our opinion, the very best way to display digital art on a wall.” Image Credits: Layer When Sotira talks about digital art, he isn’t talking about digital photography or videos. Layer is working with hundreds of artists like Casey Reas , who makes generative art — no, not the kind of generative AI art like you’d get from ChatGPT, which is created with LLMs that use other artists’ work without their consent. Instead, many of these artists are writing their own software to create digital artworks that change over time according to what the code says. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW But these artworks can require a lot of computing power to execute. That’s part of why Layer is so pricey — it needs the technological capacity to display these new kinds of works. “You’re looking at an over 35-year history of extraordinary artists developing the medium of code-based art and essentially, the pixels on the display are being governed by the code that’s been written that runs live on that GPU, rendering it in full resolution,” Sotira said. “It’s actually controlling every pixel, so it’s not going through any compression algorithms.” Sotira is well aware that he’s not the first entrepreneur to try to create a better way to display digital art — when he was at DeviantArt, he was pitched on products like Layer all the time. But because of this, he knows what was missing from the products that were pitched to him in the past. “One of the driving principles is that you can plug it in, turn it on, and leave it alone, and it should know how to sequence art for you,” he said. In his experience, he enjoys tinkering with these devices for a few weeks, but then it becomes tedious to continue updating the display, so he wanted his own canvas to be more self-sustaining. “It’s going to be on your wall for five years, so it has to play really, really well in your life.” Image Credits: Layer Layer seems like a highly expensive and very niche product, but some venture capitalists and entrepreneurs are betting on it. While in stealth, the startup raised $5.7 million in funding from Expa Ventures, Human Ventures, and Slauson & Co., plus angels like",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/05/the-founder-of-deviantart-is-making-a-22000-display-for-digital-art/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 656,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple says its App Store helped generate $1.3T in billings and sales, most without a commission",
      "content": "Ahead of its Worldwide Developers Conference on June 9, Apple on Thursday offered a new update on its global App Store business, sharing that developers generated $1.3 trillion in billings and sales in 2024. The company stressed that 90% of those billings and sales did not involve paying Apple a commission. The study also noted that developer billings and sales for digital goods and services in 2024 totaled $131 billion, driven by mobile games, photo and video editing apps, and other enterprise tools. Physical goods and services, meanwhile, topped $1 trillion, thanks to increased demand for online food delivery and pickup, and online grocery apps. In-app advertising revenue was $150 billion last year. Spending across digital goods and services, physical goods and services, and in-app advertising has more than doubled since 2019, with physical goods and services seeing the strongest growth at more than 2.6 times, Apple said. The numbers are intended to highlight how the App Store creates financial opportunities for mobile developers that extend beyond sales from in-app purchases. The storefront provides a place for developers to have their apps discovered by consumers, and Apple provides the technical infrastructure required to run an app business. This position ignores the fact that the App Store is now a mature ecosystem, and apps are a selling point for the iPhone itself. Developers today have a number of tools at their disposal to host, distribute, and manage their own applications, if they choose, but Apple’s policies prevent this. That’s starting to change, however. In a recent court ruling in favor of Epic Games in the U.S., Apple was required to let developers link to their own websites for processing in-app purchases without having to pay Apple a commission. In Europe, the tech giant is fighting against the rules proposed by the Digital Markets Act (DMA), which, among other things, directs Apple to give developers the right to inform their customers about alternative payment mechanisms. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The new data comes from an Apple-funded study by economists Professor Andrey Fradkin from the Boston University Questrom School of Business and Dr. Jessica Burley from Analysis Group. The latter has been working with Apple for years throughout its antitrust legal battles to document the App Store’s success in a more positive light for the company. The study highlights other regional growth trends, like how the billings and sales facilitated by the App Store more than doubled over the past five years in the U.S., China, and Europe. Digital payment spending, meanwhile, also grew by more than seven times in the U.S. since 2019, thanks to the broad adoption of mobile payments. The report also reiterated other metrics, like how the App Store attracts 813 million average weekly visitors worldwide, and pointed to the various investments Apple has made in tools and technologies to support developers, like coding and distribution platforms, frameworks, analytics, anti-fraud systems, developer support, and more.",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/05/apple-says-its-app-store-helped-generate-1-3t-in-billings-and-sales-most-without-a-commission/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 563,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Perplexity received 780 million queries last month, CEO says",
      "content": "Perplexity received 780 million queries in May, CEO Aravind Srinivas shared onstage at Bloomberg’s Tech Summit on Thursday. Srinivas said that the AI search engine is seeing more than 20% growth month-over-month. “Give it a year, we’ll be doing, like, a billion queries a week if we can sustain this growth rate,” Srinivas said. “And that’s pretty impressive because the first day in 2022, we did 3,000 queries, just one single day. So from there to doing 30 million queries a day now, it’s been phenomenal growth.” Srinivas went on to note that the same growth trajectory is possible, especially with the new Comet browser that it’s working on. “If people are in the browser, it’s infinite retention,” he said. “Everything in the search bar, everything on the new tab page, everything you’re doing on the sidecar, any of the pages you’re in, these are all going to be extra queries per active user, as well as seeking new users who just are tired of legacy browsers, like Chrome. I think that’s going to be the way to grow over the coming year.” Srinivas said the reason Perplexity is developing Comet is to shift the role of AI from simply providing answers to actually completing actions on your behalf. He explained that when you get an AI-powered answer, it’s essentially four or five searches in one. On the other hand, AI performing an action would be getting an entire browsing session done with one prompt. “You really need to actually have a browser and hybridize the compute on the client and the server side in the most seamless way possible,” he said. “And that calls for rethinking the whole browser.” He went on to explain that Perplexity isn’t thinking of Comet as “yet another browser,” but as a “cognitive operating system.” Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW “It’ll be there for you every time, anytime, for work or life, as a system on the side, or like, just going and doing browsing sessions for you,” Srinivas said. “And I think that’ll fundamentally make us rethink how we even think about the internet. Like, earlier we would browse the internet, but now people are increasingly living on the internet. Like a lot of our life actually exists there. And if you want to build a proactive, personalized AI, it needs to live together with you, and that’s why we need to rethink the browser entirely.” While the company hasn’t revealed too much about the browser, Srinivas said in April that one reason Perplexity is developing its own browser is to track user activity beyond its own app so that it can sell premium ads, which would essentially mirror what Google quietly did to become the giant it is today. It’s currently unknown when exactly Comet will launch, but Srinivas previously said on X that it will launch in the coming weeks. Comet will have a native virtual meets recording, transcription and searches over them. Won’t be part of the first release, but very fast follow up. As for release date: it’s going to take a min of three weeks and a max of five weeks. Reliability and latency have improved over… — Aravind Srinivas (@AravSrinivas) May 13, 2025",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/05/perplexity-received-780-million-queries-last-month-ceo-says/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 612,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "What a developer needs to know about SCIM",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~3800 words, got 256. URL: https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim?utm_source=tldrnewsletter",
      "summary": "19-minute read",
      "url": "https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 256,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 256/3800 words (6.7%)"
    },
    {
      "title": "X Is Testing a Way To Take Community Notes to the Next Level",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.adweek.com/media/x-is-testing-a-way-to-take-community-notes-to-the-next-level/?utm_source=tldrnewsletter",
      "summary": "3-minute read",
      "url": "https://www.adweek.com/media/x-is-testing-a-way-to-take-community-notes-to-the-next-level/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "AirPods to get camera control, sleep detection, new gestures, more",
      "content": "Skip to main content AirPods Exclusive Exclusive: AirPods to get camera control, sleep detection, new gestures, more Marcus Mendes | Jun 4 2025 - 12:31 pm PT Yesterday, 9to5Mac exclusively reported a handful of iOS 26 features that Apple has been working on and may announce on WWDC for its native apps. Today, we are sharing five previously unreported AirPods features that may also be announced as early as Monday, during the keynote. Control AirPods with head gestures Last year, Apple introduced head gestures so AirPods Pro 2 and AirPods 4 users can nod “up and down or side to side to accept or decline Announce Calls and to interact with or dismiss Announce Notifications without using your hands”. 9to5Mac has learned that Apple is working on new head gestures, including extending a Conversation Awareness volume adjustment before returning users to their original noise-control settings. Currently, you can end a Conversation Awareness volume adjustment by “pressing and holding on the AirPods stem or swiping up on the stem”, but soon, a head gesture might also do the trick. Sleep auto-pause Many people tend to fall asleep with their AirPods on. Some doze off while listening to music, and some do so while listening to podcasts. Currently, users must resort to sleep timers, a feature not all music or podcast apps support. Luckily, our sources say Apple has been working on detecting when a user has fallen asleep while wearing their AirPods and automatically pausing playback. It is not clear whether this will be a standalone feature or if it will work in tandem with the Apple Watch, which already has sleep detection. Still, it’ll sure be a welcome feature. Camera control Years later than anyone would have initially guessed, Apple is finally poised to allow for AirPods Camera Control. Like with the old EarBuds, this new feature will let users trigger the iPhone or iPad camera with a click on the AirPods stem. While touching one’s ear in every remotely taken photo might look weird, I do hope Apple supports this feature even if the user is just casually holding one AirPod. Audio Mix Last year, Apple introduced the Audio Mix alongside iPhone 16, which “uses machine learning to identify and separate background elements from speech and lets you choose from three voice options,” In-frame, Studio and Cinematic. Now, 9to5Mac has learned that Apple has been developing a similar “studio quality” mic mode for AirPods , turning them into a strong competitor in the creator lav mic market. Wider classroom support Apple has been working on a feature aimed at improving the pairing experience in shared iPads, which will be especially welcome in classrooms where multiple students use the same device. Top comment by Ryan W Liked by 6 people AirPods Pro are probably the most satisfying product experience I've ever had. Most companies stop adding major features after releasing a product. With the AirPods, Apple gives us new stuff year after year. The AirPods I own now are significantly more capable than the ones I bought three years ago. View all comments The goal is to streamline how AirPods connect to iPadOS in these settings, likely with less friction and fewer manual steps. As always, Apple may choose to delay or tweak some of these features before making them public. But if even a few of them make it to the WWDC25 keynote on Monday, it would mark yet another year that great features are added to current models, rather than new features being associated with a newly announced hardware. WWDC25: What to expect Apple is expected to unveil major updates across all its platforms, including iOS 26 , macOS 26 , iPadOS 26 , watchOS 26 , and more. With AI reportedly playing a smaller role this year, the spotlight will likely fall on design refreshes, and long-time wishlist picks like better window management support on the iPad. The keynote kicks off Monday, June 9, and 9to5Mac will feature full coverage before, during, and after the event. See you then! Best AirPods deals currently on Amazon AirPods Pro 2, USB-C Charging : 20% off at $199 AirPods (3rd Generation) : 19% off at $136 AirPods 4, USB-C and Wireless Charging : 6% off at $168 AirPods 4 USB-C Charging : 8% off at $119 AirPods Max Wireless, USB-C Charging, Midnight : 9% off at $499 Add 9to5Mac to your Google News feed. You’re reading 9to5Mac — experts who break news about Apple and its surrounding ecosystem, day after day. Be sure to check out our homepage for all the latest news, and follow 9to5Mac on",
      "summary": "3-minute read",
      "url": "https://9to5mac.com/2025/06/04/exclusive-airpods-ios-26-wwdc-features/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "dev",
      "word_count": 767,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "ChatGPT introduces meeting recording and connectors for Google Drive, Box, and more",
      "content": "OpenAI’s ChatGPT is adding new features for business users, including integrations with different cloud services, meeting recordings, and MCP connection support for connecting to tools for deep research. As part of the launch, ChatGPT is gaining connectors for Dropbox, Box, SharePoint, OneDrive, and Google Drive. This allows ChatGPT to look for information across users’ own services to answer their questions. For instance, an analyst could use the company’s slide deck and documents to build out an investment thesis. OpenAI said that the new feature will follow an organization’s access control hierarchy. Recording and transcription of meetings, now a table-stakes feature of the productivity suites, is also now available. The feature can generate notes with time-stamped citations and suggest actions. Users will be able to query for information in their meeting notes, as they can with documents and files across the integrated services. Plus, users can convert action items into a Canvas document, OpenAI’s tool for writing and coding projects . The feature competes with ClickUp, Zoom, and, more recently, Notion , which have all added some type of transcription and meeting summarization features to their products. In addition, the company is introducing deep research connectors for HubSpot, Linear, and select Microsoft and Google tools in beta. OpenAI said that users can now prepare detailed research reports through Deep Research using knowledge and data from these sources, along with web information. OpenAI said connectors are available to all paid users. Customers can also use MCP (model context protocol) to connect to other tools for deep research. MCP support will be available to Pro, Team, and Enterprise users, the company said. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The addition of these features indicates that OpenAI wants businesses to use ChatGPT even more than they already might be in their enterprise workspaces. The company is likely to succeed here. While startups like Notion and Lux Capital-backed Context are also trying to build AI-powered office tools that provide research and analysis features, OpenAI has a first-mover advantage. Three million customers already",
      "summary": "2-minute read",
      "url": "https://techcrunch.com/2025/06/04/chatgpt-introduces-meeting-recording-and-connectors-for-google-drive-box-and-more/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 412,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Scientists in Japan develop plastic that dissolves in seawater within hours",
      "content": "WAKO, Japan, June 4 (Reuters) - Researchers in Japan have developed a plastic that dissolves in seawater within hours, offering up a potential solution for a modern-day scourge polluting oceans and harming wildlife. While scientists have long experimented with biodegradable plastics, researchers from the RIKEN Center for Emergent Matter Science and the University of Tokyo say their new material breaks down much more quickly and leaves no residual trace. Sign up here. At a lab in Wako city near Tokyo, the team demonstrated a small piece of plastic vanishing in a container of salt water after it was stirred up for about an hour. While the team has not yet detailed any plans for commercialisation, project lead Takuzo Aida said their research has attracted significant interest, including from those in the packaging sector. Scientists worldwide are racing to develop innovative solutions to the growing plastic waste crisis, an effort championed by awareness campaigns such as World Environment Day taking place on June 5. Item 1 of 11 A researcher shows a sample of ocean-degradable plastic at the Center for Emergent Matter Science (CEMS) of Japanese research institution Riken in Wako, Saitama Prefecture, Japan May 27, 2025. REUTERS/Manami Yamada [1/11] A researcher shows a sample of ocean-degradable plastic at the Center for Emergent Matter Science (CEMS) of Japanese research institution Riken in Wako, Saitama Prefecture, Japan May 27, 2025. REUTERS/Manami Yamada Purchase Licensing Rights , opens new tab Plastic pollution is set to triple by 2040, the UN Environment Programme has predicted, adding 23-37 million metric tons of waste into the world's oceans each year. \"Children cannot choose the planet they will live on. It is our duty as scientists to ensure that we leave them with best possible environment,\" Aida said. Aida said the new material is as strong as petroleum-based plastics but breaks down into its original components when exposed to salt. Those components can then be further processed by naturally occurring bacteria, thereby avoiding generating microplastics that can harm aquatic life and enter the food chain. As salt is also present in soil, a piece about five centimetres (two inches) in size disintegrates on land after over 200 hours, he added. The material can be used like regular plastic when coated, and the team are focusing their current research on the best coating methods, Aida said. The plastic is non-toxic, non-flammable, and does not emit carbon dioxide, he added. Reporting by Irene Wang; Editing by John Geddie and Lincoln Feast. Our Standards: The Thomson Reuters Trust Principles. , opens new tab Share Purchase Licensing Rights Sustainability Paramount to lay off 3.5% of US staff in latest job cut, memo shows Boards, Policy & Regulation category · June 10, 2025 · 12:37 PM UTC · ago Paramount Global is laying off 3.5% of its U.S. staff in the latest round of job cuts as the company grapples with a decline in cable TV",
      "summary": "2-minute read",
      "url": "https://www.reuters.com/sustainability/climate-energy/scientists-japan-develop-plastic-that-dissolves-seawater-within-hours-2025-06-04/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 484,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "One thing Tesla and Comma.ai overlooked in self-driving",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://deepdream.substack.com/p/one-thing-tesla-and-commaai-overlooked?utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://deepdream.substack.com/p/one-thing-tesla-and-commaai-overlooked?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Mistral releases a vibe coding client, Mistral Code",
      "content": "French AI startup Mistral is releasing its own “vibe coding” client, Mistral Code, to compete with incumbents like Windsurf, Anysphere’s Cursor, and GitHub Copilot. Mistral Code, a fork of the open source project Continue, is an AI-powered coding assistant that bundles Mistral’s models, an “in-IDE” assistant, local deployment options, and enterprise tools into a single package. A private beta is available as of Wednesday for JetBrains development platforms and Microsoft’s VS Code. “Our goal with Mistral Code is simple: deliver best-in-class coding models to enterprise developers, enabling everything from instant completions to multi-step refactoring through an integrated platform deployable in the cloud, on reserved capacity, or air-gapped, on-prem GPUs,” Mistral wrote in a blog post provided to TechCrunch. AI programming assistants are growing increasingly popular. While they still struggle to code quality software , their promise to boost coding productivity is pushing companies and developers to adopt them rapidly. One recent poll found that 76% of developers have used or were planning to use AI tools in their development processes last year. The Mistral Code client and dashboards Image Credits: Mistral Mistral Code is said to be powered by a combination of in-house models including Codestral (for code autocomplete), Codestral Embed (for code search and retrieval), Devstral (for “agentic” coding tasks), and Mistral Medium (for chat assistance). The client supports more than 80 programming languages and a number of third-party plug-ins, and can reason over things like files, terminal outputs, and issues, the company said. Mistral claimed that customers including consulting firm Capgemini, Spanish and Portuguese bank Abanca, and French national railway company SNCF are using Mistral Code in production. “Customers can fine-tune or post-train the underlying models on private repositories or distill lightweight variants,” Mistral explained in its blog post. “For IT managers, a rich admin console exposes granular platform controls, deep observability, seat management, and usage analytics.” Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW Mistral said it plans to continue making improvements to Mistral Code and contribute at least a portion of those upgrades to the Continue open source project. Image Credits: Mistral Founded in 2023, Mistral is a frontier model lab building a range of AI-powered services, including a chatbot platform, Le Chat, and mobile apps. It is backed by venture investors like General Catalyst, and has raised over €1.1 billion (roughly $1.24 billion) to date. A few weeks ago, Mistral launched the aforementioned Codestral, Devstral, and Mistral Medium models. Around the same time, the company rolled out Le Chat Enterprise, a corporate-focused chatbot service that offers tools like an AI agent builder, and integrates Mistral’s models with third-party services like Gmail, Google Drive, and SharePoint.",
      "summary": "2-minute read",
      "url": "https://techcrunch.com/2025/06/04/mistral-releases-a-vibe-coding-client-mistral-code/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 513,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "LLMs are mirrors of operator skill",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://ghuntley.com/mirrors/?utm_source=tldrnewsletter",
      "summary": "9-minute read",
      "url": "https://ghuntley.com/mirrors/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
    },
    {
      "title": "Nintendo Switch 2 draws long lines and excited gamers",
      "content": "IE 11 is not supported. For an optimal experience visit our site on another browser. Skip to Content Get more news Live on June 4, 2025, 7:40 PM EDT By Brian Cheung , Steve Kopack and Kalhan Rosenblatt Listen to this article / For the crowds outside Nintendo’s store in New York’s Rockefeller Center on Wednesday, a long wait was well worth getting their hands on the new Switch 2 console. Some camped out for hours, bringing their own makeshift waterproof tents and lawn chairs. Others stood in the line, which stretched several city blocks. A handful even dressed up as iconic Nintendo characters, including Luigi, the beloved brother and sidekick in the video game “Super Mario.” “Really, it’s just all about the community and sort of sharing that love of Nintendo with each other,” said Lauren Hyde, who has been in line since she traveled from Maryland on Monday. The Japanese gaming giant is releasing the coveted console Thursday, eight years after the original version hit shelves. The first iteration of the Switch was a major hit for Nintendo, selling roughly 150 million units, according to the company , making it the third-best-selling console of all time, behind the PS2 and the Nintendo DS. Nintendo Switch 2, the successor to the Nintendo Switch system, will be released in 2025. Nintendo The hype for the Switch 2 has been building for months, with Nintendo teasing a brief first look at it in January . With a bigger screen, better graphics and larger controls, the next iteration of the console has drummed up enough enthusiasm among gamers that many showed up early at stores like Best Buy and GameStop to pick up their units at midnight. “We have a lot of wonderful products that create buzz,” said Jason Bonfig, Best Buy’s senior executive vice president in charge of customer offerings and fulfillment. “Switch is very, very special. They’re a passionate community.” Bonfig estimated that 70% of Best Buy’s customers indicated they wanted to pick up their Switch 2s in-store, “and the vast majority of those customers will be there at midnight to do so.” “Something about consumer electronics in general with that instant gratification is really important,” he added. Meanwhile, the Nintendo Store is selling units at midnight only to those who secured reservations on its website, meaning many lining up outside its New York City location were doing so mostly for fun. Nintendo even warned in a message on its website that “delivery by June 5 is not guaranteed” for customers who had not already secured pre-order invitations. Though consumers have been fatigued by high prices amid the ongoing global trade war, Nintendo said in April that it would not hike the price of the $449.99 device at launch. There is also hope among some in the gaming world that the Switch 2 could spur the creation of new games and therefore new jobs. The industry has taken a hit in recent years after widespread layoffs affected many studios, and a performers strike has continued into 2025. While some companies release consoles to reviewers early, Nintendo has remained quiet about many of the details of the Switch 2, holding off on review copies for gaming news sites like IGN . Nintendo has not commented on how many customers have pre-ordered the Switch 2. Brian Cheung Brian Cheung is a business and data correspondent for NBC News. Steve Kopack Steve Kopack is a producer at NBC News covering business and the economy. Kalhan Rosenblatt Kalhan Rosenblatt is a reporter covering youth and internet culture for NBC News, based in New York.",
      "summary": "3-minute read",
      "url": "https://www.nbcnews.com/business/business-news/nintendo-switch-2-release-lines-excited-gamers-rcna210910?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 600,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "“AI will replace all the jobs!” Is Just Tech Execs Doing Marketing",
      "content": "Over the weekend, I went digging for evidence that AI can, will, or has replaced a large percent of jobs. It doesn’t exist. Worse than that, actually, there’s hundreds of years of evidence and sophisticated analyses from hundreds of sources showing the opposite is true: AI will almost certainly create more jobs than it displaces, just like thousands of remarkable technologies before it. I don’t want anyone to think I’m raining on this parade without first attempting to convince myself that the opposite was true, and that AI really would be the first technology in 120 years to displace a massive portion of the workforce. So, dry though it may be, let’s walk through the logic together. The majority of statements that have received press (and there have been dozens in the last 5 years) center on the claim that AI will destroy 20-50% of the current need for human labor. I’ll attempt to address each of the most robust points inherent in those arguments, rather than trying to argue that any one innovation or upgrade to a model’s capability hasn’t done it yet (or won’t): If AI is going to make such a huge percentage of jobs redundant, there must be historical analogies–i.e. other technologies that massively upended labor markets. What are these and how have they affected jobs in the past? MIT’s Technology Review noted that this “fear of new tech taking jobs,” is far from new. The automation of farm work is the most notable and most labor-impacting example we have from history, rapidly unemploying a huge portion of human beings in the developing economies of the late 19th and 20th centuries. And yet, at the conclusion of this era (~1940s/50s), the conclusion was that “technological unemployment is a myth,” because “technology has created so many new industries” and has expanded the market by “lowering the cost of production to make a price within reach of large masses of purchasers.” In short, technological advances had created more jobs overall. Last year, Quarterly Journal of Economics published a groundbreaking study on how technological innovations have impacted labor forces across industries since 1980. MIT did a nice summarization : “the number of studies that support the labour replacement effect is more than offset by the number of studies that support the labour-creating/reinstating and real income effects.” This 2023 paper looked at 127 previous studies of technology supposedly replacing labor forces from the 18th century to the present, concluding that “the labor displacing effect of technology appears to be more than offset by compensating mechanisms that create or reinstate labor.” The Economic Policy Institute did a deep dive into what drives labor market demand and unemployment, concluding: “Productivity growth (which technology sometimes enables and other times drives) has not historically been associated with higher unemployment or higher inequality,” and that “Anxieties over widespread technology-driven unemployment lack an empirical base.” Perhaps the closest analogy to AI is the personal computer revolution of the 1980s. Millions of jobs in communication, documentation, research, analysis, and engineering became obsolete within a decade, and yet, the McKinsey Global Institute concluded in 2018 that “We tallied up all the jobs destroyed in the US since 1980 as a result of the rise of personal computing and the Internet, and it’s about 3.5 million,” but “When we add up all the jobs created, we find that over 19 million jobs have been created as a result of the personal computer and Internet. We see a net gain of 15.8 million jobs in the US over the last few decades. That’s about 10 percent of the civilian labor force today.” If AI is going to have these massive impacts but hasn’t yet, why not? Folks who claim AI will destroy the labor market have claimed this radical change is “only a few years away,” “on the immediate horizon,” or “imminent,” for the last 5 years, yet we’re at historically low unemployment (yes, even accounting for underemployment and the way the BLS counts employment ). The US labor market is within a single percentage point of its post-war unemployment low, measured in 1953 at 3.4% . If AI is killing jobs, it’s doing so at an imperceptibly slow rate; why could that be? Is it still too early? Did other technologies take a long time to show their impacts on labor markets? The broad consensus from rational industry observers, analysts, economists, and even AI-hyped technologists is that the end of cheap money (i.e. higher US interest rates) has driven most of the lower-than-pre-pandemic-demand for entry-level talent (just as it has in times of inflation-fighting interest hikes of the past). Machine-learning, the technology underpinning AI, has been around for decades , with widespread adoption in tech companies between 2006-2013. The current generative-AI era, based on the transformer architecture model, kicked off in 2017, with significant public examples and tech adoption from 2018-2020. Most of the current, press-driven AI hype cycle, however, skyrocketed in late 2021 with OpenAI’s release of GPT-3 (longtime readers here will recall that Britney Muller showed off techniques extremely similar to what’s now associated with modern LLMs back in July 2018 ). We’ve had 15-20 years of robust machine learning development and adoption, and another 5-10 years of broad LLM/generative AI adoption , improvement, and usage, yet labor market fluctuation has been far more dependent on other factors: the Covid pandemic itself, the post-pandemic surge and decline in tech hiring, inflation-fighting tactics by government banks, and (most recently), a renewal of early-20th-century-style tariffs and trade wars. When controlling for these events. The effects of previous technological advancements also took time, but the most salient examples (of farm equipment in the 1910-1920 era and the personal computer in the 1980s) showed millions of displaced workers within 5 years . AI’s slower changes bode poorly for the argument that it will have a larger impact than those events. Even if one assumes that AI was the only contributor to labor market changes between 2021-2025, the change has been incredibly slight, *even* in the software engineering market where it supposedly has the greatest impact. There was a greater loss (nearly 150%) in percentage of software engineering jobs between 2019-2021 than from 2021-2025. I found it particularly revealing that one of the most commonly cited examples of AI killing labor needs in the software field is the death of StackOverflow, and yet, a robust analysis of that site’s usage from 2008-2020 shows that “What really happened is a parable of human community and experiments in self-governance gone bizarrely wrong.” However, it seems likely that the perception of AI and its adoption are slowing hiring in the software engineering market in the post-bubble-popping era (2024-25). This thoughtful analysis by Gergely Orosz concludes a well-visualized, data-driven walkthrough with: “LLMs are a leading cause of the fall in software developer job postings: there’s uncertainty at large companies about whether to hire as fast as previously, given the productivity hype around AI tooling, and businesses are opting to “wait and see” by slowing down recruitment, as a result.” It strains credibility to look at the data, history, and analyses and conclude that AI will eventually kill 20-50% of all jobs, when its largest impact in the prior 5-20 years of adoption (depending on one’s starting point) is ~10% variation in a job sector that employs ~1% of US workers . Assuming AI will have an effect similar to 20th Century farm equipment’s on agriculture, why will that labor force behave differently to their 20th Century counterparts (and either refuse to or be prevented from finding new jobs)? This point is hard to find citations for, given that it’s a future-looking, theoretical assertion. We can, however, compare the impact of the tractor (and farm machinery more broadly) on the economy from 1910-1960. Tractors and farm equipment resulted in the shutdown of a huge number of farms, and a decline in the number of people employed in farming, from ~33% to ~2% of the labor force (notably, even that massive upheaval was less significant than the prognostications by tech company leaders that AI will displace half of all jobs). Nothing like it has happened in the American economy since, and only the industrial revolution of the 18th/19th centuries can compete in scale of transformation. A superb breakdown of farm machinery’s impact on a sector that employed more than a quarter of all Americans comes from Olmstead and Rhode at UC Davis : Is it possible that AI will do to broad sectors of the economy what mechanized farm equipment did to agriculture? Rationally, it’s difficult to fathom generative AI having a greater economic and labor-force impact than the PC revolution of the 1980s. AI makes many tasks more efficient, but evidence that it can wholesale replace entire human functions in a tractor-like way is pure speculation that exists in imagination, not reality. The core assertion by the “AI will replace 20-50% of all jobs” crowd seems to be that the past 20 years of machine learning and generative AI improvements are not indicative of what will happen in the future: a leap in capability that will enable company management to instruct an AI on a job function (“get us press,” or “optimize our marketing campaign,” or “record and audit our financials” ) and rely on machines to correctly determine what needs to be done, how to do it, and then complete all associated tasks with little to no human supervision, intervention, or additional labor. It’s impossible to argue against the assertion that AI will do what’s described above, because it’s based not on objective data, but rather on subjective belief about a possible future. Fighting about what someone believes may come about in the future is generally non-productive, so I’ll avoid that to spare us all a lot of wasted time 😉 I’ll move on from the dry argument analysis and citation process and attempt to summarize (and opine on) what’s really going on here. Leaders of AI companies, and some AI proponents, marketers, journalists, and even critics have found that when they make scary predictions about their field destroying the job market, press and media eat it up. This media coverage, because it’s scary and the AI hype cycle is in full swing, draws clicks. Those clicks lead to employees, managers, and leaders at other businesses being scared into learning and adopting AI in their businesses. Incentive also exists for those who criticize AI, AI companies, or their ethics/models/practices: these folks also benefit directly from the attention they earn when they amplify the message of AI as a job destroying technology. If you’re feeling like the “AI will take all our jobs” discussion is familiar, you’re in good company. Many others have pointed out the similarities to stories like: Source: Jalopnik Source: InsideHook Source: Honest Jobs Mechanization really did take jobs from farm workers. Automation took jobs from manual laborers. The PC took jobs from clerical and communication workers. But, all of these resulted in greater productivity, employment, and more optionality for workers. It’s both anti-historic and anti-evidence that AI will somehow prove to be the exception. Could AI, along with thousands of other impactful technological, political, social, demographic, and black-swan-event changes permanently alter the employment landscape in our lifetimes? Absolutely. In fact, one of my favorite stats from this overly-ambitious weekend of research was MIT’s estimation that 60% of employment in 2018 was in types of jobs that didn’t exist before 1940 . By the time I’m in my 80s, y’all better have destroyed more than half of all the existing jobs, and that’s just to keep up with the 20th Century’s pace of change. But, don’t expect AI to do it for you in the next decade; that’s just marketing. p.s. If you’re looking for the TL;DR, Ed Zitron on Bluesky has got you: p.p.s. I agree there’s evidence that this fear-based marketing campaign has been successful enough to disrupt some hiring, especially for early-stage jobs in a few tech-heavy fields . But squinting at the evidence, it’s <0.1% of jobs (<200,000 total) being affected, and even here, the unbalanced capital vs. labor market is a far more compelling explanation .",
      "summary": "14-minute read",
      "url": "https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 2027,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple Notes Expected to Gain Support for Exporting in Markdown in iOS 26",
      "content": "Apple Notes Expected to Gain Support for Exporting in Markdown in iOS 26 Wednesday June 4, 2025 4:37 am PDT by Tim Hardwick Apple's Notes app is rumored to be getting limited Markdown support in iOS 26 and macOS 26, according to 9to5Mac . The feature would allow users to export text in the markdown format. Markdown is a lightweight markup language that some writers prefer to use over rich text. Rather than using HTML for bold, italics, links, and headers, Markdown uses quick character shortcuts like **bold** or #header. It sounds like the feature will only add support for exporting text with markdown formatting and not writing in markdown directly. If the rumor holds up, it's likely to be unveiled at next week's Worldwide Developers Conference alongside other iOS 26 improvements, including automatic translation and polls in Messages , not to mention a major visual redesign . Related Roundups: iOS 26 , macOS 26 Tahoe Tag: Apple Notes [ 74 comments ] Get weekly top MacRumors stories in your inbox. Popular Stories Friday June 6, 2025 7:27 am PDT by Joe Rossignol Apple is planning to announce several new features for the Messages and Phone apps on iOS 26, according to Bloomberg's Mark Gurman. In a lengthy report outlining his WWDC 2025 expectations today, Gurman said that the two main changes in the Messages app will be the ability to create polls, as well as the option to set a background image within a conversation. 9to5Mac was first to report... • 46 comments Monday June 9, 2025 9:00 am PDT by Jordan Golson Apple's Worldwide Developers Conference (WWDC) starts today with the traditional keynote kicking things off at 10:00 a.m. Pacific Time. MacRumors is on hand for the event and we'll be sharing details and our thoughts throughout the day. We're expecting to see a number of software-related announcements led by a design revamp across Apple's platforms that will also see the numbering of all of... • 1205 comments Friday June 6, 2025 6:17 am PDT by Joe Rossignol While the so-called \"iPhone 17 Air\" is not expected to launch until September, there are already plenty of rumors about the ultra-thin device. Overall, the iPhone 17 Air sounds like a mixed bag. While the device is expected to have an impressively thin and light design, rumors indicate it will have some compromises compared to iPhone 17 Pro models, including worse battery life, only a single ... • 76 comments Friday June 6, 2025 8:32 am PDT by Joe Rossignol In a lengthy report outlining his WWDC 2025 expectations today, Bloomberg's Mark Gurman shared more details about iOS 26's rumored new design. According to Gurman, iOS 26 will feature a \"digital glass\" design inspired by visionOS, the operating system for Apple's Vision Pro headset. That is a well-known rumor by now, but he goes on to provide some more specific details, as listed below:There ... • 64 comments Saturday June 7, 2025 5:27 am PDT by Joe Rossignol macOS 26 will drop support for several older Intel-based Mac models currently compatible with macOS Sequoia, according to a private account on X with a proven track record of leaking information about Apple's software platforms. macOS 26 will be compatible with the following Mac models, the account said:MacBook Air (M1 and later) MacBook Pro (2019 and later) iMac (2020 and later) Mac... • 165 comments Sunday June 8, 2025 11:44 am PDT by Joe Rossignol Apple's long-rumored AirTag 2 might be coming soon. In his Power On",
      "summary": "1-minute read",
      "url": "https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 585,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple names 2025 Design Awards winners",
      "content": "Days before its Worldwide Developers Conference 2025 (WWDC), Apple has named the winners and finalists of its 2025 Design Awards for apps. This year’s winners and finalists mostly include indie apps and startups, and contrary to the trend everywhere else, generative AI apps are noticeably missing from the list. This is the second year in a row Apple has skipped AI-centric apps for its awards . The company did highlight some apps with AI features, though, including Speechify (which won in the Inclusivity category), and music creation and editing app Moises (both are venture-backed startups). Unsurprisingly, Apple focuses more on how developers have used its tools to build better experiences. The company named 12 winners (one app and one game in each) across six categories: Delight and Fun, Innovation, Interaction, Inclusivity, Social Impact, and Visuals and Graphics. In its list of finalists this year, Apple highlighted a large number of apps and games from small developers around the world. From @Apple : \"Speechify is more than a great app; it’s a critical resource that helps people live their lives.\" A big thanks to our friends @Apple & proud the @SpeechifyAI team for winning this year's 2025 Apple Design Award in the lead up to WWDC. https://t.co/t7k8nQK7HD — Rohan Pavuluri (@RPavuluri) June 3, 2025 Watch Duty , which helped share information during California wildfires, won the award in the social impact category. Meanwhile, M13-backed developer prototyping tool Play was awarded the best app in the innovation category. Huge news — Play has won the Apple Design Award for Innovation! To be recognized by @Apple — the company whose platforms have inspired so much of what we’re building — is an incredible honor. Immensely proud of our incredible team and so grateful to all the designers, teams,… pic.",
      "summary": "2-minute read",
      "url": "https://techcrunch.com/2025/06/04/apple-names-2025-design-awards-winners/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 296,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Machine Code Isn't Scary",
      "content": "Machine Code Isn't Scary The first programming language I ever learned was ActionScript. Writing code for Macromedia's Flash might be the furthest away from \"bare metal\" as you can possibly get. As I continued learning new languages, this starting heritage stuck with me. I was mostly interested in high-level, \"web languages\". Low-level languages felt impenetrable. Over time, I learned a bit more about them here and there, but for some reason, this notion stuck with me. Low-level things are scary, and machine code epitomized that most directly. When I Googled things asking about writing in \"straight machine code\", I was met with discouraging messages rather than learning. Eventually, I decided I needed to overcome this barrier if I was going to achieve my goals. In doing so, I learned something I didn't expect. Machine code isn't scary. If you can make sure your JSON conforms to a JSON schema, you can write machine code. Which Machine Code? One problem with machine code is that there isn't simply one standard. There are many different \"instruction sets\" depending on the processor. Most modern PCs use x86-64 machine code, but newer Macs, Raspberry Pis, and most mobile devices use ARM. There are other architectures out there, especially as you go back in time. The goal of this article won't be to give you a deep understanding of any particular instruction set, but instead, to give you enough information about how machine code typically works so you cannot be afraid of machine code. So we will start by having our examples be in ARM 64-bit (also written as aarch64). Once we have a decent understanding of that, we will talk a bit about x86-64. Machine Code Basics To understand the basics of machine code, you need three concepts: Instructions Registers Memory Instructions are exactly what they sound like; they are the code that will run. Machine code instructions are just numbers. In fact, in AArch64, every instruction is a 32-bit number. Instructions encode what operation the machine should run (add, move, subtract, jump, etc.) and accept some arguments for what data to operate on. These arguments might be constants (meaning like the number 2; these constants are often called \"immediates\"), but they can also be registers or a memory address. For now, just think of a register as a variable and memory as a list. Arm Instructions Here is an example of the instruction add immediate . 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 sf 0 0 1 0 0 0 1 0 sh imm12 Rn Rd Now this might look a bit confusing, but once you've seen these tables long enough, they start to be fairly straightforward. Each column in this table represents a single bit in a 32-bit number. If the value is a 0 or 1, that just means it is already filled in. If it has a label, it is a variable that needs to be filled in. sf tells us whether the registers we are going to use are 64-bit or 32-bit registers. sh stands for shift. sh goes in conjunction with imm12, which stands for a 12-bit immediate (constant). So if we want to add 42 to something, we would put 000000101010 in for imm12 and set sh to 0 (meaning we aren't shifting the number). But what if we want to represent a number larger than 12 bits? Well, the add instruction doesn't let us represent all such numbers; but setting sh to 1 lets us shift our number by 12 bits. So for example we can represent 172032172032 by leaving our 42 alone and setting sh to 1. This is a clever technique for encoding larger numbers in a small space. Variables that start with R are registers, in this case, Rn is our argument to add, and Rd is our destination. So the above instruction can be thought of like this: struct Add { is_sixty_four_bit : boolean , shift : boolean , immediate : u12 , n : Register , destination : Register , } Our add instruction is really just a data structure where we put the right parts in the right places. Registers Registers are small places to store values. Every instruction set will have a different number of these registers, different sizes of registers, different kinds of registers, and different naming conventions for registers. For AArch64, there are 31 general-purpose registers numbered X0 through X30 for 64-bit registers. Let's say we want to add 42 to register X0 and store the result in X1; we use this binary number. sf operation sh imm12 Rn Rd 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 To encode our registers into our instruction, we just use their number. So register X0 would be 00000 and register X18 would be 10010 . Registers are simply places where we can store values. But by convention, registers can be used for different things. These are called calling conventions and they are how \"higher\" level languages like C encode function calls. Writing out all these binary numbers all the time (or even converting them to hex) can often be tedious. So instead, we usually talk about instructions in a simple text format called assembly. add x1, x0, #0x2a In order to feel cool, people usually write numbers in assembly as hex values. This is just the number 42. You can see that assembly hides some of the details of the encoding we just made. We don't think about sf, sh, what size our number is, that a register is Rn vs Rd. Instead, the destination comes first and the arguments after. Because of this lack of detail, a single assembly instruction add might actually map to many different machine code instructions depending on its arguments. Memory The last piece we have to understand for machine code is memory. To understand what is going on with memory, we will look at an instruction that lets us store things in memory. This instruction is called STR or not written in shorthand, store. 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 1 x 1 1 1 0 0 1 0 0 imm12 Rn Rt Using this instruction, we are going to store some value (RT) into the address (RN) + some offset (imm12). So if we think about memory as a big array, this instruction is like writing into that array. array[offset] = value . The x here is like our sf before, it controls whether we are using 64-bit values or not. If we want to make this concrete, let's say we have a value in X2, we have an address of memory in X1 and we want to store a value 2 bytes offset from that. We would get this structure: x operation imm12 Rn Rt 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 Since writing that all is tedious, we often just write the assembly notation. We are storing the value in x2 based on the address stored in x1 + 2. str x2, [x1, #0x2] X86-64 X86 encoding is a bit different, but it more or less has the same parts. We are still working with instructions, registers, and memory. Some names are a bit different. Instead of the consistent 0-30 naming, we get the historical baggage of the following 64-bit registers: rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8-r15). However, the biggest difference is that x86 is not a fixed width instruction set. We can't simply give a nice little diagram of every instruction using 32 bits. Instead, instructions are assembled from parts. These parts are given different names; when you see an instruction encoding, it tells you how to put the parts together. REX 7 6 5 4 3 2 1 0 0 1 0 0 W R X B The first part is called the REX. This is a prefix that we can use to help us with 64-bit operations. Not sure if there is an official justification for the name REX, but my understanding is that it is the \"Register Extension Prefix\". Unfortunately, because the REX is a prefix, it will only make sense when we see what comes later. REX is there for backward compatibility. The W in REX lets us signal that we are using 64-bit or not for certain operations. The R and B will \"extend\" our registers in certain operations. In other words, it allows more registers than you used to be able to (These are those r8-r15 registers with a different naming convention than the older registers). We need these because, before 64-bit x86, we had fewer registers and our instructions only had 3 bits per register. With 16 registers, we need an extra bit. (X is for the SIB structure, which we don't cover here). ModR/M 7 6 5 4 3 2 1 0 mod reg rm Our next part is ModR/M. ModR/M keeps up with the tradition of naming things incredibly short and confusing names. mod actually means Mode. mod tells us if rm is acting as a register or if it is a pointer to memory. If mod == 11 then rm is being used as a register, otherwise, it is being used as a pointer. reg just is a register. OpCode OpCode is simple, it is a number. It can be 1-3 bytes long. Putting It Together There are other parts, but we won't cover them here. With just these parts, we can build up an instruction. Let's say we want to move a 32-bit signed immediate to a 64-bit register. We can consult a table of instruction encodings and we will get this: REX.W + C7 /0 id So now we can assemble our parts and make our instruction. Let's start with REX.W. This notation just means REX with W set to 1. Then there’s B8, which is just a number written in hex. /0 is yet more shorthand for using the ModR/M but setting the reg to 0. Finally, id means \"immediate doubleword\", in other words, a constant number that is 32 bits long. So given all that, we can write our instruction. So let's move the number 42 to the rbx register. Byte Index Bits Description Byte 0 55–48 01001000 REX.W = 1 Byte 1 47–40 11000111 Opcode C7 Byte 2 39–32 11000011 ModR/M: reg=000, r/m=011 (RBX) Byte 3 31–24 00101010 42 Byte 4 23–16 00000000 the rest of 42 Byte 5 15–8 00000000 ... Byte 6 7–0 00000000 ... Why is RBX 011? Well, because the table says so. Yeah, I did say that x86 is a bit weird. The Rest of It I won't pretend that this is all you need. But I will say that starting here can get you further than you think. There are some other things to learn, like various flags for things like overflow, there’s also calling conventions, which are about which registers you use when for things like function calls. We haven't really talked about the stack here, but that's memory that you write to to keep track of things. Nor have we talked about jumps, or how to encode larger immediates in ARM, but you’ve gotten the basics. It’s easier than you would think to hop on compiler explorer and learn how things are done. Learning machine code and writing things at this low level has unlocked so many things that were mental blocks for me before. Relying on libraries made by others to do these low-level things always left a gap in my knowledge that made me doubt my understanding. Even if I intellectually could explain things, actually doing them has made a huge difference for me. So if you, like me, find low-level things intimidating, I can't recommend enough starting from scratch, at the lowest possible level for your task. What I've found over and over again with low-level details, their not hard, their just poorly documented and poorly explained.",
      "summary": "7-minute read",
      "url": "https://jimmyhmiller.com/machine-code-isnt-scary?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 2077,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Life as a physical process",
      "content": "It’s always hard to define life. Everyone has their favorite definition – some describe it as a struggle against entropy, while others describe it as an emergent property of chemicals. Countless books have been written on the topic, yet we’re far from a consensus. Against the backdrop of the second law of thermodynamics, life seems like an improbable accident. When everything tends to go towards disorder, how come life is able to create cities, computers and space ships? How do we reconcile all the beautiful complexity we see around us with the stupidly simple laws we observe in physics? Recently, I read this beautiful book, named Life as No One Knows It , by Sara Walker and had a minor revelation that maybe we’ve been looking at life from a completely wrong lens. All this while, we have been fixated on different manifestations of life as we find it on Earth. We see bacteria, zebra fish, or fruit fly and start wondering whether we got lucky here on Earth, or whether our universe contains many more similar instances of life. But, is this even the right question to ask? If we’re identifying life as we know it on Earth, we’re blinding ourselves to the only instance of life as we know it (with self-replication, cells, DNA, etc.). But, why should we expect life to be the same way in the rest of the universe? Are cells fundamental? Does life have to be necessarily carbon-based? Sure, we can define life as anything. It’s an overloaded term. But if we keep defining it with traits we observe on Earth, we will be like that guy who only searches for his keys under the street light because that’s the only thing he can see. Of course, to discover alien life, we need to first know what we’re looking for. But defining all life as being Earth-like is too myopic. What should we do? Well, only when we define life as a precise physical phenomena that doesn’t refer back to life on Earth, can we expect to discover it at unexpected places in the universe. What could such a definition look like? My take-home message from the book was this: Life is a physical process that gives rise to systems that are statistically unlikely to assemble and persist (by themselves) Sara, in her book, champions Assembly Theory which posits that each object in the universe has two numbers associated with it: assembly index and copy number. Assembly index is effectively the minimal number of steps required to (recursively) assemble an object from available building blocks. Think of it as analogous to number of steps required to assemble a lego shape. Copy number is how many copies of the object exist in the universe. Once you accept these basic concepts, you can then define life as a process that generates objects with high assembly index and copy number . High assembly index assures that the process to produce it could not go about randomly. You just don’t smash a bunch of lego pieces and make a Taj Mahal. You need knowledge of it, and that knowledge could only come about via a history of selection. That history is what life is. So, if you see a complicated object – one that could not have assembled spontaneously from basic physics – you should assume a Darwinian-like selection process to be underway. High copy number of that complicated object simply means you get more statistical assurance that some creative process is underway that’s producing extremely complicated objects in multiple numbers. Some concepts in the Assembly theory aren’t clear to me. For example, what is an object and what are building blocks? My current understanding is that Assembly theory works wonderfully at the chemical level where you can define objects as molecules, and building blocks as atoms. Then, assembly index can be precisely calculated via the shortest method of synthesising the molecule from atoms. But, outside that application, I’m not sure how will we calculate assembly index of objects like stars, or really complicated ones like kidneys. Nevertheless, v iewed from the lens of statistical impossibility, life suddenly elevates from the realm of Earth-biology to the study of patterns . We can start looking at life not as wet, mushy thing as we know it, but more like a creative force that builds artifacts that could not have come about randomly (because those complicated objects prevent their creative force from dying out.) In fact, with this lens, life doesn’t even remain a binary thing. Instead of saying something is alive or dead, it’s probably better to talk in terms of degree of aliveness. Aliveness is what an object does to its surroundings. Does it create more complicated outputs v/s the inputs it consumes? Then it’s perpetuating life. The further the distance in complicatedness between an object’s outputs v/s inputs, the more alive something can said to be. (I love that this definition aligns well with how we view agency in people. High agency people shape their environment, while low agency people get shaped! ) Overall, I strongly recommend reading the book. It’s a unique take on why we should go beyond life as we know it on Earth, and start looking at it as a physical phenomena that subsumes what we see here on Earth. To me, the shift is a bit like finally recognising Earth revolves around the Sun (and not the other way around). The beautiful thing about rising one level is that it can help us see life at places we never expected it to be. Life is a creative force that refuses to die out by creating objects that can’t assemble spontaneously. Perhaps AIs are alive in that sense? They’re a complicated object, present in multiple copies and are playing a vital role in creating more statistically unlikely objects in the universe. So, why disqualify them? Join 200k followers Follow @paraschopra Get my new essays in your email",
      "summary": "5-minute read",
      "url": "https://invertedpassion.com/life-as-a-physical-process/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 994,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "23andMe's Former CEO Pushes Purchase Price Nearly $50 Million Higher",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/articles/23andmes-former-ceo-pushes-purchase-price-nearly-50-million-higher-7ec98421?st=HxjmWW&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "2-minute read",
      "url": "https://www.wsj.com/articles/23andmes-former-ceo-pushes-purchase-price-nearly-50-million-higher-7ec98421?st=HxjmWW&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Reddit Sues Anthropic, Alleges Unauthorized Use of Site's Data",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/tech/ai/reddit-lawsuit-anthropic-ai-3b9624dd?st=zWM5Ru&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "2-minute read",
      "url": "https://www.wsj.com/tech/ai/reddit-lawsuit-anthropic-ai-3b9624dd?st=zWM5Ru&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Musk Says SpaceX Revenue Will Near $16 Billion in 2025",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/business/spacex-revenue-to-exceed-nasas-budget-in-2026-elon-musk-says-3321ea31?st=4dsszH&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "2-minute read",
      "url": "https://www.wsj.com/business/spacex-revenue-to-exceed-nasas-budget-in-2026-elon-musk-says-3321ea31?st=4dsszH&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "big_tech",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Beyond 2nm: Apple's A20 chip to introduce new packaging breakthrough",
      "content": "Skip to main content Apple Silicon Beyond 2nm: Apple’s A20 chip to introduce new packaging breakthrough Marcus Mendes | Jun 3 2025 - 4:02 pm PT Apple is planning to overhaul its chip design for the 2026 iPhones, in a move that could mark the first time it uses advanced multi-chip packaging in a mobile device. It sounds complicated, but here’s what this means. According to analyst Jeff Pu in a new report for GF Securities , the iPhone 18 Pro, 18 Pro Max, and the long-rumored iPhone 18 Fold are expected to debut Apple’s A20 chip, built on TSMC’s second-gen 2nm process (N2). But that’s only part of the story. The more interesting bit is how those chips will be assembled. How those chips will be assembled For the first time, Apple is set to adopt Wafer-Level Multi-Chip Module (WMCM) packaging for its iPhone processors. WMCM allows different components, like the SoC and DRAM, to be integrated directly at the wafer level, before being diced into individual chips. It uses a technique that connects the dies without needing an interposer or substrate, which can bring both thermal and signal integrity benefits. If you have no idea what a silicon interposer is, this video does a great job of explaining it. In other words, Apple’s next-gen chip won’t just be smaller and more power-efficient thanks to N2. It’ll also be physically closer to its onboard memory, enabling better performance and potentially lower power consumption for tasks like AI processing and high-end gaming. Behind the scenes, Pu reports TSMC is building a dedicated production line and expects to quickly ramp up by 2027: “TSMC will establish a dedicated WMCM production line at its AP7, leveraging equipment and process similar to CoWoS-L without on-substrate. We see TSMC is preparing capacity of up to 50KPM by end of 2026 and estimate the capacity to reach 110-120KPM by end-2027, due to the increase in adoption.” What this means For Apple, this is a big leap in chip design, similar to when it adopted 3nm ahead of most of the industry. And for the broader mobile market, it suggests that technologies once reserved for data center GPUs and AI accelerators are making their way into smartphones. And if you’re wondering what this means for the iPhone 18 Fold: it seems Apple isn’t just reserving its most novel hardware for its form factor. It may also be the company’s testbed for next-gen silicon packaging. Add 9to5Mac to your Google News feed. You’re reading 9to5Mac — experts who break news about Apple and its surrounding ecosystem, day after day. Be sure to check out our homepage for all the latest news, and follow 9to5Mac on",
      "summary": "2-minute read",
      "url": "https://9to5mac.com/2025/06/03/apples-a20-chip-packaging-breakthrough/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "dev",
      "word_count": 448,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Human Brain Cells on a Chip for Sale",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://spectrum.ieee.org/biological-computer-for-sale?utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://spectrum.ieee.org/biological-computer-for-sale?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "big_tech",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Neuralink competitor Paradromics completes first human implant",
      "content": "Key Points Paradromics has implanted its brain-computer interface in a human for the first time. The procedure took place at the University of Michigan with a patient who was already undergoing neurosurgery to treat epilepsy. The company said the procedure demonstrated that its system can be safely implanted and record neural activity. Dr. Matthew Willsey working in the operating room. Courtesy of the University of Michigan Neurotech startup Paradromics on Monday announced it has implanted its brain-computer interface in a human for the first time. The procedure took place May 14 at the University of Michigan with a patient who was already undergoing neurosurgery to treat epilepsy. The company's technology was implanted and removed from the patient's brain in about 20 minutes during that surgery. Paradromics said the procedure demonstrated that its system can be safely implanted and record neural activity. It's a major milestone for the nearly 10-year-old startup, as it marks the beginning of its next chapter as a clinical-stage company. Once regulators give it the green light, Paradromics plans to kick off a clinical trial later this year that will study the long-term safety and use of its technology in humans. \"We've shown in sheep that our device is best in class from a data and longevity standpoint, and now we've also shown that it's compatible with humans,\" Paradromics founder and CEO Matt Angle told CNBC in an interview. \"That's really exciting and raises a lot of excitement for our upcoming clinical trial.\" A brain-computer interface, or BCI, is a system that deciphers brain signals and translates them into commands for external technologies. Paradromics' system is called the Connexus Brain-Computer Interface, and the company says it will initially help patients with severe motor impairments such as paralysis speak through a computer. More CNBC health coverage From ‘Cockroach Award’ to the Big Board: Hinge Health’s unlikely path to IPO How UnitedHealthcare became the face of America’s health insurance frustrations When 20,000 devices were paralyzed by a bad update, a Georgia health system turned to Apple Paradromics' BCI has not been cleared by the U.S. Food and Drug Administration, and it still has a long road ahead before it reaches commercialization. But for Angle, who founded the company in 2015, the procedure in May was a success, and one that was years in the making. \"You do all of these steps, you validate the hardware, you have this really high degree of rational certainty that things are going to work,\" he said, \"but still emotionally when it works and when it happens the way you expected it to, it's still very, very gratifying.\" Though Paradromics' BCI has not been officially cleared for use by regulators, organizations like the University of Michigan can use new devices for research as long as they can demonstrate that there is not a significant risk to patients. Dr. Oren Sagher, professor of neurosurgery at the University of Michigan, oversaw the traditional clinical component of the procedure in May. Dr. Matthew Willsey, assistant professor of neurosurgery and biomedical engineering at the University of Michigan, led the research component, including the placement of Paradromics' device. BCIs have been studied in academia for decades, and several other startups, including Elon Musk 's Neuralink , are developing their own systems. Paradromics' Connexus Brain-Computer Interface. Courtesy: Paradromics \"It's absolutely thrilling,\" Willsey said in an interview. \"It's motivating, and this is the kind of thing that helps me get up in the morning and go to work.\" Each company's BCI is slightly different, but Paradromics is designing a BCI that can record brain activity at the level of individual neurons. Angle compared this approach to placing microphones inside vs. outside a stadium. Inside a stadium, microphones would capture more detail, such as individual conversations. Outside a stadium, microphones would only capture the roar of the crowd, he said. Other prominent BCI companies include Synchron , which is backed by Jeff Bezos and Bill Gates, and Precision Neuroscience . Both have implanted their systems in humans. Paradromics has raised nearly $100 million as of February, according to PitchBook. The company announced a strategic partnership with Saudi Arabia's Neom in February, but declined to disclose the investment amount. \"The last demonstration stuff has been shown, and we're really excited about the clinical trial that's coming up,\" Angle said. WATCH: Inside Paradromics, the Neuralink competitor hoping to commercialize brain implants before the end of the decade watch now VIDEO 5:03 05:03 Inside Paradromics, the Neuralink competitor hoping to commercialize brain implants before the end of the decade The Edge",
      "summary": "5-minute read",
      "url": "https://www.cnbc.com/2025/06/02/neuralink-paradromics-human-implant.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "big_tech",
      "word_count": 756,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Why GUIs are built at least 2.5 times",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://patricia.no/2025/05/30/why_lean_software_dev_is_wrong.html?utm_source=tldrnewsletter",
      "summary": "25-minute read",
      "url": "https://patricia.no/2025/05/30/why_lean_software_dev_is_wrong.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "science",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Meta and Yandex are de-anonymizing Android users' web browsing identifiers",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~4200 words, got 299. URL: https://arstechnica.com/security/2025/06/meta-and-yandex-are-de-anonymizing-android-users-web-browsing-identifiers/?utm_source=tldrnewsletter",
      "summary": "21-minute read",
      "url": "https://arstechnica.com/security/2025/06/meta-and-yandex-are-de-anonymizing-android-users-web-browsing-identifiers/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 299,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 299/4200 words (7.1%)"
    },
    {
      "title": "Does AI Progress Have a Speed Limit?",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://asteriskmag.com/issues/10/does-ai-progress-have-a-speed-limit?utm_source=tldrnewsletter",
      "summary": "34-minute read",
      "url": "https://asteriskmag.com/issues/10/does-ai-progress-have-a-speed-limit?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Google's NotebookLM now lets you share your notebook — and AI podcasts — publicly",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/678915/google-notebooklm-share-public-link?utm_source=tldrnewsletter",
      "summary": "1-minute read",
      "url": "https://www.theverge.com/news/678915/google-notebooklm-share-public-link?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Meta Prioritizing Ultralight Headset With Puck For 2026 Over Traditional Quest 4",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.uploadvr.com/meta-prioritizing-puffin-for-2026-pushing-out-quest-4-to-2027/?utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://www.uploadvr.com/meta-prioritizing-puffin-for-2026-pushing-out-quest-4-to-2027/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Epic Games' MetaHuman creation tool launches out of early access",
      "content": "MetaHuman , a game dev tool from Epic Games to quickly create realistic digital human characters, is now out of early access and available to all developers with today’s release of Unreal Engine 5.6. With the new update, developers can now use MetaHuman Animation to generate real-time animations and audio from almost any mono camera (such as webcams and most smartphones). Any camera that works with Unreal Engine LiveLink, including some Android phones, will be able to take advantage of these features. MetaHuman Creator, meanwhile, now has an expanded assortment of character mesh shapes and textures, and enhanced fidelity thanks to improvements to the capturing and processing of scanned data. This will help teams create even more lifelike characters for their projects. Epic Games shows off MetaHuman’s real-time animation features at Unreal Fest 2025. During the State of Unreal keynote at Unreal Fest 2025, Epic Games demonstrated a real-time demo of MetaHuman with an alien character named Blue. Using just a minimal camera setup, an actor backstage was able to speak to the crowd in real time, with MetaHuman accurately capturing his facial expressions and movements without any noticeable delays. Epic also revealed that it updated the Unreal Engine EULA license to include MetaHumans, so now developers can use the tool in any game engine or creative software. Disclosure: Epic Games paid my way to Unreal Fest.",
      "summary": "2-minute read",
      "url": "https://venturebeat.com/gaming-business/epic-games-metahuman-creation-tool-launches-out-of-early-access/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 227,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 227/400 words (56.8%)"
    },
    {
      "title": "Stop Over-thinking AI Subscriptions",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://steipete.me/posts/2025/stop-overthinking-ai-subscriptions?utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://steipete.me/posts/2025/stop-overthinking-ai-subscriptions?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Colossal Labs will release the sounds of dire wolves howling ‘later this year,' founder says",
      "content": "Ben Lamm, the billionaire founder of Colossal Labs, and Sophie Turner spoke about his company’s efforts in reviving the long-extinct dire wolf during a panel at SXSW’s first London outpost. The wolf re-entered the public consciousness because of the television show “Game of Thrones,” in which Turner was a leading character. It turns out that even she thought dire wolves were mythical creatures. “Were you a fan of ‘Game of Thrones?’” Turner asked. “I was a fan of ‘Game of Thrones,’” Lamm said. “When ‘Game of Thrones’ came out, which was a while ago, I never thought, ‘Oh, by the way, one day we’ll make dragons or dire wolves.” Lamm retold the story of how he and his team at Colossal “brought back” the dire wolf species, which had been extinct for more than 10,000 years. He also provided some updates as to what the company is working on next, including a bio-acoustic project, announced just a few weeks ago . Lamm said his dire wolves started howling at 3 weeks old, responding to human singing and the sounds of owls. The first two, Romulus and Remus, are now around 6 months old. Lamm and his team are looking at ways to use AI to help monitor the behavior of the wolves, identifying patterns of their emotions and “wolf linguistics,” as he put it. “What’s crazy about it is the dire wolves have their own unique pitch and inflection,” Lamm told the crowd. “We’re going to release that later this year,” he said, referring to a bio-acoustic project, which will map out the different types of dire wolf howls. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The wolves live on a 2,000-acre preservation in a top-secret location. He referred to the park as “very Yellowstone.” It has an animal hospital, as well as full-time caretakers and full-time security. The wolves are learning socialization tactics that would help them in the wild. “They’re starting to do some light predation on the preserve,” he said. “They’re becoming more and more true wolf-like every day.” He also said that they are about to officially introduce the two wolves to a newly created dire wolf, a female named Khaleesi, who is around 3 months old. The public met Colossal Labs’ efforts to “de-extinct” animals with fascination and skepticism, much of it focused on whether the dire wolves are merely genetically edited gray wolves. But investors seem quite interested in where Colossal could be headed with its innovation. TechCrunch reported back in March that the company is looking to “revive” the Tasmanian tiger and the dodo, two other extinct animals. The company announced this year a $200 million Series C at a $10.2 billion valuation and said in March it genetically modified mice to grow mammoth-like fur. (Lamm said it impressively took only a month to do.) Turner did ask Lamm a question that he unsurprisingly gets quite often. “What about dinosaurs?” she asked. “Can you bring them back?” His company isn’t focused on that at the moment, as bringing a dinosaur back is quite complicated, Lamm explained. But later in the conversation, he said there might be a new update coming from Colossal soon. “I think maybe this summer, there could be a dodo update that’s interesting,” he said. Update: This story was updated to provide clarity about the skepticism around Colossal’s dire wolf program. Colossal Labs says a link included in this report is inaccurate. “We are in an issue with them because they misquoted Beth Shapiro,” spokesperson Emily Mailaender wrote in an email. “She never said they weren’t dire wolves. We have been working to get it corrected.” TechCrunch will remove the link if the media outlet retracts its story.",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/03/colossal-labs-will-release-the-sounds-of-dire-wolves-howling-later-this-year-founder-says/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 692,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "How Much Should the World's Richest Man Get Paid?",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/business/elon-musk-tesla-pay-package-fcd86d99?st=2vtDJs&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.wsj.com/business/elon-musk-tesla-pay-package-fcd86d99?st=2vtDJs&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Meta Aims to Fully Automate Ad Creation Using AI",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/tech/ai/meta-aims-to-fully-automate-ad-creation-using-ai-7d82e249?st=jA6Fqa&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://www.wsj.com/tech/ai/meta-aims-to-fully-automate-ad-creation-using-ai-7d82e249?st=jA6Fqa&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Artificial Blood That Could Work for All Blood Types in Trials",
      "content": "CLOSE X By Jasmine Laws is a US News Reporter at Newsweek based in London, U.K. Her focus is reporting on health insurance, including Medicare and Medicaid, but she also writes about U.S. politics, crime and global affairs. Jasmine joined Newsweek in 2024 from News UK and had previously written for the Metro, Byline Times, The Human Perspective magazine and The Express. She has previously extensively covered stories within the U.K. art industry, human rights, health, and human trafficking. She was one of the nominees for Best News Piece at the U.K's Anti-Slavery Day 2023 Awards in Parliament. She is a graduate of Durham University and completed her NCTJ at News Associates. You can get in touch with Jasmine by emailing jasmine.laws@newsweek.com. Languages: English. Writers Page Jasmine Laws US News Reporter Newsweek Is A Trust Project Member FOLLOW Speed: 0.5x Speed: 1x Speed: 1.5x Speed: 2x 🎙️ Voice is AI-generated. Inconsistencies may occur. ✖ Clinical trials to explore the use of universal artificial blood are underway in Japan after initial volunteer doses began in March. The research, led by Professor Hiromi Sakai's laboratory, plans to assess artificial blood, usable for all blood types and storable for up to two years, as a potential solution to critical shortages in blood supplies for emergency and chronic health care worldwide. Newsweek has contacted Sakai for comment via an email to Nara Medical University. Why It Matters There have been other trials to explore the use of artificial blood in recent years, including in the U.S. and the U.K. While Japan's efforts are not the first, it comes amid a new push in medical research to find ways of reducing problems caused by blood shortages . The World Health Organization estimates that more than 118 million blood donations are collected each year—with 40 percent coming from high-income countries, home to 16 percent of the world's population. This means a large portion of the global population has limited access to blood transfusion treatment. Universal artificial blood could reduce preventable deaths in injury, surgery and childbirth—settings where supply mismatches or stockouts cost millions of lives annually in low-income nations alone. A stock image of blood units stored in a cold storage room. A stock image of blood units stored in a cold storage room. Sina Schuldt/dpa via AP What To Know Nara Medical University's trial administered 100 to 400 milliliters of the artificial blood to 16 healthy adult volunteers in March, according to the local news outlet Kyodo News . The next stage would be to examine the treatment's efficacy and safety if no side effects were reported. Newsweek has not been able to determine whether the participants experienced any side effects following the March transfusions. Sakai's lab said on its website that the artificial blood would solve problems present in the traditional blood transfusion system, including \"possibility of infection, blood type mismatching, immunological response, and short shelf life which is insufficient for stockpiling for emergency situations.\" Artificial blood could reduce preventable deaths in settings where limited supply costs millions of lives in low-income countries. Artificial blood could reduce preventable deaths in settings where limited supply costs millions of lives in low-income countries. Getty The blood was created by extracting hemoglobin, the oxygen-carrying molecule, from expired donor blood—older than three weeks—and encapsulating it in a lipid shell. Known as hemoglobin vesicles, these particles mimic natural red blood cells and can carry oxygen efficiently. They are also free of any blood type markers, making them universally compatible and virus-free. The research builds on a 2022 trial in Japan, when artificial hemoglobin vesicles were tested to determine if they carry oxygen in the way red blood cells do. Participants in the study experienced minor side effects from the trial, such as fever or rash, that were resolved quickly. What People Are Saying Ash Toye, a professor of cell biology in the School of Biochemistry at the University of Bristol, England, told Newsweek : \"The launch of a new clinical trial in Japan using artificial blood product derived from human hemoglobin marks a potentially exciting step forward in transfusion medicine. While this area has long held promise, previous efforts have faced significant challenges, particularly around safety, stability, and oxygen delivery efficacy. This trial will need to demonstrate not only that the artificial blood is safe in humans but that it can perform as reliably as donor blood under a range of clinical conditions. There are advantages with an artificial blood product as you will not have to worry about compatibility, and as it's usually smaller, it can penetrate areas which are blocked, e.g., by a stroke or clot.\" Toye added: \"However, as it uses human hemoglobin sourced from blood donors as a starting material, it faces the same challenges in terms of infection risk as human blood . But as with normal donor human blood, they will likely mitigate this by testing. The reliance on human blood also restricts the scale that can be achieved here. But I assume at some point they will move to recombinant human blood sources. I look forward to hearing more about the artificial product and the trial.\" What Happens Next Nara Medical University plans to advance from volunteer safety trials to broader efficacy studies, seeking clinical approval and practical rollout of its universal artificial blood by 2030, according to Kyodo News. The outcome could redefine worldwide transfusion medicine and transform the humanitarian response to trauma, surgery and chronic disease care. Request Reprint & Licensing Submit Correction View Editorial & AI Guidelines Top stories Millions of Americans See Credit Scores Plunge Senator Calls LA Unrest 'Anarchy and Chaos' As Trump Deploys Guard: Live Updates Gavin Newsom Dares Donald Trump to Arrest Him: 'Get It Over With' Richard Blumenthal Reveals Trump 'Martial Law' Fear About the writer Jasmine Laws is a US News Reporter at Newsweek based in London, U.K. Her focus is reporting on health insurance, including Medicare and Medicaid, but she also writes about U.S. politics, crime and global affairs. Jasmine joined Newsweek in 2024 from News UK and had previously written for the Metro, Byline Times, The Human Perspective magazine and The Express. She has previously extensively covered stories within the U.K. art industry, human rights, health, and human trafficking. She was one of the nominees for Best News Piece at the U.K's Anti-Slavery Day 2023 Awards in Parliament. She is a graduate of Durham University and completed her NCTJ at News Associates. You can get in touch with Jasmine by emailing jasmine.laws@newsweek.com. Languages: English. Writers Page Jasmine Laws FOLLOW Jasmine Laws is a US News Reporter at Newsweek based in London, U.K. Her focus is reporting on health insurance, including Medicare and Medicaid, but she also writes about U.S. politics, crime and global affairs. Jasmine joined Newsweek in 2024 from News UK and had previously written for the Metro, Byline Times, The Human Perspective magazine and The Express. She has previously extensively covered stories within the U.K. art industry, human rights, health, and human trafficking. She was one of the nominees for Best News Piece at the U.K's Anti-Slavery Day 2023 Awards in Parliament. She is a graduate of Durham University and completed her NCTJ at News Associates. You can get in touch with Jasmine by emailing jasmine.laws@newsweek.com. Languages: English. Jasmine Laws is a US News Reporter at Newsweek based in London, U.K. Her focus is reporting on health insurance, ...",
      "summary": "4-minute read",
      "url": "https://www.newsweek.com/artificial-blood-japan-all-blood-types-2079654?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "science",
      "word_count": 1230,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Blue Origin launches 6 passengers on sub-orbital trip to the edge of space",
      "content": "Blue Origin launches 6 passengers on sub-orbital trip to the edge of space - CBS News Watch CBS News Blue Origin spacecraft carries 6 to edge of space Blue Origin spacecraft carries 6 to edge of space 00:20 An international crew of four men and two women blasted off and rocketed to the edge of space Saturday, enjoying a few minutes of weightlessness and an out-of-this-world view before plunging back to Earth to wrap up Blue Origin's twelfth New Shepard passenger flight. \"It was such an incredible ride, very moving, very spiritual,\" Panamanian attorney Jaime Alemán, a former ambassador to the United States, said after landing. \"Even better than I ever imagined. I've been traveling, thanks to the gods, all my life, this was like the cherry on top of a cake. I mean, to go up in space and see how huge it is, you can only begin to imagine how much more there is still to discover.\" he New Shepard rocket and crew capsule climb away from Blue Origin's West Texas launch site on the company's 12th crewed sub-orbital space flight. Blue Origin webcast Blue Origin's hydrogen-fueled single-stage booster roared to life at 9:39 a.m. EDT and quickly climbed away from the company's West Texas launch site, accelerating to just over 2,000 mph before releasing the New Shepard spacecraft. The crew capsule, equipped with the largest windows of any operational spaceship, continued coasting up to an altitude of nearly 65 miles, just above the internationally recognized boundary between the discernible atmosphere and space, before beginning the descent to landing. At the moment the capsule was released, its three American passengers — Aymette Medina Jorge, Gretchen Green and Paul Jeris — along with Alemán, Canadian Jesse Williams and New Zealand's Mark Rocket, began enjoying about three minutes of weightlessness as they coasted upwards. The New Shepard single-stage rocket flew itself back to landing near its launch pad for refurbishment and work to ready the vehicle for its next flight. Blue Origin webcast The reusable New Shepard booster, meanwhile, also continued upward before slowing and beginning its on tail-first descent. Seven minutes after liftoff, the rocket's BE-3 engine re-ignited, four landing legs deployed and the booster settled to a picture-perfect touchdown on a landing pad near the launch site. The gumdrop-shaped New Shepard capsule took a more leisurely flight home, descending under three large parachutes to touchdown in the West Texas desert 10 minutes after liftoff. \"You guys, we did it!\" one of the passengers exclaimed. Passenger Gretchen Green celebrates her trip to space after touchdown in the West Texas desert. Blue Origin webcast Blue Origin has now launched 12 passenger flights since the sub-orbital spacecraft carried Amazon- and Blue Origin-founder Jeff Bezos and three others aloft in July 2021. The company has now launched 68 passengers, including four who have flown twice. Blue Origin and its passengers do not reveal how much it costs to fly aboard a New Shepard, but it's estimated to cost upwards of $500,000 per seat. Jorge's seat was sponsored by Farmacias Similares, a Mexican company focused on accessible healthcare across Latin America. Her crewmates presumably paid for their own tickets. Blue Origin has had the sub-orbital passenger market to itself since June 2024 when competitor Virgin Galactic, founded by entrepreneur Richard Branson, retired its original rocketplane to focus on building two upgraded Delta-class spacecraft. Virgin is expected to resume flights next year. Elon Musk Jeff Bezos Virgin Galactic Blue Origin Richard Branson William Harwood Bill Harwood has been covering the U.S. space program full-time since 1984, first as Cape Canaveral bureau chief for United Press International and now as a consultant for CBS News. More from CBS News Trump threatens Musk with \"serious consequences\" if he donates to Democrats Warren asks for contingency plans on national security after Trump, Musk fallout Oklahoma's top prosecutor says state plans to retry Richard Glossip American Airlines flight diverted because it was too big for its destination View CBS News In Open Continue Be the first to know Get browser notifications for breaking news, live events, and exclusive reporting. Not Now Turn On",
      "summary": "3-minute read",
      "url": "https://www.cbsnews.com/news/blue-origin-launches-six-passengers-on-sub-orbital-trip-to-the-edge-of-space/?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "science",
      "word_count": 685,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "My AI Skeptic Friends Are All Nuts",
      "content": "Author Name Thomas Ptacek @tqbf @tqbf Image by Annie Ruygt A heartfelt provocation about AI-assisted programming. Tech execs are mandating LLM adoption. That’s bad strategy. But I get where they’re coming from. Some of the smartest people I know share a bone-deep belief that AI is a fad — the next iteration of NFT mania. I’ve been reluctant to push back on them, because, well, they’re smarter than me. But their arguments are unserious, and worth confronting. Extraordinarily talented people are doing work that LLMs already do better, out of spite. All progress on LLMs could halt today, and LLMs would remain the 2nd most important thing to happen over the course of my career. Important caveat : I’m discussing only the implications of LLMs for software development. For art, music, and writing? I got nothing. I’m inclined to believe the skeptics in those fields. I just don’t believe them about mine. Bona fides: I’ve been shipping software since the mid-1990s. I started out in boxed, shrink-wrap C code. Survived an ill-advised Alexandrescu C++ phase. Lots of Ruby and Python tooling. Some kernel work. A whole lot of server-side C, Go, and Rust. However you define “serious developer”, I qualify. Even if only on one of your lower tiers. level setting † (or, God forbid, 2 years ago with Copilot) First, we need to get on the same page. If you were trying and failing to use an LLM for code 6 months ago †, you’re not doing what most serious LLM-assisted coders are doing. People coding with LLMs today use agents. Agents get to poke around your codebase on their own. They author files directly. They run tools. They compile code, run tests, and iterate on the results. They also: pull in arbitrary code from the tree, or from other trees online, into their context windows, run standard Unix tools to navigate the tree and extract information, interact with Git, run existing tooling, like linters, formatters, and model checkers, and make essentially arbitrary tool calls (that you set up) through MCP. The code in an agent that actually “does stuff” with code is not, itself, AI. This should reassure you. It’s surprisingly simple systems code, wired to ground truth about programming in the same way a Makefile is. You could write an effective coding agent in a weekend. Its strengths would have more to do with how you think about and structure builds and linting and test harnesses than with how advanced o3 or Sonnet have become. If you’re making requests on a ChatGPT page and then pasting the resulting (broken) code into your editor, you’re not doing what the AI boosters are doing. No wonder you’re talking past each other. the positive case LLMs can write a large fraction of all the tedious code you’ll ever need to write. And most code on most projects is tedious. LLMs drastically reduce the number of things you’ll ever need to Google. They look things up themselves. Most importantly, they don’t get tired; they’re immune to inertia. Think of anything you wanted to build but didn’t. You tried to home in on some first steps. If you’d been in the limerent phase of a new programming language, you’d have started writing. But you weren’t, so you put it off, for a day, a year, or your whole career. I can feel my blood pressure rising thinking of all the bookkeeping and Googling and dependency drama of a new project. An LLM can be instructed to just figure all that shit out. Often, it will drop you precisely at that golden moment where shit almost works, and development means tweaking code and immediately seeing things work better. That dopamine hit is why I code. There’s a downside. Sometimes, gnarly stuff needs doing. But you don’t wanna do it. So you refactor unit tests, soothing yourself with the lie that you’re doing real work. But an LLM can be told to go refactor all your unit tests. An agent can occupy itself for hours putzing with your tests in a VM and come back later with a PR. If you listen to me, you’ll know that. You’ll feel worse yak-shaving. You’ll end up doing… real work. but you have no idea what the code is Are you a vibe coding Youtuber? Can you not read code? If so: astute point. Otherwise: what the fuck is wrong with you? You’ve always been responsible for what you merge to main . You were five years go. And you are tomorrow, whether or not you use an LLM. If you build something with an LLM that people will depend on, read the code. In fact, you’ll probably do more than that. You’ll spend 5-10 minutes knocking it back into your own style. LLMs are showing signs of adapting to local idiom, but we’re not there yet. People complain about LLM-generated code being “probabilistic”. No it isn’t. It’s code. It’s not Yacc output. It’s knowable. The LLM might be stochastic. But the LLM doesn’t matter. What matters is whether you can make sense of the result, and whether your guardrails hold. Reading other people’s code is part of the job. If you can’t metabolize the boring, repetitive code an LLM generates: skills issue! How are you handling the chaos human developers turn out on a deadline? † (because it can hold 50-70kloc in its context window) For the last month or so, Gemini 2.5 has been my go-to †. Almost nothing it spits out for me merges without edits. I’m sure there’s a skill to getting a SOTA model to one-shot a feature-plus-merge! But I don’t care. I like moving the code around and chuckling to myself while I delete all the stupid comments. I have to read the code line-by-line anyways. but hallucination If hallucination matters to you, your programming language has let you down. Agents lint. They compile and run tests. If their LLM invents a new function signature, the agent sees the error. They feed it back to the LLM, which says “oh, right, I totally made that up” and then tries again. You’ll only notice this happening if you watch the chain of thought log your agent generates. Don’t. This is why I like Zed’s agent mode : it begs you to tab away and let it work, and pings you with a desktop notification when it’s done. I’m sure there are still environments where hallucination matters. But “hallucination” is the first thing developers bring up when someone suggests using LLMs, despite it being (more or less) a solved problem. but the code is shitty, like that of a junior developer Does an intern cost $20/month? Because that’s what Cursor.ai costs. Part of being a senior developer is making less-able coders productive, be they fleshly or algebraic. Using agents well is both a both a skill and an engineering project all its own, of prompts, indices, and (especially) tooling. LLMs only produce shitty code if you let them. † (Also: 100% of all the Bash code you should author ever again) Maybe the current confusion is about who’s doing what work. Today, LLMs do a lot of typing, Googling, test cases †, and edit-compile-test-debug cycles. But even the most Claude-poisoned serious developers in the world still own curation, judgement, guidance, and direction. Also: let’s stop kidding ourselves about how good our human first cuts really are. It’s hard to get a good toolchain for Brainfuck, too. Life’s tough in the aluminum siding business. † (and they surely will; the Rust community takes tooling seriously) A lot of LLM skepticism probably isn’t really about LLMs. It’s projection. People say “LLMs can’t code” when what they really mean is “LLMs can’t write Rust”. Fair enough! But people select languages in part based on how well LLMs work with them, so Rust people should get on that †. I work mostly in Go. I’m confident the designers of the Go programming language didn’t set out to produce the most LLM-legible language in the industry. They succeeded nonetheless. Go has just enough type safety, an extensive standard library, and a culture that prizes (often repetitive) idiom. LLMs kick ass generating it. All this is to say: I write some Rust. I like it fine. If LLMs and Rust aren’t working for you, I feel you. But if that’s your whole thing, we’re not having the same argument. but the craft Do you like fine Japanese woodworking? All hand tools and sashimono joinery? Me too. Do it on your own time. † (I’m a piker compared to my woodworking friends) I have a basic wood shop in my basement †. I could get a lot of satisfaction from building a table. And, if that table is a workbench or a grill table, sure, I’ll build it. But if I need, like, a table? For people to sit at? In my office? I buy a fucking table. Professional software developers are in the business of solving practical problems for people with code. We are not, in our day jobs, artisans. Steve Jobs was wrong: we do not need to carve the unseen feet in the sculpture. Nobody cares if the logic board traces are pleasingly routed. If anything we build endures, it won’t be because the codebase was beautiful. Besides, that’s not really what happens. If you’re taking time carefully golfing functions down into graceful, fluent, minimal functional expressions, alarm bells should ring. You’re yak-shaving. The real work has depleted your focus. You’re not building: you’re self-soothing. Which, wait for it, is something LLMs are good for. They devour schlep, and clear a path to the important stuff, where your judgement and values really matter. but the mediocrity As a mid-late career coder, I’ve come to appreciate mediocrity. You should be so lucky as to have it flowing almost effortlessly from a tap. We all write mediocre code. Mediocre code: often fine. Not all code is equally important. Some code should be mediocre. Maximum effort on a random unit test? You’re doing something wrong. Your team lead should correct you. Developers all love to preen about code. They worry LLMs lower the “ceiling” for quality. Maybe. But they also raise the “floor”. Gemini’s floor is higher than my own. My code looks nice. But it’s not as thorough. LLM code is repetitive. But mine includes dumb contortions where I got too clever trying to DRY things up. And LLMs aren’t mediocre on every axis. They almost certainly have a bigger bag of algorithmic tricks than you do: radix tries, topological sorts, graph reductions, and LDPC codes. Humans romanticize rsync ( Andrew Tridgell wrote a paper about it!). To an LLM it might not be that much more interesting than a SQL join. But I’m getting ahead of myself. It doesn’t matter. If truly mediocre code is all we ever get from LLMs, that’s still huge. It’s that much less mediocre code humans have to write. but it’ll never be AGI I don’t give a shit. Smart practitioners get wound up by the AI/VC hype cycle. I can’t blame them. But it’s not an argument. Things either work or they don’t, no matter what Jensen Huang has to say about it. but they take-rr jerbs So does open source. We used to pay good money for databases. We’re a field premised on automating other people’s jobs away. “Productivity gains,” say the economists. You get what that means, right? Fewer people doing the same stuff. Talked to a travel agent lately? Or a floor broker? Or a record store clerk? Or a darkroom tech? When this argument comes up, libertarian-leaning VCs start the chant: lamplighters, creative destruction, new kinds of work. Maybe. But I’m not hypnotized. I have no fucking clue whether we’re going to be better off after LLMs. Things could get a lot worse for us. LLMs really might displace many software developers. That’s not a high horse we get to ride. Our jobs are just as much in tech’s line of fire as everybody else’s have been for the last 3 decades. We’re not East Coast dockworkers ; we won’t stop progress on our own. but the plagiarism Artificial intelligence is profoundly — and probably unfairly — threatening to visual artists in ways that might be hard to appreciate if you don’t work in the arts. We imagine artists spending their working hours pushing the limits of expression. But the median artist isn’t producing gallery pieces. They produce on brief: turning out competent illustrations and compositions for magazine covers, museum displays, motion graphics, and game assets. LLMs easily — alarmingly — clear industry quality bars. Gallingly, one of the things they’re best at is churning out just-good-enough facsimiles of human creative work. I have family in visual arts. I can’t talk to them about LLMs. I don’t blame them. They’re probably not wrong. Meanwhile, software developers spot code fragments seemingly lifted from public repositories on Github and lose their shit. What about the licensing? If you’re a lawyer, I defer. But if you’re a software developer playing this card? Cut me a little slack as I ask you to shove this concern up your ass. No profession has demonstrated more contempt for intellectual property. The median dev thinks Star Wars and Daft Punk are a public commons. The great cultural project of developers has been opposing any protection that might inconvenience a monetizable media-sharing site. When they fail at policy, they route around it with coercion. They stand up global-scale piracy networks and sneer at anybody who so much as tries to preserve a new-release window for a TV show. Call any of this out if you want to watch a TED talk about how hard it is to stream The Expanse on LibreWolf. Yeah, we get it. You don’t believe in IPR. Then shut the fuck up about IPR. Reap the whirlwind. It’s all special pleading anyways. LLMs digest code further than you do. If you don’t believe a typeface designer can stake a moral claim on the terminals and counters of a letterform, you sure as hell can’t be possessive about a red-black tree. positive case redux When I started writing a couple days ago, I wrote a section to “level set” to the state of the art of LLM-assisted programming. A bluefish filet has a longer shelf life than an LLM take. In the time it took you to read this, everything changed. Kids today don’t just use agents; they use asynchronous agents. They wake up, free-associate 13 different things for their LLMs to work on, make coffee, fill out a TPS report, drive to the Mars Cheese Castle, and then check their notifications. They’ve got 13 PRs to review. Three get tossed and re-prompted. Five of them get the same feedback a junior dev gets. And five get merged. “I’m sipping rocket fuel right now,” a friend tells me. “The folks on my team who aren’t embracing AI? It’s like they’re standing still.” He’s not bullshitting me. He doesn’t work in SFBA. He’s got no reason to lie. There’s plenty of things I can’t trust an LLM with. No LLM has any of access to prod here. But I’ve been first responder on an incident and fed 4o — not o4-mini, 4o — log transcripts, and watched it in seconds spot LVM metadata corruption issues on a host we’ve been complaining about for months. Am I better than an LLM agent at interrogating OpenSearch logs and Honeycomb traces? No. No, I am not. To the consternation of many of my friends, I’m not a radical or a futurist. I’m a statist. I believe in the haphazard perseverance of complex systems, of institutions, of reversions to the mean. I write Go and Python code. I’m not a Kool-aid drinker. But something real is happening. My smartest friends are blowing it off. Maybe I persuade you. Probably I don’t. But we need to be done making space for bad arguments. but i’m tired of hearing about it And here I rejoin your company. I read Simon Willison , and that’s all I really need. But all day, every day, a sizable chunk of the front page of HN is allocated to LLMs: incremental model updates, startups doing things with LLMs, LLM tutorials, screeds against LLMs. It’s annoying! But AI is also incredibly — a word I use advisedly — important. It’s getting the same kind of attention that smart phones got in 2008, and not as much as the Internet got. That seems about right. I think this is going to get clearer over the next year. The cool kid haughtiness about “stochastic parrots” and “vibe coding” can’t survive much more contact with reality. I’m snarking about these people, but I meant what I said: they’re smarter than me. And when they get over this affectation, they’re going to make coding agents profoundly more effective than they are today. Previous post ↓ Using Kamal 2.0 in Production",
      "summary": "16-minute read",
      "url": "https://fly.io/blog/youre-all-nuts/?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 2831,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "9 Lessons From Cursor's System Prompt",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~10400 words, got 2591. URL: https://byteatatime.dev/posts/cursor-prompt-analysis/?utm_source=tldrnewsletter",
      "summary": "52-minute read",
      "url": "https://byteatatime.dev/posts/cursor-prompt-analysis/?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 2591,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 2591/10400 words (24.9%)"
    },
    {
      "title": "Google's Pixel 10 phones will reportedly launch on August 13th",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/678088/google-pixel-10-release-date-launch-august?utm_source=tldrnewsletter",
      "summary": "2-minute read",
      "url": "https://www.theverge.com/news/678088/google-pixel-10-release-date-launch-august?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Of Course Workers Want a Four-Day Week. Companies Should Too",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/lifestyle/workplace/of-course-workers-want-a-four-day-week-companies-should-too-0837a0a1?st=nmPfYi&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://www.wsj.com/lifestyle/workplace/of-course-workers-want-a-four-day-week-companies-should-too-0837a0a1?st=nmPfYi&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "What happened in the 2010s?",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.strangeloopcanon.com/p/what-happened-in-the-2010s?utm_source=tldrnewsletter",
      "summary": "12-minute read",
      "url": "https://www.strangeloopcanon.com/p/what-happened-in-the-2010s?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Typing 118 WPM Broke My Brain in the Right Ways (A Year-Long Journey)",
      "content": "Typing 118 WPM Broke My Brain in the Right Ways (A Year-Long Journey) Viral on HN • June 2, 2025 • 6 min read Typing became my therapy. Not even kidding. Started at 60 WPM, felt like dragging my feet through mud every time I had to write code comments or documentation. Now? 118 WPM. No home row bullshit. No \"proper finger placement.\" Just pure, chaotic rhythm. Proof here because I know you don't believe me. Update: Posted this on HN and it went absolutely wild - hit for the day with ~150 points and ~200 comments. Guess I'm not the only one obsessed with typing fast. Check out the thread if you want to see the chaos. My Daily Brain Reset Ritual Every morning, before I even think about opening VS Code, before Slack starts destroying my soul, before the daily standup where we pretend we know what we're doing, I just type . Five minutes. That's it. It's like meditation but actually useful. Resets my brain, calms the static from yesterday's debugging session, and gets me in the zone. Way better than staring at my ceiling wondering why I chose this career. Speed is Useless if You're Wrong Here's the thing that took me way too long to figure out: rushing makes everything worse. Every time I tried to go fast, I'd mess up, backspace like crazy, and end up slower than when I started. Classic developer move, honestly. So I slowed down. Focused on hitting every key right. And boom — speed just... happened. 90 WPM, 100 WPM, 118 WPM. All came when I was hitting 100% accuracy consistently. It's like debugging. You can't just randomly change stuff and hope it works. You gotta be methodical, get it right, then optimize. Same energy. Some words still absolutely wreck me though. \"Obviously\" keeps coming out as \"obviousily\" and I want to throw my keyboard every time. So now I just slow down for those demon words and blast through the rest. Strategic speed management or whatever. Screw the Rules, Find Your Flow Plot twist: I don't type \"correctly\" at all. My fingers just go wherever they want. It's like anarchist typing. My left pinky probably hasn't touched the 'A' key in months, but somehow I'm still in the 99.5 percentile. Turns out the \"proper way\" is just a suggestion. Like following PEP 8 or using semicolons in JavaScript. Sure, it's nice, but if your way works better, who cares? When Your Brain is Fried You know those days when you've been staring at the same bug for 3 hours and your brain feels like mush? Or when you've been doomscrolling",
      "summary": "6-minute read",
      "url": "https://balaji-amg.surge.sh/blog/typing-118-wpm-brain-rewiring?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 442,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 442/1200 words (36.8%)"
    },
    {
      "title": "Digg's founders explain how they're building a site for humans in the AI era",
      "content": "The rebooted version of social site Digg aims to bring back the spirit of the old web at a time when AI-generated content is threatening to overwhelm traditional social media platforms, drowning out the voices of real people. This presents an opportunity to build a social site for the AI era, where the people who create content and manage online communities are given a bigger stake in a platform’s success, Digg’s founders think. A Web 2.0-era news aggregation giant, Digg was once valued at $175 million at its height back in 2008 and is now being given new life under the direction of its original founder, Kevin Rose, and Reddit co-founder Alexis Ohanian. The two recently teamed up to announce a new vision for Digg , which will focus on enabling discovery and community, the way that the early internet once allowed for. Speaking at The Wall Street Journal’s Future of Everything conference on Thursday, the founders offered more insight as to how they plan to accomplish that goal with the Digg reboot. Initially, the two touched on problems they encountered in the earlier days of social media, with Ohanian recalling how he chose to resign from Reddit’s board over disagreements about the company’s approach to hate speech that he felt was bad for society and the business. For instance, the company was allowing a forum on Reddit called “r/WatchPeopleDie” to continue operating up until the Christchurch mass shooting, which caught the attention of the media, he said. It was only then that Reddit decided to adjust its policies around violence and gore on the platform. After Reddit, Ohanian went on to found venture capital firm Seven Seven Six , where he says he’s focused on building businesses that are more “values-aligned.” He said he sees Digg as another step in that direction. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW Rose reflected on the early days of machine learning, where the technology was often used to reward posts on which people would rant about the “most obscure, kind of fringe-y weirdness,” he said. “Sometimes that can be good, but oftentimes it’s pushing really weird agendas. And that’s not even getting into the whole bot and AI side of things that are also pushing those agendas,” Rose said. With Digg, the founders want to create a new community focused on serving real people, not AI or bots, they said. Alexis Ohanian. Image Credits: WSJ's Future of Everything conference “I’ve long",
      "summary": "8-minute read",
      "url": "https://techcrunch.com/2025/06/02/diggs-founders-explain-how-theyre-building-a-site-for-humans-in-the-ai-era/?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 483,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 483/1600 words (30.2%)"
    },
    {
      "title": "AI didn't kill Stack Overflow",
      "content": "by Matthew Tyson Contributing Writer AI didn’t kill Stack Overflow feature May 28, 2025 9 mins Developer Generative AI Technology Industry How the groundbreaking developer forum moderated itself into oblivion. Credit: PeopleImages.com - Yuri A / Shutterstock It would be easy to say that artificial intelligence killed off Stack Overflow, but it would be truer to say that AI delivered the final blow. What really happened is a parable of human community and experiments in self-governance gone bizarrely wrong. Stack Overflow once was the internet’s senior engineer, the backstop where developers turned with problems that stymied us. It was a space for technical sharing where the ethos and attitude of open source programming thrived. Stack Overflow was not the first or only site of its kind. It was part of a wave of next-generation programming forums that appeared at the turn of the millennium, some of which still exist . And these themselves were the descendants of user groups and Usenet. As we enter the era of large language models (LLMs), all such forums are facing an existential crisis. Do we even need them anymore? Before we get too far into answering that question, let’s take a closer look at Stack Overflow: What made it great, how it stumbled, and how much of a role AI plays in its current decline. A long pattern of declining usage To start, we can look at this chart of declining monthly questions on Stack Overflow . The chart goes back as far as when the site began in 2008. As it shows, the site once reached a fairly sustained height of 200,000 new questions a month. After that, we see a pattern of gradual decline beginning in 2014. There is a COVID-19 bump in 2020, when everyone was working remotely, followed by a continuing collapse. By the beginning of 2023, about a year after ChatGPT erupted onto the scene, the bottom really drops out. It’s clear that ChatGPT and similar tools were the last straw for Stack Overflow. The site’s diminishing engagement almost exactly mirrors the rise of ChatGPT. But generative AI just put an exclamation mark on something that was already happening. What launched Stack Overflow into the stratosphere was human interaction and the vibrant culture that rose up around it. More than any other site like it, Stack Overflow captured the interactive component of software development. But then the experiment in self-moderation took on an oppressive tone, as its leaders systematically dismantled the very quality that made the platform great. By the time LLMs came along, Stack Overflow was already operating on an arid vision of transactional Q&A. When generative AI came for Stack Overflow, the one thing that could have saved it—the human element—had already been stripped out. The rep game: How Stack Overflow won, and then lost, the ‘net Stack Overflow’s most revolutionary aspect was its reputation system. That is what elevated it above the crowd. The brilliance of the rep game allowed Stack Overflow to absorb all the other user-driven sites for developers and more or less kill them off. On Stack Overflow, users earned reputation points and badges for asking good questions and providing helpful answers. In the beginning, what was considered a good question or answer was not predetermined; it was a natural byproduct of actual programmers upvoting some exchanges and not others. The reputation game was always imperfect: People could and did game the game. But whatever; it was fun, and most users found it helpful. So, what happened? Stack Overflow evolved toward being a so-called self-governing platform, where the power to govern (or moderate) was granted by reputation. Users with enough reputation were empowered to manage various aspects of the platform. Most importantly, they became responsible for moderating questions and answers for “quality.” For Stack Overflow, the new model, along with highly subjective ideas of “quality” opened the gates to a kind of Stanford Prison Experiment . Rather than encouraging a wide range of interactions and behaviors, moderators earned reputation by culling interactions they deemed irrelevant. Suddenly, Stack Overflow wasn’t a place to go and feel like you were part of a long-lived developer culture. Instead, it became an arena where you had to prove yourself over and over again . The downside of gamification Initially, gamification was a booster rocket. It took a beautiful aspect of software development culture—the mysterious joy of giving and receiving help for its own sake—and added a fun new way to score reputation. But what drove that helping culture? I remember a non-programmer looking over my shoulder once when I was on Stack Overflow. “Why do people help? Just for nothing?” The joy of being able to help someone by sharing what you’ve learned is something you must experience yourself to understand. Possibly the best analogy is seeing someone whose car has broken down on the side of the road. You pull over to help because you’ve been there; you know what being broken down on the side of the road feels like. Maybe you can help, and even if you can’t, at least the stranded driver knows someone cares. And then there is the boost of discovering the source of the problem: “Look, here’s a loose coolant clamp.” That shared thrill is what we lost when Stack Overflow let the reputation game win. Software development and the culture of helping Whether the culture of helping each other will survive in this new age of LLMs is a real question. Is human helping still necessary? Or can it all be reduced to inputs and outputs? Maybe there’s a new role for humans in generating accurate data that feeds the LLMs. Maybe we’ll evolve into gardeners of these vast new tracts of synthetic data. But returning to Stack Overflow and the community it once represented: Is there some radical resurrection in its future? Before AI entered the scene, it was clear Stack Overflow needed to back out of a dead-end street of its own creation. It’s possible the site could have returned to greatness by embracing what once made it great: the community and culture of software development. That culture thrives on making people feel welcome. Practically speaking, it means letting people with foolish or off-topic questions interact with those with more experience . Someday, they’ll become the ones with experience. Maybe they’ll come back to return the favor. It’s also clear that developers still want and appreciate community, even in the age of AI. We see that ethos alive and well in spaces like dev.to , and also in the success of GitHub’s open source, social coding model. GitHub is possibly the center of the coding universe, the true heir to the Iron Throne of the old user groups. Of course, it’s also just another useful tool, one that remains essential even in an AI-centric software universe. This perhaps boils down to the kernel at the heart of coding for coding’s sake. By nature, software developers will always create code, just like musicians are always producing music. Even if AI could produce great music, musicians would still do it. After all, we didn’t get to Bach, Beethoven, or the Beatles and just say, “Okay, we’re good, we have music now.” Humans have an inherent need to create, and for software developers, coding is how we do it. There is a way of writing, building, and doing software that is joyful, challenging, and rewarding. AI can be a part of that. But if it is allowed to wholesale replace it, the act of coding for coding’s sake starts to look more like an enthusiast’s hobby. It becomes akin to handcrafting wooden furniture pieces in the age of mass-produced furniture products. Don’t lose the human element Where does a site like Stack Overflow fit into this picture? Matt Asay recently offered some interesting ideas , like tying reputation to AI model contributions. But to truly make a comeback, Stack Overflow would have to believe in the future of human programmers and their culture. It would have to fundamentally say: This is a place where the human side of software development lives, and everything that happens here is in support of that basic mission. The rise and fall of Stack Overflow is a poignant reminder that platforms built for humans thrive on genuine community, not just generating content. Its genius was harnessing the enthusiasm of developers. That energy was gradually diminished by a bizarre twist where a working democracy sprouted an aristocracy, and that aristocracy killed democracy. The arrival of sophisticated AI happened in parallel, but it didn’t cause the collapse; it merely exposed the extent to which the community had already lost its spark. AI will continue to reshape the technological landscape, and the ramifications will continue to unfold before our very eyes. The lesson of Stack Overflow is even more important in the new world that is coming: Humans are the drivers of meaning and purpose. Subtract that human element at your own peril. by Matthew Tyson Contributing Writer Matthew Tyson is a founder of Dark Horse Group, Inc. He believes in people-first technology. When not programming or playing guitar, Matt explores the backcountry and the philosophical hinterlands. He has written for InfoWorld , CSOonline and other Foundry publications since 2007. More from this author analysis JavaScript innovation and the culture of programming Jun 6, 2025 3 mins how-to JavaScript promises: 4 gotchas and how to avoid them Jun 4, 2025 8 mins feature Putting agentic AI to work in Firebase Studio May 21, 2025 14 mins feature 8 ways to do more with modern JavaScript May 7, 2025 14 mins analysis Experiments in JavaScript: What’s new in reactive design May 2, 2025 3 mins feature Catching up with Angular 19 Apr 30, 2025 7 mins feature Comparing Angular, React, Vue, and Svelte: What you need to know Apr 23, 2025 12 mins analysis Blissfully fast and simple JavaScript frameworks Apr 4, 2025 5 mins Show me more Popular Articles Videos feature Mastering AI risk: An end-to-end strategy for the modern enterprise By John Verry Jun 10, 2025 8 mins Data Governance Data and Information Security Generative AI analysis What the AI coding assistants get right, and where they go wrong By Andrew C. Oliver Jun 10, 2025 7 mins Artificial Intelligence Development Tools Generative AI analysis Don’t be a victim of high cloud costs By David Linthicum Jun 10, 2025 5 mins Cloud Computing Cloud Management video Python 3.14's new template string feature Jun 6, 2025 5 mins Python video How to use the new Python Installation Manager tool for Python 3.14 May 27, 2025 4 mins Python video How to use Marimo | A better Jupyter-like notebook system for Python May 13, 2025 4 mins Python",
      "summary": "9-minute read",
      "url": "https://www.infoworld.com/article/3993482/ai-didnt-kill-stack-overflow.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 1789,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Airbnb CEO Brian Chesky wants to build the everything app",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/decoder-podcast-with-nilay-patel/677324/airbnb-ceo-brian-chesky-services-redesign-app-future-travel?utm_source=tldrnewsletter",
      "summary": "78-minute read",
      "url": "https://www.theverge.com/decoder-podcast-with-nilay-patel/677324/airbnb-ceo-brian-chesky-services-redesign-app-future-travel?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "ChatGPT future just revealed — get ready for a ‘super assistant'",
      "content": "(Image credit: Shutterstock) ChatGPT has dominated headlines since it arrived in 2022, quickly growing into one of the most powerful and popular AI tools available today. But OpenAI still has ambitious plans for its generative AI model, as recently revealed in an internal strategy document that outlines its goal to create users' de facto \"interface to the internet.\" The heavily redacted document from late 2024 came to light this week as part of the discovery process in the Justice Department’s antitrust case against Google . In it, OpenAI describes the company's plans to evolve ChatGPT into an “AI super assistant that deeply understands you and is your interface to the internet.” Even with much of the document blacked out, it's clear how much OpenAI expects ChatGPT to revolutionize how we go online. The company sees it less as a tool and more as a companion for surfing the web. “Today, ChatGPT is in our lives through existing form factors — our website, phone, and desktop apps,” the document reads. “But our vision for ChatGPT is to help you with all of your life, no matter where you are.\" That includes everything from taking meeting notes or preparing a presentation to helping you catch up with friends or find the best restaurant. (Image credit: Getty Images) OpenAI goes on to describe ChatGPT as \"T-shaped\" because it combines “broad skills for daily tasks that are tedious, and deep expertise for tasks that most people find impossible,\" like learning to code. While the first half of 2025 focused on building out ChatGPT as a \"super assistant,\" the second half will shift to generating \"enough monetizable demand to pursue these new models.\" “In the first half of next year, we’ll start evolving ChatGPT into a super-assistant: one that knows you, understands what you care about, and helps with any task that a smart, trustworthy, emotionally intelligent person with a computer could do,” the document states. “The timing is right. Models like 02 and 03 are finally smart enough to reliably perform agentic tasks, tools like computer use can boost ChatGPT’s ability to take action, and interaction paradigms like multimodality and generative UI allow both ChatGPT and users to express themselves in the best way for the task.” Sign up to get the BEST of Tom's Guide direct to your inbox. Get instant access to breaking news, the hottest reviews, great deals and helpful tips. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors OpenAI reveals ChatGPT's biggest hurdles (Image credit: Shutterstock) The document also offers a fascinating glimpse into how OpenAI views its competitors like Google Gemini, Microsoft Copilot, and Meta AI. “Looking ahead to 2025, [REDACTED] poses the biggest threat due to their ability to embed equivalent functionality across their products (e.g. without facing the business model cannibalization risks that Google does,\" the document states. The blacked-out portion is fairly short, just a few letters long, which makes Meta the most likely candidate. OpenAI also stated its support for regulations requiring platforms to let users choose ChatGPT as their default assistant. Another hurdle OpenAI references is its growing infrastructure needs to keep up with ChatGPT's ballooning user base, which explains why CEO Sam Altman has made building out data centers one of the company's cornerstone strategies. \"We are leading here, but we can’t rest,” the document reads, warning that “growth and revenue won’t line up forever.” More from Tom's Guide The ‘3-word rule’ that makes ChatGPT give expert-level responses Tom's Guide AI Awards 2025: 17 best AI tools and gadgets right now I just tested the newest versions of Claude, Gemini, DeepSeek and ChatGPT — and the winner completely surprised me Category Back to Laptops Brand Apple Asus Dell Lenovo Storage Size 32GB 256GB Screen Size 13.3-inch 13.4-inch 14-inch Colour Black Blue Gold Grey Silver Price Any Price Showing 10 of 34 deals Filters ☰ SORT BY Price (low to high) Price (high to low) Product Name (A to Z) Product Name (Z to A) Retailer name (A to Z) Retailer name (Z to A) Apple 13\" MacBook Air M4 (2025) 1 AU$2,197 View Apple 15\" MacBook Air M4 (2025) 2 AU$2,134 View Dell XPS 13 (Silver) 3 AU$1,333.17 View Lenovo Yoga Slim 7x (Gen 9) (14-inch Grey) 4 AU$2,349 AU$1,489 View Asus ROG Zephyrus G14 (2024) Our Review ☆ ☆ ☆ ☆ ☆ 5 AU$4,999 View Apple 13\" MacBook Air M4 (2025) 6 AU$1,714 View Apple 15\" MacBook Air M4 (2025) 7 AU$2,134 View Dell XPS 13 9350 (2024) (13.4-inch) 8 AU$2,197.80 View Lenovo Yoga Slim 7x (Gen 9) (14-inch Blue) 9 AU$2,859 AU$1,999 View Asus ROG Zephyrus G14 (2024) Our Review ☆ ☆ ☆ ☆ ☆ 10 AU$4,891.56 View Alyse Stanley News Editor Alyse Stanley is a news editor at Tom’s Guide, overseeing weekend coverage and writing about the latest in tech, gaming, and entertainment. Before Tom’s Guide, Alyse worked as an editor for the Washington Post’s sunsetted video game section, Launcher. She previously led Gizmodo’s weekend news desk and has written game reviews and features for outlets like Polygon, Unwinnable, and Rock, Paper, Shotgun. She’s a big fan of horror movies, cartoons, and roller skating. She's also a puzzle fan and can often be found contributing to the NYT Connections coverage on Tom's Guide You must confirm your public display name before commenting Please logout and then login again, you will then be prompted to enter your display name. Logout LATEST ARTICLES 1 macOS Tahoe 26 is ready to try out — here’s how to download the developer beta 2 7 top new movies to stream this week on Prime Video, Hulu, Disney Plus and more (June 10-16) 3 Macs under threat from new malware campaign impersonating major ISP — how to stay safe 4 ChatGPT Voice just got a huge upgrade — here's everything it can do now 5 I just tested the Hisense U8QG Mini-LED TV and it’s a brightness star",
      "summary": "3-minute read",
      "url": "https://www.tomsguide.com/ai/chatgpt-future-just-revealed-get-ready-for-a-super-assistant?utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 996,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Google Now Lets You Use AI Without Internet on Smartphones",
      "content": "Published on May 28, 2025 In AI News Google Now Lets You Use AI Without Internet on Smartphones The Edge Gallery app is now available for Android, and an iOS variant is coming soon. Nalini Nirad by Supreeth Koundinya Google has unveiled a new application called Edge Gallery, which lets users download and use the company’s AI models on smartphones without the internet. The Android app can be downloaded and installed through the Google AI Edge GitHub repository . The company announced that an iOS app will be available soon. “This app is a resource to understand the practical implementation of the LLM Inference API and the potential of on-device generative AI,” said the company in the blog post . AIM was able to download and install the application on an Android device. The app lets you download its Gemma 3 and 3n models, along with Alibaba’s Qwen 2.5 models. Users can install models across three categories: to have conversations, to upload and understand images, and to use ‘Prompt Lab’, a single-turn LLM in which users can ask one prompt, and the model provides one response. Models of varying sizes are available based on their capabilities; users can download a lightweight model of about 500 MB or a more capable model of approximately 4 GB. To download these models on the Edge Gallery app, users must sign in and agree to usage terms on Hugging Face, a platform where these open-source models are deployed. Once the model is installed, users can access and engage in conversations with the AI model without an internet connection. One of the models that Google provides in the Edge Gallery app is the recently announced Gemma 3n model . The small language model is built to operate locally on smartphones, using minimal memory and resources. Source: Google On the LMArena leaderboard for text-based tasks, the Gemma 3n scored 1293 points. In comparison, OpenAI’s o3-mini scored 1329 points, and the o4-mini scored 1379 points. The leading model is the Gemini 2.5 Pro model with 1446 points. Given that the model will not use the internet to fetch real-time information, it will only be able to provide information based on its knowledge cutoff date. For instance, the Gemma 3n model only contains information in its training data up to June 2024. 📣 Want to advertise in AIM? Book here Supreeth Koundinya Supreeth is an engineering graduate who is curious about the world of artificial intelligence and loves to write stories on how it is solving problems and shaping the future of humanity. Related Posts Did Big Tech Companies in the US Lay Off Thousands to Save Tax? Why Instacart Moved to Postgres & pgvector to Boost Semantic Search iOS 26 Brings Plenty of New Updates to Apple Intelligence So, How Does Hugging Face Make Money? How Stripe Used AI to Boost Fraud Detection from ‘59-97% Overnight’ Google Releases Upgraded Preview of Gemini 2.5 Pro, Better at Coding and Reasoning Association of Data Scientists GenAI Corporate Training Programs Enquire Today >> Our Upcoming Conference MachineCon GCC Summit 2025 Be Part of The Most Powerful GCC Summit 20-22, June, 2025 | 📍 ITC Grand, Goa Download the easiest way to stay informed AWS Summit Mumbai 2025: Unveiling the Blockbuster Lineup of Speakers The keynote sessions featuring these industry and technology leaders promise to offer invaluable perspectives on cloud computing, AI, and digital transformation. EPAM Thinks You Should Rethink Your Data Stack for AI Navigating the Future of Talent Skills in a Transforming Business Landscape Latest AI News OpenAI’s o3 is Genius, Scores 135 in Toughest IQ test Dai-ichi Life Group and Capgemini Sign Deal to Establish GCC in India IBM Plans ‘World’s First’ Fault-Tolerant Quantum Computer by 2029 IBM Introduces MCP Gateway to Simplify GenAI Tool Integration Bhashini and CRIS Sign MoU to Build AI Solutions for Indian Railways OpenAI’s Annual Revenue Touches $10 Billion, Up 81.8% From Last Year Wayve, Uber Begin Level 4 Autonomy Trials in the UK Wipro Boosts Regional Commitment with New HQ in Riyadh",
      "summary": "2-minute read",
      "url": "https://analyticsindiamag.com/ai-news-updates/google-now-lets-you-use-ai-without-internet-on-smartphones/?utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 670,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "It's Waymo's World. We're All Just Riding in It",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/tech/waymo-cars-self-driving-robotaxi-tesla-uber-0777f570?st=p2QtzY&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "11-minute read",
      "url": "https://www.wsj.com/tech/waymo-cars-self-driving-robotaxi-tesla-uber-0777f570?st=p2QtzY&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Elon Musk to build enormous Texas ‘gigabay' to store 1,000 Starships",
      "content": "Return to Homepage Top Stories: Latest on L.A. protests Austria school shooting RFK Jr. fires CDC vaccine panel Diddy trial recap Manhunt: New Travis Decker photos Trump's travel ban takes effect Are your groceries healthy? Storms tear through South Inside the Trump-Musk breakup 'Devil in the Ozarks' manhunt Credit: X/SpaceX SpaceX will build the “biggest structure in the world” to house up to 1,000 of its 400ft-tall Starship rockets per year, Elon Musk has claimed. The Tesla billionaire and world’s richest man laid out his vision for “making life multiplanetary” in an update at SpaceX’s Starbase in Texas. Speaking to staff last night, Mr Musk said the company planned to build a “gigabay”, which he said would be “a truly enormous structure”. Advertisement Advertisement He said the building, designed for 1,000 Starships per year, would be “by some measures the biggest structure in the world”. The first so-called gigabay would be built in Texas , with a further facility in Florida as SpaceX seeks to launch multiple rockets per day in order to reach and colonise Mars. Mr Musk has said humans must colonise the Red Planet to avoid potential extinction risks, such as nuclear war or an asteroid strike, and that the colony would need to become self-sustaining. He said: “Having two strong, self-sustaining planets will be critical for the long-term survival of civilisation.” Advertisement Advertisement The billionaire further claimed the company’s Super Heavy booster rockets would one day be able to fly missions “every hour, maybe every two hours give a bit of extra time”. The Super Heavy boosters are the first stage of the Starship rocket. They are designed to be reusable, falling back to Earth after launching. He added SpaceX would ultimately need to launch 1,000 to 2,000 rockets to Mars in every two-year window to carry the cargo needed to set up a colony. Mr Musk is known for his bold claims and ambitious timelines, including his vision of sending humans to Mars by 2029. He said its first mission could include landing an Optimus robot on Mars. On Tuesday night, Mr Musk’s company undertook a ninth test flight of Starship, the world’s most powerful rocket. The test saw the first re-use of its Super Heavy booster, recycling a rocket body that had previously returned to Earth and been caught by a pair of “chopsticks” on its landing tower. Advertisement Advertisement However, the Starship second stage went into a spin as it returned to Earth, breaking up over the Indian Ocean . Mr Musk has promised to launch another Starship test mission within a few weeks. Nasa is planning to use SpaceX’s megarocket for a manned mission to the Moon as soon as 2027, returning humans to the lunar surface for the first time since 1972. On Thursday, Mr Musk revealed new designs for SpaceX’s Starship, which he said would be “taller” and have a redesigned separation mechanism. He said future generations of its Starship would be 465ft tall and have twice the payload capacity of the Saturn V rocket that conducted the original Moon missions. Advertisement Advertisement The billionaire added that his ambition was that “anyone who wants to move to Mars can do so”, which he said would be the “best adventure that anyone could possibly do”. He added there was a “50/50” chance SpaceX would launch an uncrewed Starship mission at the end of 2026, when Mars is at the closest point in its orbit to Earth. Broaden your horizons with award-winning British journalism. Try The Telegraph free for 1 month with unlimited access to our award-winning website, exclusive app, money-saving offers and more. About Our Ads Solve the daily Crossword 29,385 people played the daily Crossword recently. Can you solve it faster than others? 29,385 people played the daily Crossword recently. Can you solve it faster than others? Crossword Play on Yahoo",
      "summary": "3-minute read",
      "url": "https://www.yahoo.com/news/elon-musk-build-enormous-texas-144308533.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 641,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Systems Correctness Practices at Amazon Web Services",
      "content": "The P Programming Language Lightweight Formal Methods Fault Injection as a Service Metastability and Emergent System Behavior Formal Proof Benefits Beyond Correctness Challenges and Opportunities for the Future Conclusion References Footnotes Amazon Web Services (AWS) strives to deliver reliable services that customers can trust completely. This requires maintaining the highest standards of security, durability, integrity, and availability—with systems correctness serving as the cornerstone for achieving these priorities. An April 2015 article published in Communications of the ACM , titled “How Amazon Web Services Uses Formal Methods,” highlighted the approach for ensuring the correctness of critical services that have since become among the most widely used by AWS customers. 21 Central to this approach was TLA+, 14 a formal specification language developed by Leslie Lamport. Our experience at AWS with TLA+ revealed two significant advantages of applying formal methods in practice. First, we could identify and eliminate subtle bugs early in development—bugs that would have eluded traditional approaches such as testing. Second, we gained the deep understanding and confidence needed to implement aggressive performance optimizations while maintaining systems correctness. Moreover, 15 years ago, AWS’s software testing practice relied primarily on build-time unit testing, often against mocks, and limited deployment-time integration testing. Since then, we have significantly evolved our correctness practices, integrating both formal and semi-formal approaches into the development process. As AWS has grown, formal methods have become increasingly valuable—not only for ensuring correctness but also for performance improvements, particularly in verifying the correctness of both low- and high-level optimizations. This systematic approach toward systems correctness has become a force multiplier at AWS’s scale, enabling faster development cycles through improved developer velocity while delivering more cost-effective services to customers. This article surveys the portfolio of formal methods used across AWS to deliver complex services with high confidence in its correctness. We consider an umbrella definition of formal methods that encompasses these rigorous techniques—from traditional formal approaches (such as theorem proving, 7 , 10 deductive verification, 18 and model checking 8 , 14 ) to more lightweight semi-formal approaches (such as property-based testing, 6 , 19 fuzzing, 9 and runtime monitoring 11 ). The P Programming Language As the use of formal methods was expanded beyond the initial teams at AWS in the early 2010s, we discovered that many engineers struggled to learn and become productive with TLA+. This difficulty seemed to stem from TLA+’s defining feature: It is a high-level, abstract language that more closely resembles mathematics than the imperative programming languages most developers are familiar with. While this mathematical nature is a significant strength of TLA+, and we continue to agree with Lamport’s views on the benefits of mathematical thinking, 15 we also sought a language that would allow us to model check (and later prove) key aspects of systems design while being more approachable to programmers. We found this balance in the P programming language. 8 P is a state-machine-based language for modeling and analysis of distributed systems. Using P, developers model their system designs as communicating state machines, a mental model familiar to Amazon’s developer population—most of whom develop systems based on microservices and service-oriented architectures (SOAs). P has been developed at AWS since 2019 and is maintained as a strategic open source project. 22 Teams across AWS that build some of its flagship products—from storage (for example, Amazon S3, EBS), to databases (for example, Amazon DynamoDB, MemoryDB, Aurora), to compute (for example, EC2, IoT)—have been using P to reason about the correctness of their system designs. For example, P was used in migrating Simple Storage Service (S3) from eventual to strong read-after-write consistency. 1 A key component of S3 is its index subsystem, an object metadata store that enables fast data lookups. To achieve strong consistency, the S3 team had to make several nontrivial changes to the S3 index protocol stack. 25 Because these changes were difficult to get right at S3 scale, and the team wanted to deliver strong consistency with high confidence in correctness, they used P to formally model and validate the protocol design. P helped eliminate several design-level bugs early in the development process and allowed the team to deliver risky optimizations with confidence, as they could be validated using model checking. In 2023, the P team at AWS built PObserve, which provides a new tool for validating the correctness of distributed systems both during testing and in production. With PObserve, we take structured logs from the execution of distributed systems and validate post hoc that they match behaviors allowed by the formal P specification of the system. This allows for bridging the gap between the P specification of the system design and the production implementation (typically in languages like Rust or Java). While there are significant benefits from verifying protocols at design time, runtime monitoring of the same properties for the implementation makes the investment in formal specification much more valuable and addresses classic concerns with the deployment of formal methods in practice (that is, connecting design-time validation with system implementation). Lightweight Formal Methods Another way that AWS has brought formal methods closer to its engineering teams is through the adoption of lightweight formal methods . Property-based testing. The most notable single example of leveraging lightweight formal methods is in Amazon S3’s ShardStore, where the team used property-based testing throughout the development cycle both to test correctness and to speed up development (described in detail by Bornholt, et al. 4 ). The key idea in their approach was combining property-based testing with developer-provided correctness specifications, coverage-guided fuzzing (an approach where the distribution of inputs is guided by code coverage metrics), failure injection (where hardware and other system failures are simulated during testing), and minimization (where counterexamples are automatically reduced to aid human-guided debugging). Deterministic simulation. Another lightweight method widely used at AWS is deterministic simulation testing, in which a distributed system is executed on a single-threaded simulator with control over all sources of randomness, such as thread scheduling, timing, and message delivery order. Tests are then written for particular failure or success scenarios, such as the failure of a participant at a particular stage in a distributed protocol. The nondeterminism in the system is controlled by the test framework, allowing developers to specify orderings they believe are interesting (such as ones that have caused bugs in the past). The scheduler in the testing framework can also be extended for fuzzing of orderings or exploring all possible orderings to be tested. Deterministic simulation testing moves testing of system properties, like behavior under delay and failure, closer to build time instead of integration testing. This accelerates development and provides for more complete behavioral coverage during testing. Some of the work done at AWS on build-time testing of thread ordering and systems failures has been open sourced as part of the shuttle a and turmoil b projects. Continuous fuzzing or random test-input generation. Continuous fuzzing, especially coverage-guided scalable test-input generation, is also effective for testing systems correctness at integration time. During the development of Amazon Aurora’s data-sharding feature (Aurora Limitless Database 3 ), for example, we made extensive use of fuzzing to test two key properties of the system. First, by fuzzing SQL queries (and entire transactions), we validated that the logic partitioning SQL execution over shards is correct. Large volumes of random SQL schemas, datasets, and queries are synthesized and run through the engines under test, and the results compared with an oracle based on the non-sharded version of the engine (as well as other approaches to validation, like those pioneered by SQLancer 23 ). Fuzzing, combined with fault-injection testing, is also useful for testing other aspects of database correctness such as atomicity, consistency, and isolation. In database testing, transactions are automatically generated, their correct behavior is defined using a formally specified correctness oracle, and then all possible interleaving of transactions and statements within the transaction is executed against the system under test. We also use post hoc validation of properties such as isolation (following approaches such as Elle 13 ). Fault Injection as a Service In early 2021, AWS launched Fault Injection Service (FIS) 2 with the goal of making testing based on fault injection accessible to a wide range of AWS customers. FIS allows customers to inject simulated faults, from API errors to I/O pauses and failed instances, into test or production deployments of their infrastructure on AWS. Injecting faults allows customers to validate that the resiliency mechanisms they have built into their architectures (such as failovers and health checks) actually improve availability and do not introduce correctness problems. Fault-injection testing based on FIS is widely used by AWS customers and internally within Amazon. For example, Amazon.com ran 733 FIS-based fault-injection experiments in preparation for Prime Day 2024. In 2014, Yuan et al. found that 92% of catastrophic failures in tested distributed systems were triggered by incorrect handling of nonfatal errors. Many distributed-systems practitioners who were told about this research were surprised the percentage was not higher. Happy-case catastrophic failures are rare simply because the happy case of systems is executed often, tested better (both implicitly and explicitly), and is significantly simpler than the error cases. Fault-injection testing and FIS make it much easier for practitioners to test the behavior of their systems under faults and failures, closing the gap between happy-case and error-case bug density. While fault injection is not considered a formal method, it can be combined with formal specifications. Defining the expected behavior using a formal specification, and then comparing results during and after fault injection to the specified behavior, allows for catching a lot more bugs than simply checking for errors in metrics and logs (or having a human look and say, “Yup, that looks about right”). Metastability and Emergent System Behavior Over the past decade, there has been an emerging interest in a particular class of systems failure: those where some triggering event (like an overload or a cache emptying) causes a distributed system to enter a state where it does not recover without intervention (such as reducing load below normal). This class of failures, dubbed metastable failures , 5 is one of the most important contributors to unavailability in cloud systems. The figure, adapted from Bronson et al., 5 illustrates a common type of metastable behavior: Load increases on the system are initially met with increasing goodput, followed by saturation, followed by congestion and goodput dropping to zero (or near zero). From there, the system cannot return to healthy state by slightly reducing load. Instead, it must follow the dotted line and may not recover until load is significantly reduced. This type of behavior is present even in simple systems. For example, it can be triggered in most systems with timeout-and-retry client logic. Figure. Metastable system behavior under load. Traditional formal approaches to modeling distributed systems typically focus on safety (nothing bad happens) and liveness (something good eventually happens), but metastable failures remind us that systems have a variety of behaviors that cannot be neatly categorized this way. We have increasingly turned to discrete-event simulation to understand the emergent behavior of systems, investing both in custom-built systems simulations and tooling that allow the use of existing system models (built in languages such as TLA+ and P) to simulate system behavior. Extending exhaustive model checkers, like TLA+’s TLC with probabilistic simulations, also allows for the generation of statistical results such as posterior latency distributions, making model checking useful for tasks such as understanding the achievability of latency service-level agreements (SLAs). Formal Proof In some cases, the formal methods enumerated so far in this article are not sufficient. For critical security boundaries such as authorization and virtualization, for example, proofs of correctness can be both desirable and worth the significant investment needed to create them. In 2023, AWS introduced the Cedar authorization policy language for writing policies that specify fine-grained permissions. Cedar was designed for automated reasoning and formal proof. 12 , 24 The language was designed to be well-suited for proof, and the implementation was built in the verification-aware programming language Dafny. Using Dafny, the team was able to prove that the implementation satisfies a variety of security properties. This type of proof goes beyond testing. It is a proof in the mathematical sense. The team also applied a differential testing approach using the Dafny code as a correctness oracle to verify the correctness of the production-ready Rust implementation. Publishing the Dafny code and test procedures as open source, along with the Cedar implementation, allows Cedar users to check the team’s work on correctness. Another example is the Firecracker virtual machine monitor (VMM). Firecracker uses a low-level protocol called virtio to expose emulated hardware devices (such as a network card or solid-state drive) to guest kernels running inside the VM. This emulated device is a critical security boundary because it is the most complex interaction between the untrusted guest and trusted host. The Firecracker team used a tool called Kani 20 that can reason formally about Rust code to prove key properties of this security boundary. Again, proof here goes beyond testing and ensures that the critical properties of this boundary are held no matter what the guest attempts to do. Proofs around the behaviors of programs are an important part of AWS’s software correctness program, so we support development on tools such as Kani, Dafny, 18 and Lean, 16 and the underlying tools—such as satisfiability modulo theory (SMT) solvers—that power them. The ability to use formal models and specifications—for model-checking systems at design time, for validating in-production behavior using runtime monitoring by serving as a correctness oracle, for simulating emergent systems behavior, and for building proofs of critical properties—allows AWS to amortize the engineering effort of developing these specifications over a larger amount of business and customer value. Benefits Beyond Correctness Finally, as discussed in the aforementioned 2015 paper, formal methods are a crucial part of safely improving the performance of cloud systems. Modeling a key commit protocol for the Aurora relational database engine in P and TLA+ allowed us to identify an opportunity to reduce the cost of distributed commits from 2 to 1.5 network roundtrips without sacrificing any safety properties. These kinds of stories are usual for teams that adopt formal methods, driven by at least two different dynamics. First, the act of deeply thinking about and formally writing down distributed protocols forces a structured way of thinking that leads to deeper insights about the structure of protocols and the problem to be solved. Second, having the ability to formally check (and, in some cases, prove) that proposed design optimizations are correct allows naturally conservative distributed-systems engineers to be bolder in their protocol design choices without increasing risk and boosting the developer velocity toward delivering reliable services. These productivity and cost benefits are limited not only to high-level design optimizations but also to low-level code that normally gets ignored. In one example, the AWS team identified optimizations to the implementation of the Rivest-Shamir-Adleman (RSA) public-key encryption scheme on our ARM-based Graviton 2 processor, which could improve throughput by up to 94%. 17 Using the HOL Light interactive theorem prover, the team was able to prove the correctness of these optimizations. Given the high percentage of cloud CPU cycles spent on cryptography, this type of optimization can significantly reduce infrastructure costs and aid sustainability while at the same time improving customer-visible performance. Challenges and Opportunities for the Future Despite significant success in scaling formal and semi-formal testing methods across AWS over the past 15 years, several challenges persist, particularly in industrial adoption of formal methods. The primary barriers for formal methods tools include their steep learning curve and the specialized domain expertise required. Additionally, many of these tools remain academic in nature and lack user-friendly interfaces. Even well-established semi-formal approaches face adoption challenges. For example, deterministic simulation, a key distributed-systems testing technique used successfully at AWS and in projects like FoundationDB, remains unfamiliar to many experienced distributed-systems developers joining AWS. Similar gaps exist in the adoption of other proven methodologies, such as fault-injection testing, property-based testing, and fuzzing. The challenge is educating distributed-systems developers about these testing methods and tools, teaching the art of rigorous thinking. The education gap begins at the academic level, where even basic formal reasoning approaches are rarely taught, making it difficult for graduates from top institutions to adopt these tools. Although formal methods and automated reasoning are crucial for industry applications, they continue to be viewed as niche fields. We anticipate that increased industry adoption of formal methods and automated reasoning will attract more talent to this domain. Metastability and other emergent properties of large-scale systems represent another critical research area facing similar awareness challenges. Common practices that lead to metastable system behavior, such as “retry N times on timeout,” continue to be widely recommended despite their known issues. Current tools and techniques for understanding emergent system behavior are still in their early stages, making system stability modeling expensive and complex. Ongoing research in this area holds promising potential for advancement. Looking ahead, we believe large language models and AI assistants will significantly help address the adoption challenges of formal methods in practice. Just as AI-assisted unit testing has gained popularity, these tools are expected soon to help developers create formal models and specifications, making these advanced techniques more accessible to the broader developer community. Conclusion Building reliable and secure software requires a range of approaches to reason about systems correctness. Alongside industry-standard testing methods (such as unit and integration testing), AWS has adopted model checking, fuzzing, property-based testing, fault-injection testing, deterministic simulation, event-based simulation, and runtime validation of execution traces. Formal methods have been an important part of the development process—perhaps most importantly, formal specifications as test oracles that provide the correct answers for many of AWS’s testing practices. Correctness testing and formal methods remain key areas of investment at AWS, accelerated by the excellent returns seen on investments in these areas already. Submit an Article to CACM CACM welcomes unsolicited submissions on topics of relevance and value to the computing community. You Just Read Systems Correctness Practices at Amazon Web Services View in the ACM Digital Library This work is licensed under Creative Commons Attribution International 4.0 License . © 2025",
      "summary": "22-minute read",
      "url": "https://cacm.acm.org/practice/systems-correctness-practices-at-amazon-web-services/?utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 3023,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 3023/4400 words (68.7%)"
    },
    {
      "title": "For Some Recent Graduates, the A.I. Job Apocalypse May Already Be Here",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~1800 words, got 461. URL: https://www.nytimes.com/2025/05/30/technology/ai-jobs-college-graduates.html?unlocked_article_code=1.Lk8.juhQ.YSBBq29gTIhd&smid=url-share&utm_source=tldrnewsletter",
      "summary": "9-minute read",
      "url": "https://www.nytimes.com/2025/05/30/technology/ai-jobs-college-graduates.html?unlocked_article_code=1.Lk8.juhQ.YSBBq29gTIhd&smid=url-share&utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 461,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 461/1800 words (25.6%)"
    },
    {
      "title": "The U.S. Plan to Hobble China Tech Isn't Working",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/tech/the-u-s-plan-to-hobble-china-tech-isnt-working-56d1a512?st=EKbL7D&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "9-minute read",
      "url": "https://www.wsj.com/tech/the-u-s-plan-to-hobble-china-tech-isnt-working-56d1a512?st=EKbL7D&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "On Stepping Back",
      "content": "On Stepping Back The other day I was playing around with Claude Code, experimenting with porting some C code to Rust - not for any particular reason, just because I was curious how well it could do. As these things happen, I got more and more invested in the process, instead of just letting Claude do something and noting what happened, I kept poking in and adjusting the behavior, \"no you can't pretend the result is 0, try again\" - the usual stuff when you're trying to get an LLM to do something large or novel. But I got so invested in seeing the port of the module work that I forgot that my original goal was to see how well the LLM could do on its own. Instead of testing a potentially interesting hypothesis, I was poorly porting a module of a project that I had no interest in continuing. I was only rescued in this case from my pains by the LLM itself: Claude hit some kind of rate-limiting error, forcing me to take a break from the project. Once I was forced away, I found myself possessed with a sudden clarity: why on earth had I 5 minutes ago become so invested in this particular task? Given the forced break, my brain \"reset\" and I could step away from it, and go back to thinking about the original problem and what my goals and conclusions were. But there was no way in the moment I could do so. I just couldn't let it go. This isn't of course the first time that this has happened. Sometimes the breaks are imposed: my wife and I are off to get lunch, and while walking to the door I realize the last 2 hours spent feverishly trying to get some RPC from A to B to work are actually pointless and there's some much simpler solution. Other times I find myself sitting back and catching myself getting too deep into a problem. Of course, in any big enough problem, you're never sure what to do initially. You have to just... explore, and correct yourself if you're off. There's an unresolvable tension between trying to get through the problem at hand and contemplating \"maybe this isn't the right direction after all\". At least I can't seem to hold both of them in my head at the same time: if you're trying to debug some subtle problem involving 10 layers of software and hardware, you can't afford the brain space for questioning whether you'd be better off trying a different approach. But even more, the tenacity that makes you stick on problems, it's almost the thing that makes you a good engineer. You need to be stubborn and willing to push through stupid problems in order to get things done. I don't think there's an easy solution: how do you know when you're in too deep? In RL we'd probably characterize this as a variation on the Exploration/Exploitation -- how much energy should we invest into getting \"just this next bug fixed and working\" versus taking a sip of tea and then you should change your approach or change your job? Spending too much time fixating on one thing leads you towards a sort of monomaniacal obsession with it: \"functional programming is the only way\" sort of thing. On the other hand, you can fixate the other way and spend all your time contemplating what the best thing is to do, leading to a weird sort of nihilism as you question everything while you watch life go by. And yet stepping away from our problems can be such a powerful tool for us, giving us the space to understand if we're doing the right thing: you can't see the forest for the trees and all that. Recently I've found myself adopting a sort of ritual for stepping back and reflecting. I find if I align it to natural time boundaries: hours, days, weeks, months, years, I can sort of set a mental schedule for myself and it's not too much of a distraction. When I take a break, I can ask myself the usual questions: \"what am I doing?\", \"why am I doing it?\", \"what could I be doing instead?\". I find this can be cathartic as well, letting me note progress as well as struggles. (It should be taken for given that of course it's really hard to switch your mindset from \"debugging this thing is really important\" to \"should I debug this thing at all\", while you're in the middle of debugging said thing . But you know, you just try your best, sometimes your brain refuses to let go, and other times, maybe you get enough space to decide going off for a beer is the better bet.) Of course, the amount of time deliberating and the scope of the questions changes with the time duration: I don't want to question why I'm working at my job 8 times at day, but maybe once a week is okay? Conversely, I can afford 1 minute an hour to jot down what I'm trying to do: I'm not contemplating throwing away a project at that point, but maybe the scope of a feature or how I'm debugging an issue. But once a year, well then maybe it's worth taking a day off from everything and contemplating where you're going in life: are you happy with your decisions? In a way you're paying 1% of your time and energy as a sort of insurance against wandering too far in a bad direction. Not a bad trade.",
      "summary": "4-minute read",
      "url": "https://rjp.io/blog/2025-05-31-stepping-back?utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 934,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Why agents are bad pair programmers",
      "content": "Friday, May 30, 2025 Why agents are bad pair programmers LLM agents make bad pairs because they code faster than humans think . I'll admit, I've had a lot of fun using GitHub Copilot's agent mode in VS Code this month. It's invigorating to watch it effortlessly write a working method on the first try. It's a relief when the agent unblocks me by reaching for a framework API I didn't even know existed. It's motivating to pair with someone even more tirelessly committed to my goal than I am. In fact, pairing with top LLMs evokes many memories of pairing with top human programmers. The worst memories. Memories of my pair grabbing the keyboard and—in total and unhelpful silence—hammering out code faster than I could ever hope to read it. Memories of slowly, inevitably becoming disengaged after expending all my mental energy in a futile attempt to keep up. Memories of my pair hitting a roadblock and finally looking to me for help, only to catch me off guard and without a clue as to what had been going on in the preceding minutes, hours, or days. Memories of gradually realizing my pair had been building the wrong thing all along and then suddenly realizing the task now fell to me to remediate a boatload of incidental complexity in order to hit a deadline. So yes, pairing with an AI agent can be uncannily similar to pairing with an expert programmer. The path forward What should we do instead? Two things: The same thing I did with human pair programmers who wanted to take the ball and run with it: I let them have it. In a perfect world, pairing might lead to a better solution, but there's no point in forcing it when both parties aren't bought in. Instead, I'd break the work down into discrete sub-components for my colleague to build independently. I would then review those pieces as pull requests. Translating that advice to LLM-based tools: give up on editor-based agentic pairing in favor of asynchronous workflows like GitHub's new Coding Agent , whose work you can also review via pull request Continue to practice pair-programming with your editor, but throttle down from the semi-autonomous \"Agent\" mode to the turn-based \"Edit\" or \"Ask\" modes . You'll go slower, and that's the point . Also, just like pairing with humans, try to establish a rigorously consistent workflow as opposed to only reaching for AI to troubleshoot. I've found that ping-pong pairing with an AI in Edit mode (where the LLM can propose individual edits but you must manually accept them) strikes the best balance between accelerated productivity and continuous quality control Give people a few more months with agents and I think (hope) others will arrive at similar conclusions about their suitability as pair programmers. My advice to the AI tool-makers would be to introduce features to make pairing with an AI agent more qualitatively similar to pairing with a human. Agentic pair programmers are not inherently bad, but their lightning-fast speed has the unintended consequence of undercutting any opportunity for collaborating with us mere mortals. If an agent were designed to type at a slower pace, pause and discuss periodically, and frankly expect more of us as equal partners, that could make for a hell of a product offering. Just imagining it now, any of these features would make agent-based pairing much more effective: Let users set how many lines-per-minute of code—or words-per-minute of prose—the agent outputs Allow users to pause the agent to ask a clarifying question or push back on its direction without derailing the entire activity or train of thought Expand beyond the chat metaphor by adding UI primitives that mirror the work to be done. Enable users to pin the current working session to a particular GitHub issue. Integrate a built-in to-do list to tick off before the feature is complete. That sort of thing Design agents to act with less self-confidence and more self-doubt. They should frequently stop to converse: validate why we're building this, solicit advice on the best approach, and express concern when we're going in the wrong direction Introduce advanced voice chat to better emulate human-to-human pairing, which would allow the user both to keep their eyes on the code (instead of darting back and forth between an editor and a chat sidebar) and to light up the parts of the brain that find mouth-words more engaging than text Anyway, that's how I see it from where I'm sitting the morning of Friday, May 30th, 2025. Who knows where these tools will be in a week or month or year, but I'm fairly confident you could find worse advice on meeting this moment. As always, if you have thoughts, e-mail 'em .",
      "summary": "4-minute read",
      "url": "https://justin.searls.co/posts/why-agents-are-bad-pair-programmers/?utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 794,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Judge Examines Steps to Limit Google's Reach in AI Arms Race",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/tech/google-search-ai-antitrust-trial-57ec6fdb?st=HmfzFC&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://www.wsj.com/tech/google-search-ai-antitrust-trial-57ec6fdb?st=HmfzFC&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "The Illusion of Causality in Charts",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://filwd.substack.com/p/the-illusion-of-causality-in-charts?utm_source=tldrnewsletter",
      "summary": "8-minute read",
      "url": "https://filwd.substack.com/p/the-illusion-of-causality-in-charts?utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Claude 4 - From Hallucination to Creation?",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://omarabid.com/claude-4?utm_source=tldrnewsletter",
      "summary": "2-minute read",
      "url": "https://omarabid.com/claude-4?utm_source=tldrnewsletter",
      "published_date": "2025-06-02T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    }
  ]
}