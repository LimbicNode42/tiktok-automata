{
  "metadata": {
    "extraction_timestamp": "2025-06-12T17:42:33.538513",
    "scraper_version": "1.0",
    "extraction_statistics": {
      "total_articles": 90,
      "success_count": 43,
      "partial_count": 10,
      "failed_count": 37,
      "success_rate": 47.8,
      "partial_rate": 11.1,
      "failure_rate": 41.1,
      "failed_articles": [
        {
          "title": "IBM now describing its first error-resistant quantum compute system",
          "url": "https://arstechnica.com/science/2025/06/ibm-is-now-detailing-what-its-first-quantum-compute-system-will-look-like/?utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 308/1600 words (19.2%)"
        },
        {
          "title": "Android 16 is here",
          "url": "https://android-developers.googleblog.com/2025/06/android-16-is-here.html?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "God is hungry for Context: First thoughts on o3 pro",
          "url": "https://www.latent.space/p/o3-pro?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "News Sites Are Getting Crushed by Google's New AI Tools",
          "url": "https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141?st=NSkAAm&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Apple WWDC 2025: the 13 biggest announcements",
          "url": "https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Doctors Could Hack the Nervous System With Ultrasound",
          "url": "https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "AI's metrics question",
          "url": "https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
          "reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
        },
        {
          "title": "Silicon Valley's quest to remove friction from our lives",
          "url": "https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "A bit more on Twitter/X's new encrypted messaging",
          "url": "https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Everything Apple Plans to Show at Its iOS 26-Focused WWDC 2025 Event",
          "url": "https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Microsoft and Asus announce two Xbox Ally handhelds with new Xbox full-screen experience",
          "url": "https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "BYD's Five-Minute Charging Puts China in the Lead for EVs",
          "url": "https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Welcome to Campus. Here's Your ChatGPT",
          "url": "https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 435/1600 words (27.2%)"
        },
        {
          "title": "The hidden time bomb in the tax code that's fueling mass tech layoffs",
          "url": "https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Meta reportedly in talks to invest billions of dollars in Scale AI",
          "url": "https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Longevity Is Now a Factor When Picking an Embryo for IVF",
          "url": "https://www.wsj.com/health/embryo-ivf-screening-longevity-2b1f096a?st=WfueaT&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "What a developer needs to know about SCIM",
          "url": "https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim?utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 256/3800 words (6.7%)"
        },
        {
          "title": "X Is Testing a Way To Take Community Notes to the Next Level",
          "url": "https://www.adweek.com/media/x-is-testing-a-way-to-take-community-notes-to-the-next-level/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "One thing Tesla and Comma.ai overlooked in self-driving",
          "url": "https://deepdream.substack.com/p/one-thing-tesla-and-commaai-overlooked?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "LLMs are mirrors of operator skill",
          "url": "https://ghuntley.com/mirrors/?utm_source=tldrnewsletter",
          "reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
        },
        {
          "title": "23andMe's Former CEO Pushes Purchase Price Nearly $50 Million Higher",
          "url": "https://www.wsj.com/articles/23andmes-former-ceo-pushes-purchase-price-nearly-50-million-higher-7ec98421?st=HxjmWW&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Reddit Sues Anthropic, Alleges Unauthorized Use of Site's Data",
          "url": "https://www.wsj.com/tech/ai/reddit-lawsuit-anthropic-ai-3b9624dd?st=zWM5Ru&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Musk Says SpaceX Revenue Will Near $16 Billion in 2025",
          "url": "https://www.wsj.com/business/spacex-revenue-to-exceed-nasas-budget-in-2026-elon-musk-says-3321ea31?st=4dsszH&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Human Brain Cells on a Chip for Sale",
          "url": "https://spectrum.ieee.org/biological-computer-for-sale?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Why GUIs are built at least 2.5 times",
          "url": "https://patricia.no/2025/05/30/why_lean_software_dev_is_wrong.html?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Meta and Yandex are de-anonymizing Android users' web browsing identifiers",
          "url": "https://arstechnica.com/security/2025/06/meta-and-yandex-are-de-anonymizing-android-users-web-browsing-identifiers/?utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 299/4200 words (7.1%)"
        },
        {
          "title": "Does AI Progress Have a Speed Limit?",
          "url": "https://asteriskmag.com/issues/10/does-ai-progress-have-a-speed-limit?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Google's NotebookLM now lets you share your notebook — and AI podcasts — publicly",
          "url": "https://www.theverge.com/news/678915/google-notebooklm-share-public-link?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Meta Prioritizing Ultralight Headset With Puck For 2026 Over Traditional Quest 4",
          "url": "https://www.uploadvr.com/meta-prioritizing-puffin-for-2026-pushing-out-quest-4-to-2027/?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Stop Over-thinking AI Subscriptions",
          "url": "https://steipete.me/posts/2025/stop-overthinking-ai-subscriptions?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "How Much Should the World's Richest Man Get Paid?",
          "url": "https://www.wsj.com/business/elon-musk-tesla-pay-package-fcd86d99?st=2vtDJs&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "Meta Aims to Fully Automate Ad Creation Using AI",
          "url": "https://www.wsj.com/tech/ai/meta-aims-to-fully-automate-ad-creation-using-ai-7d82e249?st=jA6Fqa&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "9 Lessons From Cursor's System Prompt",
          "url": "https://byteatatime.dev/posts/cursor-prompt-analysis/?utm_source=tldrnewsletter",
          "reason": "Insufficient content: got 2591/10400 words (24.9%)"
        },
        {
          "title": "Google's Pixel 10 phones will reportedly launch on August 13th",
          "url": "https://www.theverge.com/news/678088/google-pixel-10-release-date-launch-august?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Of Course Workers Want a Four-Day Week. Companies Should Too",
          "url": "https://www.wsj.com/lifestyle/workplace/of-course-workers-want-a-four-day-week-companies-should-too-0837a0a1?st=nmPfYi&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
          "reason": "HTTP 401 - Request failed"
        },
        {
          "title": "What happened in the 2010s?",
          "url": "https://www.strangeloopcanon.com/p/what-happened-in-the-2010s?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        },
        {
          "title": "Airbnb CEO Brian Chesky wants to build the everything app",
          "url": "https://www.theverge.com/decoder-podcast-with-nilay-patel/677324/airbnb-ceo-brian-chesky-services-redesign-app-future-travel?utm_source=tldrnewsletter",
          "reason": "Content validation failed - content too short, paywall detected, or formatting issues"
        }
      ],
      "partial_articles": [
        {
          "title": "How AI Saved My Company From a 2-Year Litigation Nightmare",
          "url": "https://tylertringas.com/ai-legal/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 2720/4600 words (59.1%)",
          "word_count": 2720
        },
        {
          "title": "Apple Retreats",
          "url": "https://stratechery.com/2025/apple-retreats/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 3752/5800 words (64.7%)",
          "word_count": 3752
        },
        {
          "title": "The SignalFire State of Talent Report - 2025",
          "url": "https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 2308/3400 words (67.9%)",
          "word_count": 2308
        },
        {
          "title": "Field Notes From Shipping Real Code With Claude",
          "url": "https://diwank.space/field-notes-from-shipping-real-code-with-claude?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 4071/7400 words (55.0%)",
          "word_count": 4071
        },
        {
          "title": "Reverse Engineering Cursor's LLM Client",
          "url": "https://www.tensorzero.com/blog/reverse-engineering-cursors-llm-client/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 2185/3400 words (64.3%)",
          "word_count": 2185
        },
        {
          "title": "Amazon ‘testing humanoid robots to deliver packages'",
          "url": "https://www.theguardian.com/technology/2025/jun/05/amazon-testing-humanoid-robots-to-deliver-packages?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 390/600 words (65.0%)",
          "word_count": 390
        },
        {
          "title": "AMC Networks Inks Deal With AI Company Runway",
          "url": "https://www.hollywoodreporter.com/business/business-news/amc-networks-runway-ai-deal-1236255108/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 324/600 words (54.0%)",
          "word_count": 324
        },
        {
          "title": "Epic Games' MetaHuman creation tool launches out of early access",
          "url": "https://venturebeat.com/gaming-business/epic-games-metahuman-creation-tool-launches-out-of-early-access/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 227/400 words (56.8%)",
          "word_count": 227
        },
        {
          "title": "Typing 118 WPM Broke My Brain in the Right Ways (A Year-Long Journey)",
          "url": "https://balaji-amg.surge.sh/blog/typing-118-wpm-brain-rewiring?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 442/1200 words (36.8%)",
          "word_count": 442
        },
        {
          "title": "Digg's founders explain how they're building a site for humans in the AI era",
          "url": "https://techcrunch.com/2025/06/02/diggs-founders-explain-how-theyre-building-a-site-for-humans-in-the-ai-era/?utm_source=tldrnewsletter",
          "reason": "Partial extraction: got 483/1600 words (30.2%)",
          "word_count": 483
        }
      ]
    }
  },
  "articles": [
    {
      "title": "A frustrated Zuckerberg makes his biggest AI bet as Meta nears $14 billion stake in Scale AI, hires founder Wang",
      "content": "Key Points In finalizing a deal to invest $14 billion in Scale AI, Meta's Mark Zuckerberg is hiring its co-founder Alexandr Wang to help the social media company better execute on its AI ambitions. Zuckerberg has grown frustrated that rivals like OpenAI appear to be further ahead than Meta in underlying AI models and consumer-facing apps, current and former Meta employees said. Wang has built a reputation as an ambitious leader who understands AI's technical complexities and how to build a business, according to two former Meta AI employees. In this article META Follow your favorite stocks CREATE FREE ACCOUNT Mark Zuckerberg arrives before the inauguration of Donald Trump as the 47th president of the United States takes place inside the Capitol Rotunda of the U.S. Capitol building in Washington, D.C., Monday, Jan. 20, 2025. Kenny Holston | Via Reuters Mark Zuckerberg is so frustrated with Meta's standing in artificial intelligence that he's willing to spend billions of dollars to convince Scale AI CEO Alexandr Wang to join his company, people familiar with the matter told CNBC. Meta is finalizing a deal to invest $14 billion into Scale AI, according to a person familiar with the matter who asked not to be named because the terms are confidential. Bloomberg reported earlier this week that an investment could top $10 billion, and a story from The Information on Tuesday said Meta would pay close to $15 billion. As a founder of one of the most prominent AI startups, Wang has built a reputation as an ambitious leader who both understands AI's technical complexities and how to build a business that's not merely focused on research, according to two former Meta AI employees who agreed to speak on the condition of anonymity. Zuckerberg will be counting on Wang to better execute Meta's AI ambitions following the lukewarm launch of the company's latest Llama AI models. By not directly acquiring Scale AI, Meta appears to be taking a similar strategy as companies like Google and Microsoft , which have brought in prominent leaders in AI from the startups Character.AI and Inflection AI by taking large stakes in those companies rather than buying them outright. Meta is currently on trial against the Federal Trade Commission for antitrust claims, and the company doesn't want to further upset regulators by acquiring Scale AI, multiple people familiar with the matter said. As part of the deal, Meta will take a 49% stake in the data-labelling and annotation startup, The Information reported , while Wang will help lead a new AI research lab at the social networking company and will be joined by some of his colleagues. The New York Times was first to report about the new AI lab. Alexandr Wang, CEO of ScaleAI speaks on CNBC's Squawk Box outside the World Economic Forum in Davos, Switzerland on Jan. 23, 2025. Gerry Miller | CNBC Scale AI, founded in 2016, has made a splash in the era of generative AI by helping major tech companies like OpenAI, Google and Microsoft prepare data they use to train cutting-edge AI models. Meta is one of Scale AI's biggest customers, according to two people familiar with the matter. The startup, valued in a funding round about a year ago at $14 billion, is number 28 on CNBC's Disruptor 50 list . In mid-2024, the company signed one of the biggest recent commercial leases in San Francisco, gobbling up about 180,000 square feet of space in a downtown building that had been occupied by Airbnb . Scale AI has increasingly made in-roads into the defense industry, and in March announced a multimillion dollar deal with the Department of Defense . In November, it collaborated with Meta on Defense Llama, a custom version of Meta's open-source Llama foundation model designed specifically to \"support American national security missions,\" the company said in a blog post . Meta and Scale AI declined to comment. Read more CNBC tech news Small retailers on 'vacation from hell' as they seek clarity on Trump's China tariffs Chime's IPO will test public market appetite for fintech as companies line up to exit Disney and Universal sue AI image company Midjourney for unlicensed use of Star Wars, The Simpsons and more Nvidia CEO says quantum computing is reaching an 'inflection point' Meta's AI challenges Heading into 2025 , AI was one of Meta's top priorities. But Zuckerberg has grown agitated that rivals like OpenAI appear to be ahead in both underlying AI models and consumer-facing apps, current and former Meta employees said. Zuckerberg has been deprioritizing its Fundamental Artificial Intelligence Research unit, or FAIR, in favor of its more product-oriented GenAI team to help Meta make headway in AI and improve its Llama family of AI models, CNBC previously reported . Meta's release of its Llama 4 AI models in April was not well received by developers, further frustrating Zuckerberg, the people said. At the time, Meta only released two smaller versions of Llama 4 and said it would eventually release a bigger and more powerful \"Behemoth\" model. That model has yet to be made available due to Zuckerberg's concerns about its capabilities relative to competing models, the people said. In particular, there is concern about how Behemoth stacks up against the latest from companies like OpenAI and China's DeepSeek, whose models are preferred by the wider developer community. Following Llama 4's lackluster debut, Meta conducted a reorganization of its GenAI unit, splitting it into two. Connor Hayes, a longstanding Meta employee, was put in charge of AI Products, while AGI Foundations was given to Amir Frenkel, previously a vice president of engineering and product for Meta's Reality Labs hardware unit, and Ahmad Al-Dahle, the previous head of GenAI. Al-Dahle's new position as a co-leader was seen as a sign that Zuckerberg had lost confidence in him, the people said. Ahmad Al-Dahle, VP and Head of GenAI at Meta. Courtesy: Meta Zuckerberg admires Wang and considers him capable of a major role at Meta as an AI leader, the people said. A dropout from the Massachusetts Institute of Technology, Wang has built a sizable business and is familiar with AI's technical intricacies. The people described Wang as a \"wartime CEO\" who is in line with Zuckerberg's position that the U.S. faces increasing competition from China, thus requiring help from the tech industry. Wang told CNBC in January that he believes there is an \"AI war\" between the U.S. and China, and that the U.S. will need more computing power in order to compete. \"The United States is going to need a huge amount of computational capacity, a huge amount of infrastructure,\" Wang said at the time. \"We need to unleash U.S. energy to enable this AI boom.\" It's an unusual move for Zuckerberg, who has traditionally put loyalists in high-ranking positions. But it shows the magnitude of the moment and Zuckerberg's belief that a prominent outsider like Wang may be better positioned than any current Meta employee to bolster the company's position in AI, the people said. Wang also brings a lot of outside knowledge of how competitors like OpenAI are building their consumer chatbots and AI models. Data labelling and training has become more complicated in recent years as the capabilities of AI models has increased, said Vahan Petrosyan, the CEO of SuperAnnotate, one of Scale AI's competitors. \"I would say Scale have covered probably 70% of all the models that are built,\" Petrosyan said. With Wang and others from Scale AI, Meta could gain \"collective intelligence on how to build a better ChatGPT.\" \"When Meta is buying them, they're buying their intelligence,\" Petrosyan said. WATCH : Mark Zuckerberg lobbies Trump to avoid Meta antitrust trial watch now VIDEO 1:20 01:20 Mark Zuckerberg lobbies Trump to avoid Meta antitrust trial, reports say Closing Bell",
      "summary": "6-minute read",
      "url": "https://www.cnbc.com/2025/06/10/zuckerberg-makes-metas-biggest-bet-on-ai-14-billion-scale-ai-deal.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "big_tech",
      "word_count": 1296,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "OpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model",
      "content": "OpenAI has launched o3-pro, an AI model that the company claims is its most capable yet. O3-pro is a version of OpenAI’s o3 , a reasoning model that the startup launched earlier this year. As opposed to conventional AI models, reasoning models work through problems step by step, enabling them to perform more reliably in domains like physics, math, and coding. O3-pro is available for ChatGPT Pro and Team users starting Tuesday, replacing the o1-pro model. Enterprise and Edu users will get access the week after, OpenAI says. O3-pro is also live in OpenAI’s developer API as of this afternoon. O3-pro is priced at $20 per million input tokens and $80 per million output tokens in the API. Input tokens are tokens fed into the model, while output tokens are tokens that the model generates based on the input tokens. A million input tokens is equivalent to about 750,000 words, a bit longer than “War and Peace.” OpenAI o3-pro is available in the model picker for Pro and Team users starting today, replacing OpenAI o1-pro. Enterprise and Edu users will get access the week after. As o3-pro uses the same underlying model as o3, full safety details can be found in the o3 system card.… — OpenAI (@OpenAI) June 10, 2025 “In expert evaluations, reviewers consistently prefer o3-pro over o3 in every tested category and especially in key domains like science, education, programming, business, and writing help,” OpenAI writes in a changelog. “Reviewers also rated o3-pro consistently higher for clarity, comprehensiveness, instruction-following, and accuracy.” O3-pro has access to tools, according to OpenAI, allowing it to search the web, analyze files, reason about visual inputs, use Python, personalize its responses leveraging memory, and more. As a drawback, the model’s responses typically take longer than o1-pro to complete, according to OpenAI. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW O3-pro has other limitations. Temporary chats with the model in ChatGPT are disabled for now while OpenAI resolves a “technical issue.” O3-pro can’t generate images. And Canvas, OpenAI’s AI-powered workspace feature, isn’t supported by o3-pro. On the plus side, o3-pro achieves impressive scores in popular AI benchmarks, according to OpenAI’s internal testing. On AIME 2024, which evaluates a model’s math skills, o3-pro scores better than Google’s top-performing AI model, Gemini 2.5 Pro . O3-pro also beats Anthropic’s recently released Claude 4 Opus on GPQA Diamond, a test of PhD-level science knowledge.",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/10/openai-releases-o3-pro-a-souped-up-version-of-its-o3-ai-reasoning-model/?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 471,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Snap to launch smaller, lighter augmented reality Specs smartglasses in 2026",
      "content": "Key Points Snap on Tuesday announced its plans to release a sixth-generation of its augmented reality glasses in 2026, as competition in the smart glasses market continues to heat up. The maker of Snapchat said that its next-generation glasses will be called Specs, breaking with the company's Spectacles branding that it used for previous versions of its wearable devices. Larger tech rivals like Meta, Apple and Alphabet are investing heavily into cutting-edge head-worn devices. In this article SNAP Follow your favorite stocks CREATE FREE ACCOUNT The head of Snapchat operator Snap, Evan Spiegel, presents the new generation of Spectacles in Los Angeles on Sept. 17, 2024. Andrej Sokolow | Picture Alliance | Getty Images Snap on Tuesday announced its plans to release a sixth-generation of its augmented reality glasses in 2026, as competition in the smart glasses market continues to heat up. The maker of Snapchat said that its next-generation glasses will be called Specs, breaking with the company's Spectacles branding that it used for previous versions of its wearable devices. The Specs will use AR technology to let people see and interact with digital imagery that's overlaid over the physical world. Snap did not reveal a price or exact launch date for Specs, but the new glasses will be smaller and lighter than their predecessors, the company said. Snap's most recent Spectacles were released in September 2024 to developers only. That edition of the glasses was available under a leasing model that required users to commit to paying $99 a month for a full year. The consumer-focused Specs will run on the company's Snap OS operating systems. Snap said that developers will be able to incorporate Google's Gemini AI models into programs they develop for the smart glasses, giving coders more AI options to choose from as they write software for the device. Previously, developers could only use OpenAI's GPT family of AI models to build AR apps for the smart glasses. \"We couldn't be more excited about the extraordinary progress in artificial intelligence and augmented reality that is enabling new, human-centered computing experiences,\" Snap CEO Evan Spiegel said in a statement. When Snap launched its first Spectacles glasses in 2016, the $130 wearable was limited to simple features like helping users shoot short videos that they could post to Snapchat. The company updated its glasses with augmented reality displays in 2021 that allowed users to see virtual imagery overlaid by the glasses over what users saw in the real world. Since then, competition in the world of head-mounted computers has grown. Apple began selling its $3,500 Vision Pro goggles in February 2024, while Meta now has a range of cutting-edge products including Quest VR headsets, Ray-Ban Meta smartglasses and the experimental Project Orion AR glasses , which the social media company showed off last fall. Google, meanwhile, announced its own entry into the space in May when it revealed a $150 million partnership with Warby Parker , which said it will release its own smart glasses sometime after this year. Don’t miss these insights from CNBC PRO As the TACO trade goes viral, another is gaining traction: 'Anywhere But The USA' Oil giant BP seeks buyers for one of its crown jewels as it looks to stave off a takeover Best Stocks: This momentum play seeing huge investor accumulation is on the verge of another breakout Why the stock market continues to hold up in the face of uncertain trade policy WATCH : Why Meta and Snap think AR glasses will be the future of computing watch now VIDEO 12:33 12:33 Why Meta and Snap think AR glasses will be the future of computing Tech",
      "summary": "2-minute read",
      "url": "https://www.cnbc.com/2025/06/10/snap-specs-smart-glasses-ar.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 607,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "IBM now describing its first error-resistant quantum compute system",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~1600 words, got 308. URL: https://arstechnica.com/science/2025/06/ibm-is-now-detailing-what-its-first-quantum-compute-system-will-look-like/?utm_source=tldrnewsletter",
      "summary": "8-minute read",
      "url": "https://arstechnica.com/science/2025/06/ibm-is-now-detailing-what-its-first-quantum-compute-system-will-look-like/?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 308,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 308/1600 words (19.2%)"
    },
    {
      "title": "Android 16 is here",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://android-developers.googleblog.com/2025/06/android-16-is-here.html?utm_source=tldrnewsletter",
      "summary": "15-minute read",
      "url": "https://android-developers.googleblog.com/2025/06/android-16-is-here.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Google offers buyouts to employees across the company, including Search",
      "content": "Key Points Google on Tuesday offered buyouts to employees within its knowledge and information and central engineering units as well as marketing, research and communications teams, CNBC has learned. Some teams are also mandating office returns for remote workers who live within 50 miles of an office, Google confirmed to CNBC. The buyouts are part of the company's effort to reduce headcount, which Google has continued to do in waves since laying off 12,000 employees in 2023. In this article GOOGL Follow your favorite stocks CREATE FREE ACCOUNT watch now VIDEO 4:29 04:29 Google buyouts highlight tech's cost-cutting amid AI CapEx boom TechCheck Google on Tuesday offered buyouts to employees across several of its divisions, including those within its knowledge and information and central engineering units as well as marketing, research and communications teams, CNBC has learned. Knowledge and information, or K&I, is the unit that houses Google's search, ads and commerce divisions. The buyouts Tuesday are the company's latest effort to reduce headcount, which Google has continued to do in waves since laying off 12,000 employees in 2023. CNBC could not confirm how many employees were impacted by the latest round of buyouts. The Information reported earlier that the company offered buyouts to employees in the search and ads unit. The \"voluntary exit program\" applies to U.S.-based employees, and some teams are also mandating office returns for remote workers who live within 50 miles of an office, the company confirmed. They will be expected to assume a hybrid work schedule \"in order to bring folks more together in-person.\" \"Earlier this year, some of our teams introduced a voluntary exit program with severance for U.S.-based Googlers, and several more are now offering the program to support our important work ahead,\" Google spokesperson Courtenay Mencini wrote in an emailed statement to CNBC. K&I has approximately 20,000 employees. The unit underwent a reorganization in October that resulted in Google executive Nick Fox taking over the helm. Fox sent out a memo on Tuesday saying that employees who are not meeting expectations may want to take the buyout and that those who are excited by their work and doing well to remain with the company. \"I want to be very clear: If you're excited about your work, energized by the opportunity ahead, and performing well, I really (really!) hope you don't take this! We have ambitious plans and tons to get done,\" Fox wrote, according to the memo which was reviewed by CNBC. \"On the other hand, this VEP offers a supportive exit path for those of you who don't feel aligned with our strategy, don't feel energized by your work, or are having difficulty meeting the expectations of your role.\" The buyouts come after finance chief Anat Ashkenazi in October said that one of her top priorities would be to drive more cost cutting as Google expands its spending on artificial intelligence infrastructure in 2025. Google is also overhauling a popular internal learning platform to focus on teaching employees how to use modern AI tools in their work in a shift away from some of its nice-to-have programs to more business-essential offerings, CNBC reported Tuesday. FILE PHOTO: Nick Fox, VP of Product for Search and Assistant, speaks at a Google event on September 23, 2018 in San Francisco, California. Amy Osborne | Afp | Getty Images Buyouts are the new layoffs Google has done multiple buyout offers in a few units this year, making it a preferred strategy to reduce headcount. \" Platforms and Devices \" — the company's hardware unit that consists of 25,000 full-time employees working on Android, Chrome, ChromeOS, Google Photos, Google One and the Pixel devices — offered full-time U.S.-based employees the option to apply for a buyout in January. \" People Operations ,\" also known as the company's human resources department, offered voluntary buyouts in February. Google's legal and finance teams have also announced buyouts this year, a company spokesperson confirmed on Tuesday. As part of the People Operations buyouts, mid- to senior-level employees received severances of up to 14 weeks of salary and one additional week for every full year of service. In his Tuesday memo, Fox said he's been \"paying close attention\" to the other units' buyout offerings. Fox decided to also offer buyouts after hearing positive feedback from the other units, he wrote. The pivot to buyouts comes after Google faced backlash for laying off 6% of its workforce in January 2023. At the time, employees said their access to company systems were unexpectedly cut off. Some of them were long-time employees, stellar performers or on medical or maternity leave , CNBC reported at the time. The broadness and abruptness of the layoff at a time when the company was still reporting stellar earnings, created a division in trust and drop in morale. Executives later acknowledged its impact on morale. Earlier this year, some employees praised Google's decision to offer buyouts rather than immediately laying off employees, CNBC reported at the time. \"The P&D email portends layoffs, which sucks but offering buyouts first is what we asked for, is the right thing to do,\" one employee wrote in an internal forum at the time. However, buyout announcements have often been accompanied by a demand: come back to the office. Google has demanded that some remote employees return to the office if they want to keep their jobs and avoid being part of broader cost cuts at the company, CNBC reported in April. Don’t miss these insights from CNBC PRO As the TACO trade goes viral, another is gaining traction: 'Anywhere But The USA' Oil giant BP seeks buyers for one of its crown jewels as it looks to stave off a takeover Best Stocks: This momentum play seeing huge investor accumulation is on the verge of another breakout Why the stock market continues to hold up in the face of uncertain trade policy WATCH: Google Cloud is a great derivative play off growth of GenAI, says Evercore ISI's Mark Mahaney watch now VIDEO 2:38 02:38 Google Cloud is a great derivative play off growth of GenAI, says Evercore ISI's Mark Mahaney The Exchange",
      "summary": "5-minute read",
      "url": "https://www.cnbc.com/2025/06/10/google-buyouts-search-ads-unit.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 1019,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Reflections from Sam Altman",
      "content": "The second birthday of ChatGPT was only a little over a month ago, and now we have transitioned into the next paradigm of models that can do complex reasoning. New years get people in a reflective mood, and I wanted to share some personal thoughts about how it has gone so far, and some of the things I’ve learned along the way. As we get closer to AGI, it feels like an important time to look at the progress of our company. There is still so much to understand, still so much we don’t know, and it’s still so early. But we know a lot more than we did when we started. We started OpenAI almost nine years ago because we believed that AGI was possible, and that it could be the most impactful technology in human history. We wanted to figure out how to build it and make it broadly beneficial; we were excited to try to make our mark on history. Our ambitions were extraordinarily high and so was our belief that the work might benefit society in an equally extraordinary way. At the time, very few people cared, and if they did, it was mostly because they thought we had no chance of success. In 2022, OpenAI was a quiet research lab working on something temporarily called “Chat With GPT-3.5”. (We are much better at research than we are at naming things.) We had been watching people use the playground feature of our API and knew that developers were really enjoying talking to the model. We thought building a demo around that experience would show people something important about the future and help us make our models better and safer. We ended up mercifully calling it ChatGPT instead, and launched it on November 30th of 2022. We always knew, abstractly, that at some point we would hit a tipping point and the AI revolution would get kicked off. But we didn’t know what the moment would be. To our surprise, it turned out to be this. The launch of ChatGPT kicked off a growth curve like nothing we have ever seen—in our company, our industry, and the world broadly. We are finally seeing some of the massive upside we have always hoped for from AI, and we can see how much more will come soon. It hasn’t been easy. The road hasn’t been smooth and the right choices haven’t been obvious. In the last two years, we had to build an entire company, almost from scratch, around this new technology. There is no way to train people for this except by doing it, and when the technology category is completely new, there is no one at all who can tell you exactly how it should be done. Building up a company at such high velocity with so little training is a messy process. It’s often two steps forward, one step back (and sometimes, one step forward and two steps back). Mistakes get corrected as you go along, but there aren’t really any handbooks or guideposts when you’re doing original work. Moving at speed in uncharted waters is an incredible experience, but it is also immensely stressful for all the players. Conflicts and misunderstanding abound. These years have been the most rewarding, fun, best, interesting, exhausting, stressful, and—particularly for the last two—unpleasant years of my life so far. The overwhelming feeling is gratitude; I know that someday I’ll be retired at our ranch watching the plants grow, a little bored, and will think back at how cool it was that I got to do the work I dreamed of since I was a little kid. I try to remember that on any given Friday, when seven things go badly wrong by 1 pm. A little over a year ago, on one particular Friday, the main thing that had gone wrong that day was that I got fired by surprise on a video call, and then right after we hung up the board published a blog post about it. I was in a hotel room in Las Vegas. It felt, to a degree that is almost impossible to explain, like a dream gone wrong. Getting fired in public with no warning kicked off a really crazy few hours, and a pretty crazy few days. The “fog of war” was the strangest part. None of us were able to get satisfactory answers about what had happened, or why. The whole event was, in my opinion, a big failure of governance by well-meaning people, myself included. Looking back, I certainly wish I had done things differently, and I’d like to believe I’m a better, more thoughtful leader today than I was a year ago. I also learned the importance of a board with diverse viewpoints and broad experience in managing a complex set of challenges. Good governance requires a lot of trust and credibility. I appreciate the way so many people worked together to build a stronger system of governance for OpenAI that enables us to pursue our mission of ensuring that AGI benefits all of humanity. My biggest takeaway is how much I have to be thankful for and how many people I owe gratitude towards: to everyone who works at OpenAI and has chosen to spend their time and effort going after this dream, to friends who helped us get through the crisis moments, to our partners and customers who supported us and entrusted us to enable their success, and to the people in my life who showed me how much they cared. [1] We all got back to the work in a more cohesive and positive way and I’m very proud of our focus since then. We have done what is easily some of our best research ever. We grew from about 100 million weekly active users to more than 300 million. Most of all, we have continued to put technology out into the world that people genuinely seem to love and that solves real problems. Nine years ago, we really had no idea what we were eventually going to become; even now, we only sort of know. AI development has taken many twists and turns and we expect more in the future. Some of the twists have been joyful; some have been hard. It’s been fun watching a steady stream of research miracles occur, and a lot of naysayers have become true believers. We’ve also seen some colleagues split off and become competitors. Teams tend to turn over as they scale, and OpenAI scales really fast. I think some of this is unavoidable—startups usually see a lot of turnover at each new major level of scale, and at OpenAI numbers go up by orders of magnitude every few months. The last two years have been like a decade at a normal company. When any company grows and evolves so fast, interests naturally diverge. And when any company in an important industry is in the lead, lots of people attack it for all sorts of reasons, especially when they are trying to compete with it. Our vision won’t change; our tactics will continue to evolve. For example, when we started we had no idea we would have to build a product company; we thought we were just going to do great research. We also had no idea we would need such a crazy amount of capital. There are new things we have to go build now that we didn’t understand a few years ago, and there will be new things in the future we can barely imagine now. We are proud of our track-record on research and deployment so far, and are committed to continuing to advance our thinking on safety and benefits sharing. We continue to believe that the best way to make an AI system safe is by iteratively and gradually releasing it into the world, giving society time to adapt and co-evolve with the technology, learning from experience, and continuing to make the technology safer. We believe in the importance of being world leaders on safety and alignment research, and in guiding that research with feedback from real world applications. We are now confident we know how to build AGI as we have traditionally understood it. We believe that, in 2025, we may see the first AI agents “join the workforce” and materially change the output of companies. We continue to believe that iteratively putting great tools in the hands of people leads to great, broadly-distributed outcomes. We are beginning to turn our aim beyond that, to superintelligence in the true sense of the word. We love our current products, but we are here for the glorious future. With superintelligence, we can do anything else. Superintelligent tools could massively accelerate scientific discovery and innovation well beyond what we are capable of doing on our own, and in turn massively increase abundance and prosperity. This sounds like science fiction right now, and somewhat crazy to even talk about it. That’s alright—we’ve been there before and we’re OK with being there again. We’re pretty confident that in the next few years, everyone will see what we see, and that the need to act with great care, while still maximizing broad benefit and empowerment, is so important. Given the possibilities of our work, OpenAI cannot be a normal company. How lucky and humbling it is to be able to play a role in this work. (Thanks to Josh Tyrangiel for sort of prompting this . I wish we had had a lot more time.) [1] There were a lot of people who did incredible and gigantic amounts of work to help OpenAI, and me personally, during those few days, but two people stood out from all others. Ron Conway and Brian Chesky went so far above and beyond the call of duty that I’m not even sure how to describe it. I’ve of course heard stories about Ron’s ability and tenaciousness for years and I’ve spent a lot of time with Brian over the past couple of years getting a huge amount of help and advice. But there’s nothing quite like being in the foxhole with people to see what they can really do. I am reasonably confident OpenAI would have fallen apart without their help; they worked around the clock for days until things were done. Although they worked unbelievably hard, they stayed calm and had clear strategic thought and great advice throughout. They stopped me from making several mistakes and made none themselves. They used their vast networks for everything needed and were able to navigate many complex situations. And I’m sure they did a lot of things I don’t know about. What I will remember most, though, is their care, compassion, and support. I thought I knew what it looked like to support a founder and a company, and in some small sense I did. But I have never before seen, or even heard of, anything like what these guys did, and now I get more fully why they have the legendary status they do. They are different and both fully deserve their genuinely unique reputations, but they are similar in their remarkable ability to move mountains and help, and in their unwavering commitment in times of need. The tech industry is far better off for having both of them in it. There are others like them; it is an amazingly special thing about our industry and does much more to make it all work than people realize. I look forward to paying it forward. On a more personal note, thanks especially to Ollie for his support that weekend and always; he is incredible in every way and no one could ask for a better partner.",
      "summary": "8-minute read",
      "url": "https://blog.samaltman.com/reflections?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 1951,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Partial Keyframes",
      "content": "Introduction The most common way to write a CSS keyframe animation is to specify a starting point and an ending point, using from and to : Copy to clipboard @ keyframes fadeOut { from { opacity : 1 ; } to { opacity : 0 ; } } What do you suppose happens if we omit one of these blocks? For example, what if we only specify the ending point? Copy to clipboard @ keyframes fadeToTransparent { to { opacity : 0 ; } } This still works! Even without defining a starting opacity, our element will still fade out when this keyframe animation is applied. When I discovered this pattern a few years ago, I thought it was a neat little trick to shave a few bytes off my bundles, a more concise way to write keyframe animations. But it turns out, there’s a way more significant and exciting benefit to this pattern! This lil’ trick unlocks a hidden capability within keyframe animations that makes them dynamic and composable. ✨ In this tutorial, I’ll show you how this trick works, and we’ll explore some of the cool things we can do with it. There’s also a bonus tip at the end, showcasing how modern CSS makes keyframe animations even more powerful! 😄 Link to this heading Inherited values When we omit the from block from our keyframe animation, the animation’s starting values will be inherited from context. This’ll be easier to explain with a demo. Check this out: Variant : Full Partial opacity: 1.0 opacity: 0.6 opacity: 0.3 Apply keyframe Copy to clipboard @ keyframes fadeOut { from { opacity : 1 ; } to { opacity : 0 ; } } Our “traditional” setup really only works for elements that are fully opaque by default, like that first yellow ball. The others immediately snap to full opacity before gradually fading out: By contrast, when we omit the from part of the keyframe, the animation will inherit the element’s current opacity and start fading from there. 🤯 We can visualize this difference with a graph: Full Partial Opacity Time 0 0.25 0.5 0.75 1 Keyframe Start Here’s a playground with the full code, in case you wanted to poke at this a bit: index.html index.js styles.css < style > @keyframes fadeToTransparent { to { opacity : 0 ; } } .to-transparent { animation : fadeToTransparent 1000 ms forwards ; } </ style > < div class = \"row\" > < div class = \"ball\" > </ div > < div class = \"ball\" style = \" opacity : 0.6 \" > </ div > < div class = \"ball\" style = \" opacity : 0.3 \" > </ div > </ div > < button > Toggle keyframe </ button > result console Refresh results pane In this playground, the resting opacity is set via inline style, <div style=\"opacity: 0.6\"> , but that isn’t a requirement for this pattern. It’ll still work if the resting opacity is set with a CSS class, or however else you typically apply CSS! And if an element doesn’t set the opacity property at all, like that first .ball in the playground, the default value of 1 will be used. This is awesome, since it means that our fadeToTransparent keyframe is just as easy-to-use as a traditional fadeOut animation. We don’t have to explicitly set opacity: 1 in order to fade stuff out with this technique! Link to this heading Omitting the destination The same trick works in the other direction. If you omit the to value, the animation will animate from a specified value to whatever value it’s currently set to: Target Opacity : 0.5 opacity: 0.5 Render element Copy to clipboard < style > @ keyframes fadeFromTransparent { from { opacity : 0 ; } } . ball { animation : fadeFromTransparent 1000 ms ; } </ style > In this demo, we animate from 0 to whatever the element’s specified opacity is. If the element doesn’t have an explicit opacity set, it will default to 1 , acting like a regular fade-in animation. What’s the use case for this? This trick is handy when working with elements that aren’t fully opaque by default, or have state-based opacity. Here’s a real-world example: opacity: 0.7 Render element Copy to clipboard < style > @ keyframes fadeFromTransparent { from { opacity : 0 ; } } . icon-btn { opacity : 0.7 ; animation : fadeFromTransparent 1000 ms ; & : hover , &: active , &:focus-visible { opacity: 1 ; } } </ style > This button has a resting opacity of 0.7 , but when the user hovers over it, the opacity flips to 1 . This is a useful UX pattern because it helps convey that this element is interactive. * This is known as “affordance” in UX design circles. This button already has pretty good affordance, since the cursor flips to a pointer on hover, but I find the experience is even better with an additional indication. (I’ve also set it up here to rise to full opacity on focus and tap, so that you can experience this even if you’re not using a mouse, but in practice I think it’s fine for this particular detail to be mouse-only.) This button also fades in on mount, and so I’m using the partial keyframes trick to ensure that it always fades to the correct value. If the user happens to be hovering over it when it mounts, it’ll fade to 1 . Otherwise, it’ll fade to 0.7 . ✨ Link to this heading Animating to a dynamic value This next bit really blew my mind when I discovered it. With partial keyframes, we can animate to a value specified by another keyframe animation! Check this out: Twinkle Duration : 250 Render element Copy to clipboard < style > @ keyframes twinkle { from { opacity : 0.25 ; } to { opacity : 0.75 ; } } @ keyframes fadeFromTransparent { from { opacity : 0 ; } } . ball { animation : twinkle 250 ms alternate infinite , fadeFromTransparent 2000 ms ; } </ style > Let me explain what’s going on here: The twinkle keyframe animation causes the ball to oscillate between 0.25 and 0.75 opacity. It bounces back and forth thanks to the alternate keyword, and runs forever thanks to infinite . Our fadeFromTransparent keyframe sets an initial opacity of 0 , but doesn’t specify the target opacity. When combined, we fade from 0 to the ever-changing value set within twinkle . It essentially allows us to gradually introduce the flickering twinkle animation. Let’s graph the opacity changes over time, so that we can really see what’s going on here. Toggle between the two values to see the effect of stacking these keyframe animations: Animation: twinkle fadeFromTransparent twinkle + fadeFromTransparent Opacity Time 0 0.25 0.5 0.75 1 Pretty wild, right?? Multiple keyframe animations can modify the same property without one cancelling the other! Link to this heading Whimsical Animations ✨ If you found this tutorial useful, I have some good news for you! Over the past year, I’ve been working on a new course on whimsical animations. This lil’ tip was plucked straight from the course, and it’s just the tip of the iceberg. I’ll show you all of the tricks and techniques I use to come up with polished, next-level animations. You’ll learn how to use CSS, SVG, and 2D Canvas. If that sounds worthwhile, you can sign up for updates here: Whimsical Animations (opens in new tab) If you sign up, you’ll be the first to know when the course is open for registration. I also plan on sending some free goodies in the weeks ahead. 😄 Link to this heading Bonus: dynamic values in keyframe definitions There’s one more trick I want to share with you. 😄 Let’s suppose we’re building the following tail-wagging animation: Play animation For this sort of endless motion, CSS Keyframe animations are the best tool in the toolbox. But hm, keyframe animations require that we set specific, hardcoded values . So, if we only had one ball, this wouldn’t be a big deal. We could set it up like this: Copy to clipboard @ keyframes oscillate { from { transform : translateX ( -16 px ); } to { transform : translateX ( 16 px ); } } . ball { animation : oscillate 1000 ms infinite alternate ; } But, we have four balls, and they each oscillate by a different amount. 🤔 For years, this was a thorn in my side. I had to either create four nearly-identical keyframe animations, each with a different hardcoded value, or I would wire it up using CSS transitions and JavaScript intervals. Both options were thoroughly unsatisfying. And then, I had a mindblowing realization. Check this out: Copy to clipboard @ keyframes oscillate { from { transform : translateX ( calc ( var ( --amount ) * -1 )); } to { transform : translateX ( var ( --amount )); } } Instead of hardcoding a specific value like 16px inside our keyframe definition, we can access a CSS variable! With a little help from calc , we can flip that value to its negative counterpart, so that we can oscillate to/from a dynamic value. In order for this to work, we need to define an --amount value on each element that is being animated. For example, we could do that with an inline style: Copy to clipboard < style > . ball { animation : oscillate 1000 ms infinite alternate ; } </ style > < div class = \" ball \" style = \" --amount: 8px \" ></ div > < div class = \" ball \" style = \" --amount: 16px \" ></ div > < div class = \" ball \" style = \" --amount: 32px \" ></ div > < div class = \" ball \" style = \" --amount: 64px \" ></ div > How freaking cool is this?! When I first discovered this trick, it kinda blew my mind. I had no idea we could read CSS variables from within a keyframe animation! This was the final puzzle piece that fully unlocked keyframe animations for me, making them just as dynamic and flexible as CSS transitions. Here’s a full editable demo that showcases this technique: index.html index.js styles.css < style > @keyframes oscillate { from { transform : translateX ( calc ( var ( --amount ) * -1 ) ) ; } to { transform : translateX ( var ( --amount ) ) ; } } .ball { animation : oscillate 700 ms ease-in-out alternate infinite ; } </ style > <!-- Edit these values to change the oscillation amount: --> < div class = \"ball\" style = \" --amount : 8 px \" > </ div > < div class = \"ball\" style = \" --amount : 16 px \" > </ div > < div class = \"ball\" style = \" --amount : 32 px \" > </ div > < div class = \"ball\" style = \" --amount : 64 px \" > </ div > < button > Play/pause animation </ button > result console Refresh results pane In terms of browser support, I believe that this has been supported since CSS variables themselves were introduced. According to caniuse, support is sitting around 96% (opens in new tab) . CSS keyframe animations can be pretty confusing, so if you’re not sure what keywords like alternate are doing, you can check out my interactive blog post on the subject. And if you’d like to learn about whimsical animation more broadly, don’t forget to sign up for updates about my course! (opens in new tab) Last updated on June 11th, 2025 # of hits",
      "summary": "9-minute read",
      "url": "https://www.joshwcomeau.com/animation/partial-keyframes/?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 1972,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "OpenAI announces 80% price drop for o3, its most powerful reasoning model",
      "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy. Learn more Good news, AI developers! OpenAI has announced a substantial price cut on o3 , its flagship reasoning large language model (LMM), slashing costs by a whopping 80% for both input and output tokens . (Recall tokens are the individual numeric strings that LLMs use to represent words, phrases, mathematical and coding strings, and other content. They are representations of the semantic constructions the model has learned through training, and in essence, are the LLMs’ native language. Most LLM providers offer their models through application programming interfaces or APIs that developers can build apps atop of or plug their external apps into, and most LLM providers charge them for the privilege at a cost per million tokens). The update positions the model as a more accessible option for developers seeking advanced reasoning capabilities, and places OpenAI in more direct pricing competition with rival models such as Gemini 2.5 Pro from Google DeepMind, Claude Opus 4 from Anthropic, and DeepSeek’s reasoning suite. Announced by Altman himself on X Sam Altman, CEO of OpenAI, confirmed the change in a post on X highlighting that the new pricing is intended to encourage broader experimentation, writing: “we dropped the price of o3 by 80%!! excited to see what people will do with it now. think you’ll also be happy with o3-pro pricing for the performance :)” The cost of using o3 is now $2 per million input tokens and $8 per million output tokens, with an extra discount of $0.50 per million tokens when the user enters information that has been “cached,” or is stored and identical to what they provided before. This marks a significant reduction from the previous rates of $10 (input) and $40 (output), as OpenAI researcher Noam Brown pointed out on X. Ray Fernando , a developer and early adopter, celebrated the pricing drop in a post writing “LFG!” short for “let’s fucking go!” The sentiment reflects a growing enthusiasm among builders looking to scale their projects without prohibitive model access costs. Price comparison to other rival reasoning LLMs The price adjustment comes at a time when AI providers are competing more aggressively on both performance and affordability. A comparison with other leading AI reasoning models illustrates how significant this move could be: Gemini 2.5 Pro Preview , developed by Google DeepMind, charges between $1.25 and $2.50 for input depending on prompt size, and $10 to $15 for output. While its integration with Google Search offers additional functionality, that service carries its own cost — free for the first 1,500 requests per day, then $35 per thousand requests. Claude Opus 4 , marketed by Anthropic as a model optimized for complex tasks, is the most expensive of the group, charging $15 per million input tokens and $75 for output. Prompt caching read and write services come at $1.50 and $18.75 respectively, although users can unlock a 50% discount with batch processing. DeepSeek’s models, particularly DeepSeek-Reasoner and DeepSeek-Chat, undercut much of the market with aggressive low pricing. Input tokens range from $0.07 to $0.55 depending on caching and time of day, while output ranges from $1.10 to $2.19. Discounted rates during off-peak hours bring prices down even further, to as low as $0.035 for cached inputs. Model Input Cached Input Output Discount Notes OpenAI o3 $2.00 (down from $10.00) $0.50 $8.00 (down from $40.00) Flex Processing: $5 / $20 Gemini 2.5 Pro $1.25 – $2.50 $0.31 – $0.625 $10.00 – $15.00 Higher rate applies to prompts >200k tokens Claude Opus 4 $15.00 $1.50 (read) / $18.75 (write) $75.00 50% off with batch processing DeepSeek-Chat $0.07 (hit)$0.27 (miss) — $1.10 50% off during off-peak hours DeepSeek-Reasoner $0.14 (hit)$0.55 (miss) — $2.19 75% off during off-peak hours In addition, independent third-party AI model comparison and research group Artificial Analysis ran the new o3 through its suite of benchmarking tests on various tasks, and found it cost $390 to complete them all, versus $971 for Gemini 2.5 Pro and $342 for Claude 4 Sonnet. Narrowing the cost vs. intelligence gap for developers OpenAI’s pricing move not only narrows the gap with ultra-low-cost models like DeepSeek but also puts downward pressure on higher-priced offerings like Claude Opus and Gemini Pro. Unlike Claude or Gemini, OpenAI’s o3 also now offers a flex mode for synchronous processing that charges $5 for input and $20 for output per million tokens, giving developers more control over compute cost and latency depending on workload type. o3 is currently available through the OpenAI API and Playground. Users with balances as low as a few dollars can now explore the model’s full capabilities, enabling prototyping and deployment with fewer financial barriers. This could particularly benefit startups, research teams, and individual developers who previously found higher-tier model access cost-prohibitive. By substantially lowering the cost of its most advanced reasoning model, OpenAI is signaling a broader trend in the generative AI space: premium performance is quickly becoming more affordable, and developers now have a growing number of viable, economically scalable options. Daily insights on business use cases with VB Daily If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.",
      "summary": "4-minute read",
      "url": "https://venturebeat.com/ai/openai-announces-80-price-drop-for-o3-its-most-powerful-reasoning-model/?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 899,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "God is hungry for Context: First thoughts on o3 pro",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.latent.space/p/o3-pro?utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://www.latent.space/p/o3-pro?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "News Sites Are Getting Crushed by Google's New AI Tools",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141?st=NSkAAm&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141?st=NSkAAm&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "How AI Saved My Company From a 2-Year Litigation Nightmare",
      "content": "Posted in Entrepreneurship on June 8, 2025 After 2+ years and a tremendous waste of energy and money, my firm Calm Company Fund resolved a lawsuit against us on very favorable terms. What started as one of the most challenging periods of my life became an unexpected masterclass in using AI to level the playing field against well-funded opponents. If you’re an entrepreneur, especially one without a massive legal budget, this story contains lessons that could save your company. The Broken System: Why Defendants Always Lose The Delaware legal system is fundamentally broken for defendants. Due to the ironically labeled “American Rule,” even if you win, it’s extremely rare for the other side to have to cover your attorneys’ fees, much less compensate you for lost time and energy. The best case scenario is spending a small fortune to maintain the status quo of: not guilty of anything. By and large, the U.S. legal system seems to imbue people and companies with a strong right to file dumb lawsuits, even if they are mostly fabricated or based on incredibly weak claims. And the legal costs to defend against a lawsuit all the way through trial can be enormous. $1 million is a reasonable expectation. “But wait, if the lawsuit is truly dumb, won’t a judge just ‘throw it out’?“ This is the first question you get from anybody who has not actually had to interact with the U.S. legal system. And, yes, while there are two primary opportunities for a judge to throw out a case without it going to a jury trial, once you look at the basic details of each phase, you realize that they are pretty poor protection to defendants from the cost of a defense. Let’s look at the two of them. Motion to Dismiss: This is usually what folks who don’t have legal experience are thinking of when they think of a judge throwing out a dumb lawsuit. That somebody will file a lawsuit with basically no actual supporting facts or evidence or founded legal claims, and a studious judge will look it over and say this is BS and throw the case out. It happens relatively early on in the process. And it’s true that if you win a motion to dismiss, you will be able to wrap the case up quickly without too much cost. But here is the standard in Delaware for winning a motion to dismiss, and the standards are pretty similar in other U.S. jurisdictions. The court must accept all well-pleaded factual allegations as true , draw all reasonable inferences in the plaintiff’s favor , and dismiss the complaint only if the plaintiff could not recover under any reasonably conceivable set of circumstances . Put another way, you basically have to accept almost all of the entire initial lawsuit as true, in particular, the facts being alleged as true. And then determine that even if all of this stuff was totally true, the case should still not proceed. This really only works if there are technical flaws in the lawsuit that render the entire thing invalid regardless of the truth of the matter. A classic example would be that this is a federal issue being filed in a state court, or vice versa, or that the statute of limitations has passed, or that the plaintiff does not actually have standing to bring this lawsuit even if what they say happened, happened. It cannot in any way look at the truth or falseness of the claims being made or at the real meaningful validity of the legal arguments being weighed. All of those have to simply be accepted as true, and otherwise you basically can’t win a motion to dismiss. Unfortunately, if you can’t win a motion to dismiss, you will essentially be forced to move to discovery and several other shorter phases of the trial. But discovery ends up being an incredibly time-consuming and expensive process where you’re forced to hand over all sorts of internal documents and communications and all sorts of things that could potentially be relevant to the lawsuit. There can be plenty of fights over what does or does not get included there, and you also typically have to involve third-party vendors to ensure that it is happening appropriately. And all this stuff is incredibly expensive, potentially to the tune of hundreds of thousands of dollars that you are forced to do before you can make any further arguments that this case should be over. Summary Judgement: Once discovery is over, both sides would have an opportunity to file a motion for summary judgment. The motion for summary judgment is the first opportunity. Again, you have probably at this point already spent months or maybe up to a year of work and hundreds of thousands of dollars, and now this is your first opportunity to make a strong argument that the legal claims against you are so weak that they should be thrown out. Here is the standard for it: “The judgment sought shall be rendered forthwith if the pleadings, depositions, answers to interrogatories, and admissions on file, together with the affidavits, if any, show that there is no genuine issue as to any material fact and that the moving party is entitled to a judgment as a matter of law .” Summary judgment is a two-part standard, and you have to win both parts in order to win summary judgment. The first argument you need to make is that there are no genuine issues of material fact. So you essentially need to persuade the judge that the facts that they need to understand in order to rule on the case are known and not disputed between the two parties. You, in this case, can’t really make any fact-based arguments. If one party is alleging that somebody said something and the other party says, “Well, we didn’t say that,” that is a disputed fact. That’s the kind of thing that a jury trial will need to hear evidence and look at testimony and do all that to determine facts. So you need to argue that all the facts that you need are here and are undisputed. For example, if all the information is in contracts which we have the text of and emails that we have the text of, and certain things are not disputed between the two parties, then you have to establish that there is not any fact-based questions that need to be sorted out by a jury trial. And you then also need to win the idea that, based on these non-disputed facts, you should win as a matter of law. So you really can only be making arguments about the legal invalidity of the case against you, not about the truthiness of the case against you. This can be an incredibly high bar to clear because most cases are probably going to have some disputed facts. So even if you have an incredibly compelling case here, you probably will have a tough time winning summary judgment if there are any material facts that you know the parties are disputing. This is your second and potentially last time to have a judge throw the case out before it goes to a jury trial, and in both cases, the standard for the judge to throw it out is just incredibly high. The odds are very high that you’re going to continue having to go through the case process and continue spending a lot of money unless you come to a settlement. So here’s the asymmetric trap every defendant faces : Defending yourself all the way through trial will definitely be incredibly expensive, no matter how strong your case is. The chance of losing in a jury trial is always more than 0%, and even if you win, you will almost certainly not be able to recover your legal costs. So the Prisoner’s Dilemma most defendants face is: Do you want to pay us some amount for a 100% chance of making this go away now, or spend much more money with at least some chance of losing at trial? This is why ~98% of civil cases settle before trial. Founder Takeaway #1: Understanding this reality has profound implications for how you structure deals and contracts. The actual language of a contract almost doesn’t matter unless the underlying issue is worth $2-3 million or more. No matter how iron-clad your contract language, the other side can always raise enough doubt to force the matter to trial, and it can cost $1 million+ to litigate through completion . This means contract disputes over smaller amounts mostly depend on leverage or a willingness to outspend the other side. Consider this in your future contract negotiations—you’re better off identifying risks and potential areas for dispute upfront than spending money having a lawyer “button up” language you can’t afford to enforce anyway. I also now deeply understand why experienced business people have a strict “no assholes” policy. I always thought this was throwaway advice, but it only takes one lawsuit to make it clear how incredibly valuable it is to work with people you trust. Into the Litigation Black Hole Two years ago, I was sued for the first time. I’m going to leave out many of the specific facts and focus on what I learned about the process of dealing with litigation—the decisions I made and how I was feeling along the way. This was uncharted territory for me. The first curveball: our primary counsel wasn’t based in Delaware, and when someone files a lawsuit in Delaware, you immediately need Delaware-barred lawyers to defend you. You’re suddenly scrambling for referrals to lawyers you’ve never worked with to help you with what is now the most important legal issue of your life. And you don’t have much time—litigation comes with tight response deadlines. Are They An Oncologist or General Contractor? When working with paid professionals, I think people tend to put them into one of two categories: doctors or general contractors. With doctors, if your oncologist says you have a lump that needs to come out, you pretty much do whatever they say. You’re the patient; they’re the expert leading the charge. With general contractors doing home renovations, most people intuitively know that’s not the right approach. You stay on top of them, give clear instructions, try to understand the issues yourself, and assume you need to provide lots of direction and monitoring to get what you want done. Most people—entrepreneurs included, certainly myself—put lawyers in the doctor category when they should really treat them more like general contractors. The Delegation Trap When the lawsuit hit, I asked various people I trusted for their take. Almost across the board, the perspective was: “This is ridiculous. It doesn’t make any sense. I can’t even understand how there would be damages.” So I made what seemed like a rational, delegating-CEO decision: don’t let this take up too much brain space. Give it to lawyers and let them handle it. In retrospect, this was a huge mistake. This approach might be right if your business can comfortably spend $500K-$1M to defend a lawsuit. But if, like many entrepreneurs, you don’t have that kind of money, and the mounting costs will become a source of leverage against you, then you need to take it seriously and go “Founder Mode” on the legal process. The Expensive Treadmill of “Best Representation” When you tell lawyers “what do you think we should do,” they’re going to give you the “best representation” they can—because that’s what they’re incentivized to do. This means ticking every box, being as thorough as humanly possible, and taking everything the other side throws at them as seriously as possible. This approach can cost you a fortune. What lawyers typically won’t do is come up with creative ways for you to accumulate leverage against the other side to get them to back down. If you understand the dynamics I laid out earlier—that you’re essentially angling toward a settlement—this standard approach is exactly the opposite of what you should be doing. You want to spend as little money as possible so costs don’t become leverage against you. And you want to find ways to make it clear that pursuing litigation against you isn’t in the other side’s best interest. How exactly to create that leverage is more of a conversation over beers than a blog post topic. But this should be your primary focus—not perfectly playing the tit-for-tat game of legal proceedings that will mostly end up being irrelevant when two sides sit at a negotiating table. The Breaking Point I learned this the hard way. We spent a large portion of the initial proceedings allowing lawyers to follow the standard process, charging us $1,000/hour to check all the boxes and respond to every silly and pointless motion. The mounting legal bills became an accumulating source of leverage against us. Meanwhile, these new lawyers weren’t presenting me with any viable options other than continuing to fire-hose money into an absurd litigation process. The current bills piled up and the prospective costs froze our ability to make nearly all strategic decisions—like hiring or raising new funds. As I expressed my enormous frustration with this black hole of time, energy, and money, my partner gave me the pep talk I needed: she told me I needed to take control of the situation myself. She was right. How AI Became My Secret Weapon It was at this breaking point that I started using AI to review all contracts and filings, understand case law, and consider courses of action. I showed up to meetings with our lawyers with a strong point of view on where we should minimize costs and maximize our leverage. My general sense is that most lawyers are used to being totally deferred to by clients, and in this case, the new counsel we had started working with really did not like my new approach. So, I fired them and got new lawyers. It worked—but there’s no way I could have done it without LLMs getting good enough at exactly the moment I needed them. My AI Legal Research Workflow Here’s the specific process that saved my company: 1: Upload Everything to AI Projects. Use the projects feature in Claude or ChatGPT to upload all relevant documents—contracts, motions, emails, anything relevant to your case. You might need to be selective if you hit context window limitations with thousands of pages, but include as much source material as possible. For what it’s worth I didn’t necessarily find one platform (Claude or ChatGPT) or even specific models to be materially better than others at this work. 2: Treat AI Like a Coach, Not an Oracle. Instead of asking “what do we do next,” I would ask the AI to explain the principles involved, the mechanics of what was happening in each motion, where we were in the process. I wanted to understand the controlling variables myself, not just get answers. The AI is infinitely patient—you can drill down into anything you don’t understand and ask “explain that to me” or “how does that work?” This coaching approach was incredibly effective. Mental Health Tip: Do this research using voice mode while walking. Legal research isn’t fun, but being outside in fresh air using your voice rather than staring at a screen makes it much more bearable. 3: Leverage AI for Contract Analysis. AI excels at reviewing lengthy contracts. When you have a 400-page agreement where page 6 references “customary exceptions” that are finally listed on page 421, AI can instantly find and connect these references without you having to jump around through table of contents. Always have the AI excerpt the actual contract language with page numbers, then verify by searching the document yourself. Don’t take the AI’s word for it—confirm everything. 4: Have AI draft arguments and briefs. Writing from a blank page is always much more work than editing. I started showing up with outlines or drafts for our filings and briefs and asking my lawyers to discuss it with me and then edit them, rather than crafting everything completely from scratch. This is a huge time and money saver. Working with Lawyers in the AI Era I wasn’t totally upfront with even my new lawyers",
      "summary": "23-minute read",
      "url": "https://tylertringas.com/ai-legal/?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 2720,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 2720/4600 words (59.1%)"
    },
    {
      "title": "Apple Retreats",
      "content": "Apple Retreats Tuesday, June 10, 2025 Listen to Podcast Listen to this post : Log in to listen If WWDC’s opening video — which cast Apple executives as characters in the upcoming F1 movie, with Senior Vice President of Software Engineering Craig Federighi in the starring role — was a bit of a fever dream, then the opening of Federighi’s presentation of Apple’s annual software updates had the air of a regretful admission the morning after that mistakes had been made. To Federighi and Apple’s credit, there was no attempt to dance around the fact that last year’s WWDC ended up being a fever dream in its own right: Apple promised a litany of AI-enabled features, particularly for Siri, that have not and may not ever ship. Federighi, after listing the basic and hardly ground-breaking Apple Intelligence features that did ship, admitted right up front: As we’ve shared, we’re continuing our work to deliver the features that make Siri even more personal. This work needed more time to reach our high quality bar, and we look forward to sharing more about it in the coming year. That’s only two sentences, of course, but the admission was notable and necessary; last year’s WWDC — which garnered high praise, including from yours truly — revealed that Something Is Rotten in the State of Cupertino . That was the title of John Gruber’s Daring Fireball article excoriating Apple for promising something it could not deliver: Even with everything Apple overpromised (if not outright lied about) at the WWDC keynote, the initial takeaway from WWDC from the news media was wrongly focused on their partnership with OpenAI. The conventional wisdom coming out of the keynote was that Apple had just announced something called “Apple Intelligence” but it was powered by ChatGPT, when in fact, the story Apple told was that they — Apple — had built an entire system called Apple Intelligence, entirely powered by Apple’s own AI technology, and that it spanned from on-device execution all the way to a new Private Cloud Compute infrastructure they not only owned but are powering with their own custom-designed server hardware based on Apple Silicon chips. And that on top of all that, as a proverbial cherry on top, Apple also was adding an optional integration layer with ChatGPT. So, yes, given that the news media gave credit for Apple’s own actual announced achievements to OpenAI, Apple surely would have been given even less credit had they not announced the “more personalized Siri” features. It’s easy to imagine someone in the executive ranks arguing “We need to show something that only Apple can do.” But it turns out they announced something Apple couldn’t do. And now they look so out of their depth, so in over their heads, that not only are they years behind the state-of-the-art in AI, but they don’t even know what they can ship or when. Their headline features from nine months ago not only haven’t shipped but still haven’t even been demonstrated, which I, for one, now presume means they can’t be demonstrated because they don’t work. Gruber — my co-host on Dithering — has been writing and podcasting about Apple for over two decades; his podcast is called The Talk Show , and for the last ten years Apple executives have appeared for a live version of his show the week of WWDC. However, this year will be different; Gruber announced on Daring Fireball : 1 Ever since I started doing these live shows from WWDC, I’ve kept the guest(s) secret, until showtime. I’m still doing that this year. But in recent years the guests have seemed a bit predictable: senior executives from Apple. This year I again extended my usual invitation to Apple, but, for the first time since 2015, they declined. I think this will make for a fascinating show, but I want to set everyone’s expectations accordingly. I’m invigorated by this. See you at the show, I hope. Neither Gruber nor (obviously) Apple said why the invitation was declined, but it was hard to not draw a line to Gruber’s viral article; Marco Arment said Apple was Retreating to Safety : Maybe Apple has good reasons. Maybe not . We’ll see what their WWDC PR strategy looks like in a couple of weeks. In the absence of any other information, it’s easy to assume that Apple no longer wants its executives to be interviewed in a human, unscripted, unedited context that may contain hard questions, and that Apple no longer feels it necessary to show their appreciation to our community and developers in this way. I hope that’s either not the case, or it doesn’t stay the case for long. This will be the first WWDC I’m not attending since 2009 (excluding the remote 2020 one, of course). Given my realizations about my relationship with Apple and how they view developers , I’ve decided that it’s best for me to take a break this year, gain some perspective, and decide what my future relationship should look like. Maybe Apple’s leaders are doing that, too. My biggest takeaway from WWDC is that Arment got it right: Apple is indeed “Retreating to Safety”. Retreating might, however, be exactly the right thing to do. Apple’s Retreat to Its Core Competency The headline feature of WWDC this year was Liquid Glass , a new unified design language that stretches across its operating systems. I will reserve judgment on Liquid Glass’s aesthetics and usability — Gruber likes it , but I am not one to install developer betas on my devices — but will make three meta observations. First, hand-crafted UI overhauls are the polar opposite of the probabilistic world of generative AI. One is about deep consideration and iteration resulting in a finished product; the other is about in-the-moment token prediction resulting in an output that is ephemeral and disposable. Both are important and creative, but the downsides of that creativity — unfamiliarity and edge cases versus hallucination and false confidence — are themselves diametrically opposed. Apple’s historical strengths have always been rooted in designing for finality; in my first year of Stratechery I did a SWOT analysis of the big tech companies and said about Apple : Apple’s product development process is wonderful for developing finished products, but that same process doesn’t work nearly as well when it comes to building cloud services. Cloud services are never “done”; they are best developed by starting with a minimum viable product and then iterating based on usage. This is precisely opposite of what it takes to design a piece of hardware, and it’s a big reason why Apple struggles so much with cloud services (and why other services companies struggle with products). The canonical example of this, of course, was the MobileMe launch, which was delivered fully-formed and which, when faced with real world usage, crashed-and-burned. Apple’s latest offerings are better, but still suffer from too much internal development time per release. This is a hard problem to fix, because it touches the core of what makes Apple Apple. I think it matters whether or not Liquid Glass is good, because it will be a testament about the state of Apple’s strengths; the point for this Article, however, is that WWDC was a retreat to those strengths, away from a technology that is very much inline with Apple’s historical weaknesses. 2 Second, the core premise of the Liquid Glass re-design is leveraging Apple’s integration of hardware and software. This is how Vice President of Human Interface Design Alan Dye introduced Apple’s new design language: Now with the powerful advances in our hardware, silicon, and graphics technologies, we have the opportunity to lay the foundation for the next chapter of our software. Today, we’re excited to announce our broadest design update ever. Our goal is a beautiful new design that brings joy and delight to every user experience, one that’s more personal and puts greater focus on your content. All while still feeling instantly familiar. And for the first time, we’re introducing a universal design across our platforms. This unified design language creates a more harmonious experience as he move between products, while maintaining the qualities that make each unique. Inspired by the physicality and richness of visionOS, we challenged ourselves to make make something purely digital, feel natural and alive, from how it looks to how it feels as it dynamically responds to touch. To achieve this, we began by rethinking the fundamental elements that make up our software, and it starts with an entirely new expressive material we call Liquid Glass. With the optical qualities of glass, and a fluidity that only Apple can achieve, it transforms depending on your content, or even your context, and brings more clarity to navigation and controls. It beautifully refracts light, and dynamically reacts to your movement with specular highlights. This material brings a new level of vitality to every aspect of your experience, from the smallest elements you interact with, to larger ones, it responds in real time to your content, and your input, creating a more lively experience that we think you’ll find truly delightful. Elements once considered for rectangular displays have been redesigned to fit perfectly concentric with the rounded corners of the hardware. This establishes greater harmony between our software and hardware, while thoughtfully considered groups of controls, free up valuable space for your content. Liquid Glass is translucent and behaves just like glass in the real world. Its color is informed by your content and intelligently adapts between light and dark environments, and as a distinct functional layer that sits above your app, the material dynamically morphs when you need more options, or as you move between views. We’ve always cared deeply about every detail of our software design, and it’s these moments of beauty, craft, and joy that bring our products to life. Our new design blurs the lines between hardware and software to create an experience that’s more more delightful than ever, while still familiar and easy to use. Today marks an exciting and beautiful new chapter for our design, one that sets the stage for our next era of our products and how you interact with them every day. Sebastiaan de With, in a remarkably prescient post predicting the nature of this new design, emphasized how only Apple could make Liquid Glass: A logical next step could be extending physicality to the entirety of the interface. We do not have to go overboard in such treatments, but we can now have the interface inhabit a sense of tactile realism. Philosophically, if I was Apple, I’d describe this as finally having an interface that matches the beautiful material properties of its devices. All the surfaces of your devices have glass screens. This brings an interface of a matching material, giving the user a feeling of the glass itself coming alive… The interfaces of computers of the future are often surprisingly easy to imagine. We often think of them and feature them in fiction ahead of their existence: our iPhone resembles a modern Star Trek tricorder; many modern AI applications resemble the devices in sci-fi movies like ‘Her’ and (depressingly) Blade Runner 2049. It’s not surprising, then, that concept interfaces from the likes of Microsoft often feature ‘glass fiction’: The actual interface is unfortunately not nearly as inspired with such life and behavioral qualities. The reason is simple: not only is the cool living glass of the video way over the top in some places, but few companies can actually dedicate significant effort towards creating a hardware-to-software integrated rendering pipeline to enable such UI innovations…Only Apple could integrate sub pixel antialiasing and never-interrupted animations on a hardware level to enable the Dynamic Island and gestural multi-tasking; only Apple can integrate two operating systems on two chips on Vision Pro so they can composite the dynamic materials of the VisionOS UI. And, perhaps only Apple can push the state of the art to a new interface that brings the glass of your screen to life. de With’s prescience actually gives me great hope for Liquid Glass: the best innovations are obvious to those who understand what is just becoming possible, and Apple’s integration has and continues to be a meaningful advantage for things like user interfaces. Third, Apple CEO Tim Cook has for a long time extended his framing of Apple’s differentiation to be the integration of hardware, software, and services , but while that is certainly true from a financial perspective , I’ve long had a hard time buying that the services component made for better products; as I noted above, Apple’s services have typically been something to be endured, as opposed to a reason to buy their devices, and the Siri debacle has only emphasized that point. What is much more compelling — and the fact that Liquid Glass is a design language meant to unify Apple’s devices speaks to this — is the integration of Apple’s devices with each other. Every Apple product you buy is enhanced by the purchase of additional Apple products; to that end, one of the coolest parts of the WWDC presentation was about Continuity, Apple’s brand for features that connect various Apple products: Let’s talk about the power of continuity. Continuity helps you work seamlessly across Apple devices, and we’re excited to introduce two new Continuity features. First, we’re bringing Live Activities to Mac. So if you’ve ordered Uber Eats on your iPhone, the Live Activity also appears in the menu bar, and when you click, the app opens in iPhone-mirroring, so you can take action directly on your Mac. We’re also enhancing the calling experience by bringing the Phone app to Mac. You can conveniently access your familiar content, like recents, contacts, and voicemail, synced from iPhone, and easily make a call with just a click. Incoming calls look beautiful on the bigger screen, featuring stunning contact posters of your friends and family, and the Phone app on Mac includes all the great updates we talked about earlier, like hold assist, call screening, and live translation. So that’s what’s new in Continuity. These sorts of features aren’t going to change the world; they are, though, features that I can absolutely see making my life better and more convenient on an ongoing basis. And, to the broader point, they are features that only Apple can do. More generally, yes, a focus on UI design is a retreat from The Gen AI Bridge to the Future ; that future, however, will start from the devices we still use all day every day, and Apple focusing on making those devices better is a retreat that I expect will have a far more positive impact on my life than the company struggling to catch up in AI. Apple’s Retreat to Empowering Developers and Partners That’s not to say there weren’t some notable AI announcements in Apple’s keynote. First, Apple announced the Foundation Models framework: This year we’re doing something new, and we think it’s going to be pretty big. We’re opening up access for any app to tap directly into the on-device, large language model at the core of Apple Intelligence, with a new Foundation Models Framework. This gives developers direct access to intelligence that’s powerful, fast, built with privacy, and available even when you’re offline. We think this will ignite a whole new wave of intelligent experiences in the apps you use every day. For example, if you’re getting ready for an exam, an app like Kahoot can create a personalized quiz from your notes to make studying more engaging, and because it uses on-device models, this happens without Cloud API costs. Or perhaps you’re camping off-grid, poring over the hikes you downloaded to AllTrails. Just describe what you’re in the mood for, and AllTrails can use our on-device models to suggest the best option. We couldn’t be more excited about how developers can build on Apple Intelligence to bring you new experiences that are smart, available when you’re offline, and that protect your privacy. It’s important not to oversell the capabilities of Apple’s on-device AI models: of course developers who want to create something that is competitive with the output of something like ChatGPT will need to use cloud-based AI APIs. That reality, however, applies to Apple as well! Part of the folly of the initial Apple Intelligence approach is that Apple was promising to deliver beyond state-of-the-art capabilities on the cheap, using its users’ processors and power. What is compelling about the Foundation Models Framework is how it empowers small developers to experiment with on-device AI for free: an app that wouldn’t have AI at all for cost reasons now can, and if that output isn’t competitive with cloud AI then that’s the developer’s problem, not Apple’s; at the same time, by enabling developers to experiment Apple is the big beneficiary of those that discover how to do something that is only possible if you have an Apple device. Second, Apple deepened its reliance on OpenAI, incorporating ChatGPT’s image generation capabilities into Image Playground and adding ChatGPT analysis to Visual Intelligence. There is still no sign of the long-rumored Gemini integration or the ability to switch out ChatGPT for the AI provider of your choice, but the general trend towards relying on partners who are actually good at building AI is a smart move. Third, Apple is also incorporating ChatGPT much more deeply into Xcode, its Integrated Development Environment (IDE) for building apps for Apple platforms; developers can also plugin other models using API keys. Xcode still has a long ways to go to catch up to AI-first IDEs like Cursor , but again, partnering with foundational model makers is a far smarter strategy than Apple trying to do everything itself. These are, to be sure, obvious moves , but that doesn’t make them any less important, both in terms of Apple’s future, and also with regard to the theme of this Article: Apple’s initial success with the Apple II was because of 3rd-party developers, and developers were critical to making the iPhone a sustainable success. Trusting developers and relying on partners may be a retreat from Apple’s increasing insistence on doing everything itself, but it is very much a welcome one. Apple’s [Forced] Retreat to App Store Sanity Apple didn’t say much about the App Store in the keynote, but they did announce a new Games app; M.G. Siegler theorized late last month that this may be laying the groundwork for splitting up Games from the rest of the App Store: What if this new gaming-focused app – let’s just call it ‘Game Store’ – is not only meant to unify Apple’s gaming-focused efforts, but also to separate them from the App Store itself? Why might Apple do this? Because this would allow them to more easily differentiate between the two and, importantly, give the two independent policies. That means that Apple could, say, drop the rate developers have to pay when it comes to revenue share in the App Store, while keeping it the same as it is now in the ‘Game Store’. And that matters because actually, gaming makes up some two-thirds of Apple’s App Store revenue at the moment (between paid downloads and in-app purchases – but it’s predominantly the latter). It’s the actual key to Apple’s model for this segment of the Services business. And guess what else is true? In gaming, a 70/30 split is a well-established norm. In fact, it’s where Apple’s own App Store split originates from (by way of iTunes, which also copied the model back in the day)! Yes, there are others who have tried to disrupt this split, notably Epic, but Apple has a much stronger case for a 70/30 split when it comes to gaming than it now does for the overall app ecosystem. So hear me out: the ‘Game Store’ keeps the 70/30 model and the ‘App Store’ moves to something more like 85/15 as the standard (matching Apple’s currently convoluted system for small developers with various arbitrary thresholds). Perhaps for smaller developers, Apple even drops it to 90/10. Apple did not announce such a shift yesterday, but the infrastructure is now in place to do exactly what I have advocated for years: treat games differently from other apps . Gaming revenue is almost entirely based on zero marginal cost content, and games are more susceptible to abuse and more likely to be used by kids; I don’t mind Apple’s more heavy-handed approach in that case and, as Siegler notes, this level of control is the industry standard for other gaming platforms like consoles. In other words, Apple should retreat from trying to take a cut of everything digital , and instead act like a console maker where appropriate, and a more neutral computer platform for everything else. Unfortunately for Apple, keeping console-level control of games may no longer be possible, particularly after the Ninth Circuit Court of Appeals denied Apple’s request for a stay of Judge Yvonne Gonzalez Roger’s order lifting anti-steering restrictions on all apps, including games. The functional outcome of Gonzalez Rogers’ order is a retreat by Apple from its overbearing control of app monetization, albeit not one Apple is engaged in willingly. Once again, however, a retreat is exactly what Apple needs. The company has gone too far with the App Store, not only embittering developers and losing court cases, but also has put its fundamental differentiation at risk. I warned the company of exactly this in 2021’s Integrated Apple and App Store Risk : This is where the nuance I discussed in App Store Arguments becomes much more black-and-white. Yes, Apple created the iPhone and the App Store and, under current U.S. antitrust doctrine, almost certainly has the right to impose whatever taxes it wishes on third parties, including 30% on purchases and the first year of subscriptions, and completely cutting off developers from their customers. Antitrust law, though, while governed by Supreme Court precedent, is not a matter of constitutionality: it stems from laws passed by Congress, and it can be changed by new laws passed by Congress. One of the central planks of many of those pushing for new laws in this area are significant limitations on the ability of platforms to offer apps and services, or integrate them in any way that advantages their offerings. In this potential world it’s not simply problematic that Apple charges Spotify 30%, or else forces the music streaming service to hope that users figure out how to",
      "summary": "29-minute read",
      "url": "https://stratechery.com/2025/apple-retreats/?utm_source=tldrnewsletter",
      "published_date": "2025-06-11T00:00:00",
      "category": "ai",
      "word_count": 3752,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 3752/5800 words (64.7%)"
    },
    {
      "title": "Apple WWDC 2025: the 13 biggest announcements",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.theverge.com/news/682769/apple-wwdc-2025-biggest-announcements-ios-26?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "OpenAI hits $10 billion in annual recurring revenue fueled by ChatGPT growth",
      "content": "Key Points OpenAI has hit $10 billion in annual recurring revenue, according to a company spokesperson. The milestone comes roughly two and a half years after the company launched its popular ChatGPT chatbot. The figure includes revenue from the company's consumer products, ChatGPT business products and its API, an OpenAI spokesperson says. OpenAI CEO Sam Altman appears on screen during a talk with Microsoft Chair and CEO Satya Nadella at the Microsoft Build 2025 conference in Seattle on May 19, 2025. Jason Redmond | AFP | Getty Images OpenAI has hit $10 billion in annual recurring revenue, or ARR, less than three years after launching its popular ChatGPT chatbot. The figure includes sales from the company's consumer products; ChatGPT business products; and its application programming interface, or API. It excludes licensing revenue from Microsoft and large one-time deals, according to an OpenAI spokesperson. For all of last year, OpenAI had around $5.5 billion in ARR. Reaching its meteoric growth rates requires a substantial amount of cash. The San Francisco-based startup lost about $5 billion last year. OpenAI is also targeting $125 billion in revenue by 2029, according to a person familiar with the matter who asked not to be named because the details are confidential. The Information first reported on OpenAI's revenue ambitions. The new revenue metrics give some context to OpenAI's monster valuation. OpenAI closed a $40 billion funding round in March, marking the largest private tech deal on record. At today's metrics, OpenAI is valued at about 30 times revenue, which highlights the hyper growth expectations by some of its largest investors. OpenAI is backed by Japan's SoftBank , Microsoft , Coatue, Altimeter, Thrive and others. OpenAI burst onto the scene with the release of the consumer version of ChatGPT in late 2022, and began launching business products the following year. As of late March, OpenAI said it supports 500 million weekly active users. The company announced earlier this month that it has three million paying business users, up from the two million it reported in February. WATCH: OpenAI paying business users of ChatGPT hits 3 million, up from 2 million in February watch now VIDEO 1:59 01:59 OpenAI paying business users of ChatGPT hits 3 million, up from 2 million in February The Exchange Stargate partner Crusoe lands $750 million credit line for AI buildout Jordan Novet FBI says Palm Springs bombing suspects used AI chat program to help plan attack Annie Palmer Nvidia CEO Jensen Huang hammers chip controls that 'effectively closed' China market Kif Leswing Read More",
      "summary": "2-minute read",
      "url": "https://www.cnbc.com/2025/06/09/openai-hits-10-billion-in-annualized-revenue-fueled-by-chatgpt-growth.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 422,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Doctors Could Hack the Nervous System With Ultrasound",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
      "summary": "12-minute read",
      "url": "https://spectrum.ieee.org/focused-ultrasound-stimulation-inflammation-diabetes?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "XRobotics' countertop robots are cooking up 25,000 pizzas a month",
      "content": "XRobotics thinks it has cracked the code on getting pizza restaurants to adopt robotics. The San Francisco-based robotics company built a countertop robot called xPizza Cube, which is roughly the size of a stackable washing machine and uses machine learning to apply sauce, cheese, and pepperoni to pizza dough. The machines, which lease for $1,300 a month for three years, can make up to 100 pizzas an hour and be retrofitted to work with pies of different sizes and styles, like Detroit and Chicago deep dish. “This saves like almost 70, sometimes 80% of the time for the staff,” Denis Rodionov, the co-founder and CEO of XRobotics, told TechCrunch. “It is just repeatable work. If you have a pepperoni pizza, you need to place 50 slices of pepperoni one by one.” XRobotics is not the only company that has tried to introduce robotics into the restaurant industry — nor the only one focused on pizza. Zume is the most notable pizza robotics company — if that can be considered its own category. The company raised more than $420 million in venture capital for its robotic pizza trucks, before pivoting to focus on sustainable packaging in 2020 and shuttering entirely in 2023. Rodionov argues that they’ve been successful where other companies haven’t because they aren’t trying to fully transform the pizza-making process, as Zume was, but rather build technology to help existing pizza makers save on time and labor. Because they are building assistive technology, as opposed to replacement tech, Rodionov said they’ve been able to keep their device small enough to fit in existing kitchens and priced at a level that pizzerias — from mom-and-pop shops to large chains, both of which the company counts as customers — could afford. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The company found this out the hard way. XRobotics launched in 2019 and introduced the first version of the technology in 2021 . Its first robot was significantly larger and could work with more than 20 toppings, and ran into the same problems as their competitors. “We did a real pilot in the restaurant with our huge machine,” Rodionov said. “We learned a lot from that, and we figured out we needed a very small, compact solution. It was a bit scary. All the numbers, all the feelings, all the gut said you need to do this, not this . And we just followed the gut and said, ‘Yeah, we would go and make a smaller version,’ and it was tremendous success.” XRobotics launched their current model in 2023. The company declined to share how many customers it has. It said its robots are producing 25,000 pizzas per month, but how many customers that translates to is hard to calculate. The startup also recently raised a $2.5 million seed round led by FinSight Ventures with participation from SOSV, MANA Ventures, and Republic Capital. Rodionov said the company will use the capital to produce more units and install more robots for customers. XRobotics is committed to the pizza industry, at least for now, Rodionov said, considering the sheer size of the market — there are more than 73,000 pizza chains in the U.S. alone . The company plans to expand to Mexico and Canada next. “I love pizza, my co-founder too,” Rodionov said. “We have tested probably any pizza in San Francisco. Also, we test pizza in New York and Chicago.” Rodionov added that Detroit-style pizza, known for its square shape and crispy cheese crust, is his favorite.",
      "summary": "4-minute read",
      "url": "https://techcrunch.com/2025/06/09/xrobotics-countertop-robots-are-cooking-up-25000-pizzas-a-month/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 658,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple Announces Foundation Models Framework for Developers to Leverage AI",
      "content": "Apple Announces Foundation Models Framework for Developers to Leverage AI Monday June 9, 2025 10:10 am PDT by Tim Hardwick Apple at WWDC today announced Foundation Models Framework, a new API allowing third-party developers to leverage the large language models at the heart of Apple Intelligence and build it into their apps. With the Foundation Models Framework, developers can integrate Apple's on-device models directly into apps, allowing them to build on Apple Intelligence. \"Last year, we took the first steps on a journey to bring users intelligence that's helpful, relevant, easy to use, and right where users need it, all while protecting their privacy. Now, the models that power Apple Intelligence are becoming more capable and efficient, and we're integrating features in even more places across each of our operating systems,\" said Craig Federighi, Apple's senior vice president of Software Engineering. \"We're also taking the huge step of giving developers direct access to the on-device foundation model powering Apple Intelligence, allowing them to tap into intelligence that is powerful, fast, built with privacy, and available even when users are offline. We think this will ignite a whole new wave of intelligent experiences in the apps users rely on every day. We can't wait to see what developers create.\" The Foundation Models framework lets developers build AI-powered features that work offline, protect privacy, and incur no inference costs. For example, an education app can generate quizzes from user notes on-device, and an outdoors app can offer offline natural language search. Apple says the framework is available for testing starting today through the Apple Developer Program at developer.apple.com, and a public beta will be available through the Apple Beta Software Program next month at beta.apple.com. It includes built-in features like guided generation and tool calling for easy integration of generative capabilities into existing apps. Tag: WWDC 2025 [ 8 comments ] Get weekly top MacRumors stories in your inbox. Popular Stories Monday June 9, 2025 9:00 am PDT by Jordan Golson Apple's Worldwide Developers Conference (WWDC) starts today with the traditional keynote kicking things off at 10:00 a.m. Pacific Time. MacRumors is on hand for the event and we'll be sharing details and our thoughts throughout the day. We're expecting to see a number of software-related announcements led by a design revamp across Apple's platforms that will also see the numbering of all of... • 1212 comments Monday June 9, 2025 5:21 pm PDT by Juli Clover At today's WWDC 2025 keynote event, Apple unveiled a new design that will inform the next decade of iOS, iPadOS, and macOS development, so needless to say, it was a busy day. Apple also unveiled a ton of new features for the iPhone, an overhauled Spotlight interface for the Mac, and a ton of updates that make the iPad more like a Mac than ever before.",
      "summary": "2-minute read",
      "url": "https://www.macrumors.com/2025/06/09/foundation-models-framework/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 471,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "LLMs are cheap",
      "content": "LLMs are cheap Posted on 2025-06-02 in General This post is making a point - generative AI is relatively cheap - that might seem so obvious it doesn't need making. I'm mostly writing it because I've repeatedly had the same discussion in the past six months where people claim the opposite. Not only is the misconception still around, but it's not even getting less frequent. This is mainly written to have a document I can point people at, the next time it repeats. It seems to be a common, if not a majority, belief that Large Language Models (in the colloquial sense of \"things that are like ChatGPT\") are very expensive to operate. This then leads to a ton of innumerate analyses about how AI companies must be obviously doomed, as well as a myopic view on how consumer AI businesses can/will be monetized. It's an understandable mistake, since inference was indeed very expensive at the start of the AI boom, and those costs were talked about a lot. But inference has gotten cheaper even faster than models have gotten better, and nobody has an intuition for something becoming 1000x cheaper in two years. It just doesn't happen. It doesn't help that the common pricing model (\"$ per million tokens\") is very hard to visualize. So let's compare LLMs to web search. I'm choosing search as the comparison since it's in the same vicinity and since it's something everyone uses and nobody pays for, not because I'm suggesting that ungrounded generative AI is a good substitute for search. (It should also go without saying that these are just my personal opinions.) What is the price of a web search? Here's the public API pricing for some companies operating their own web search infrastructure, retrieved on 2025-05-02: The Gemini API pricing lists a \"Grounding with Google Search\" feature at $35/1k queries. I believe that's the best number we can get for Google, they don't publish prices for a \"raw\" search result API. The Bing Search API is priced at $15/1k queries at the cheapest tier. Brave has a price of $5/1k searches at the cheapest tier. Though there's something very strange about their pricing structure, with the unit pricing increasing as the quota increases, which is the opposite of what you'd expect. The tier with real quota is priced at $9/1k searches. So there's a range of prices, but not a horribly wide one, and with the engines you'd expect to be of higher quality also having higher prices. What is the price of LLMs in a similar domain? To make a reasonable comparison between those search prices and LLM prices, we need two numbers: How many tokens are output per query? What's the price per token? I picked a few arbitrary queries from my search history, and phrased them as questions, and ran them on Gemini 2.5 Flash (thinking mode off) in AI Studio: [When was the term LLM first used?] -> 361 tokens, 2.5 seconds [What are the top javascript game engines?] -> 1145 tokens, 7.6 seconds [What are the typical carry-on bag size limits in europe?] -> 506 tokens, 3.4 seconds [List the 10 largest power outages in history] -> 583 tokens, 3.7 seconds Note that I'm not judging the quality of the answers here. The purpose is just to get rough numbers for how large typical responses are. A 500-1000 token range seems like a reasonable estimate. What's the price of a token? The pricing is sometimes different for input and output tokens. Input tokens tend to be cheaper, and our inputs are very short compared to the outputs, so for simplicity let's consider all the tokens to be outputs. Here's the pricing of some relevant models, retrieved on 2025-05-02: Model Price / 1M tokens Gemma 3 27B $0.20 ( source ) Qwen3 30B A3B $0.30 ( source ) Gemini 2.0 Flash $0.40 ( source ) GPT-4.1 nano $0.40 ( source ) Gemini 2.5 Flash Preview $0.60 ( source ) Deepseek V3 $1.10 ( source ) GPT-4.1 mini $1.60 ( source ) Deepseek R1 $2.19 ( source ) Claude 3.5 Haiku $4.00 ( source ) GPT-4.1 $8.00 ( source ) Gemini 2.5 Pro Preview $10.00 ( source ) Claude 3.7 Sonnet $15.00 ( source ) o3 $40.00 ( source ) If we assume the average query uses 1k tokens, these prices would be directly comparable to the prices per 1k search queries. That's convenient. The low end of that spectrum is at least an order of magnitude cheaper than even the cheapest search API, and even the models at the low end are pretty capable. The high end is about on par with the highest end of search pricing. To compare a midrange pair on quality, the Bing Search vs. a Gemini 2.5 Flash comparison shows the LLM being 1/25th the price. Note that many of the above models have cheaper pricing in exchange for more flexible scheduling (Anthropic, Google and OpenAI give a 50% discount for batch requests, Deepseek is 50%-75% cheaper during off-peak hours). I've not included those cheaper options in the table to keep things comparable, but the presence of those cheaper tiers is worth keeping in mind when thinking about the next section... Objection! I know some people are going to have objections to this back-of-the-envelope calculation, and a lot of them will be totally legit concerns. I'll try to address some of them preemptively. Slightly different assumptions can easily lead to clawing back 10% here and 50% there. But I don't see how to bridge a 25x gap just for breaking even, let alone making the AI significantly more expensive. If you want to play around with different assumptions, there's a little calculator widget below. Surely the typical LLM response is longer than that - I already picked the upper end of what the (very light) testing suggested as a reasonable range for the type of question that I'd use web search for. There's a lot of use cases where the inputs and outputs are going to be much longer (e.g. coding), but then you'd need to also switch the comparison to something in that same domain as well. The LLM API prices must be subsidized to grab market share -- i.e. the prices might be low, but the costs are high - I don't think they are, for a few reasons. I'd instead assume APIs are typically profitable on a unit basis. I have not found any credible analysis suggesting otherwise. First, there's not that much motive to gain API market share with unsustainably cheap prices. Any gains would be temporary, since there's no long-term lock-in, and better models are released weekly. Data from paid API queries will also typically not be used for training or tuning the models, so getting access to more data wouldn't explain it. Note that it's not just that you'd be losing money on each of these queries for no benefit, you're losing the compute that could be spent on training, research, or more useful types of inference. Second, some of those models have been released with open weights and API access is also available from third-party providers who would have no motive to subsidize inference. (Or the number in the table isn't even first party hosting -- I sure can't figure out what the Vertex AI pricing for Gemma 3 is). The pricing of those third-party hosted APIs appears competitive with first-party hosted APIs. For example, the Artificial Analysis summary on Deepseek R1 hosting . Third, Deepseek released actual numbers on their inference efficiency in February. Those numbers suggest that their normal R1 API pricing has about 80% margins when considering the GPU costs, though not any other serving costs. Fourth, there are a bunch of first-principles analyses on the cost structure of models with various architectures should be. Those are of course mathematical models, but those costs line up pretty well with the observed end-user pricing of models whose architecture is known. See the references section for links. The search API prices amortize building and updating the search index, LLM inference is based on just the cost of inference - This seems pretty likely to be true, actually? But the effect can't really be that large for a popular model: e.g. the allegedly leaked OpenAI financials claimed $2/year spent on inference vs. $3B/year on training. Given the crazy growth of inference volumes (e.g. Google recently claimed a 50x increase in token volumes in the last year ) the training costs are getting amortized much more effectively. The search API prices must have higher margins than LLM inference - It's possible. I certainly don't know what the margins of any Search API providers are, though it seems fair to assume they're pretty robust. But, well, see the point above about Deepseek's releasd numbers on the R1 profit margins. Also, it seems quite plausible that some Search providers would accept lower margins, since at least Microsoft execs have testified under oath that they'd be willing to pay more for the iOS query stream than their revenue, just to get more usage data. Web search returns results 20x-100x faster than an LLM finishes the query, how could it be more expensive? - Search latency can be improved by parallelizing the problem, while LLM inference is (for now) serial in nature. The task of predicting a single token can be parallelized, but the you can't predict all the output tokens at once. But OpenAI made a loss, and they don't expect to make profit for years! - That's because a huge proportion of their usage is not monetized at all, despite the usage pattern being ideal for it. OpenAI reportedly made a loss of $5B in 2024. They also reportedly have 500M MAUs. To reach break-even, they'd just need to monetize (e.g. with ads) those free users for an average of $10/year, or $1/month. A $1 ARPU for a service like this would be pitifully low. If the reported numbers are true, OpenAI doesn't actually have high costs for a consumer service that popular, which is what you'd expect to see if the high cost of inference was the problem. They just have a very low per-user revenue, by choice. Sensitivity analysis If you want to play around with different assumptions, here's a calculator: Open in new tab Why does this matter? I mean, you're right to ask that. Nothing really matters and eventually we'll all be dead. But it is interesting how many people have built their mental model for the near future on a premise that was true for only a brief moment. Some things that will come as a surprise to them even assuming all progress stops right now: There's an argument advanced by some people about how low prices mean it'll be impossible for AI companies to ever recoup model training costs. The thinking seems to be that it's just the prices that have been going down, but not the costs, and the low prices must be an unprofitable race to the bottom for what little demand there is. What's happening and will continue to happen instead is that as costs go down, the prices go down too, and demand increases as new uses become viable. For an example, look at the OpenRouter API traffic volumes , both in aggregate and in the relative share of cheaper models. This post was mainly about APIs, but consumer usage will have exactly the same cost structure, just a different monetization structure. And given how low the unit costs must be, advertising isn't merely viable but lucrative. From this it follows that the financials of frontier AI labs are a lot better than some innumerate pundits would have you believe. They're making a loss because they're not under pressure to be profitable, and aren't actively trying to monetize consumer traffic yet. This could well be a land grab unlike APIs, since unpaid consumer queries may be used for training while paid API queries typically are not. Even the subscription pricing might be there mainly for demand management rather than trying to run a profit. The real cost problem isn't going to be with the LLMs themselves, it's with all the backend services that AI agents will want to access if even a rudimentary form of the agentic vision actually materializes. Running the AI is already cheap, will keep getting cheaper, and will always have a monetization model of some sort since it's what the end user is interacting with. Neither of those is true for the end-user services that have been turned into AI backends without their consent. An AI trying to, I don't know, book concert tickets whenever a band I like plays in my town will probably be phenomenally expensive to its third-party backends (e.g. scraping ticket sites). Those sites will be uncompensated for the expense while also removing their actual revenue streams. I don't really know how that plays out. Obviously many service owners will try to make unauthorized scraping harder, but that's a very hard problem to solve on the web. Maybe some of them give up on the web entirely, and move to mobile where they can at least get device attestations. Some might just give up on the open web, and require all usage to be signed in, with account creation being gated on something scarce. Some might become unviable and close up shop entirely. If/when that happens, what's the play on the AI agent side? Will they choose an escalating adversarial arms race with increasingly dodgy tactics, or will they eventually decide that it's better to pay for the services they use? The former seems unsustainable. If the latter, then it feels like the core engineering challenge becomes one of building data provider backends optimized specifically for AI use, with the goal of scaling to massive volumes and cheaper unit prices, with the trade-off being higher latency, lower reliability and lower quality. That could be quite interesting from a systems perspective. (Yes, I'm aware of MCP , but it's a solution to an orthogonal issue.) But one thing I'm confident won't be happening is that it's the AIs that turn out to be too expensive to run. Additional reading Below are some additional references that were not worked into the main narrative (this article was long-winded enough already). Inference economics of language models (2025) - A mathematical model for estimating the cost structure, latency/cost tradeoffs, optimal cluster size, and optimal batching based on the LLM architecture. LLM Inference Economics from First Principles - (2025) A very detailed cost-per-token computation on the cost structure of one specific model, LLama 3.3 70B. Observations About LLM Inference Pricing - (2025) Analysis of the economics driven by pricing data rather than first-principles cost structure; concludes that proprietary models have very significant markups. Large Language Models Search Architecture And Cost - (2023) Analysis on the cost of integrating LLMs into search; the LLM cost data is no longer very relevant due to the age of the article (GPT-3.5) but it uses a different way of estimating the search cost structure. If you liked this and want to be notified of new posts, follow me on",
      "summary": "16-minute read",
      "url": "https://www.snellman.net/blog/archive/2025-06-02-llms-are-cheap/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 2530,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "One Man Armies",
      "content": "One Man Armies In an overdecorated office somewhere in California, there is someone being paid so much money they could take six months of their salary, move to Brazil, and never work another day in their life — and yet this person is getting a remarkably little amount of work done every day. Just down the street, in a similar-looking office, there is a similar-looking person who is producing astonishing work at a breakneck pace. The variation in human output is kind of insane. What some may consider impossible without a team and $5M can sometimes be built by a single person who cares a lot and works hard. It’s easy to understimate just how much a one person army can achieve. So here are some of our favorite one-person (or mostly one-person) projects. We’re not including things that you wouldn't be surprised one person made, like a music album or a novel, though those are also very impressive. Stardew Valley was made by Eric Barone , who put in 70 hours a week for 4.5 years before releasing the game for the first time. He still updates the game with loads of new content. Linux was created by Linus Torvalds . He is not responsible for all of Linux and did not do everything single handedly, but he was its origin (which was a lot of work). Minecraft, before it sold to Microsoft, was developed by Markus Persson . It took him 2 years! The Art of Computer Programming is a massive tome of a lot of knowledge about computer programming and algorithms and math. It’s used by all sorts of people (from scientists to mathematicians to linguists) and it is not yet done. The one person behind this book is Donald Knuth . The Sagrada Familia was designed by Antonio Gaudí , who spent more than 40 years of his life on it. The first scientific map of the Atlantic Ocean floor, which discovered the rift valley in the Mid-Atlantic ridge and was foundational evidence for the plate tectonics theory, was created solo by Marie Tharp. Bruce Heezen, a colleague, collected the raw data as women were not allowed to work on ships at the time. The Coral Castle, a kind of weird-but-cool garden of structures in Florida, was built singlehandedly by a Latvian immigrant named Edward Leedskalnin . When asked how he moved such heavy rocks on his own for his structures, he said he knew the secrets of those who built the Pyramids. The first version of a computer program was written in the 1800s by Ada Lovelace . The Cathedral of Justo in Spain was designed and built almost exclusively by Justo Gallego Martinez . He worked on it for the better part of his life, all the way up until he died. Rollercoaster Tycoon, a game you should definitely play, was made by Chris Sawyer . Kenshi, a complex RPG game, was made in 12 years by Chris Hunt . The Theory of General Relativity, as you probably know, was created by this guy . jQuery was made by one person: John Resig . Fun fact, he works at Khan Academy as of this writing. Christianity, though it later spread in many other ways, was pioneered by Jesus of Nazareth . Islam, though it also later spread in many ways, was pioneered by Muhammad ibn Abdullah . Light is a series of seven operas, one for each day of the week. It takes nearly 30 hours to perform. It was composed over decades by Karlheinz Stockhausen . 24 Preludes, Op. 28 by Frédéric Chopin is a project that includes one prelude for each key. He wrote it entirely on his own. Many of these pieces were masterpieces and pushed the boundaries of piano at the time. Beethoven, Liszt, Rachmaninov, Mozart, Bartók, Bach, Scriabin, Debussy, and the list goes on — many of the most influential composers also put together most of their ambitious projects solo. You can take away whatever you want from this list. One takeaway is that it is extremely impressive. If the Theory of General Relativity, entire cathedrals, 29-hour operas, and billion-dollar video games can be made by a single person, your own grand and seemingly unrealistic ideas may not be too ambtious, either. And, if you care enough about them, they might be worth trying. It sounds a bit like a feel-good platitude—but it may also be true. If you are looking for a list of ideas to try, you could read this. * * * Enjoy these essays? Enter your email and we'll let you know when we post a new one. No spam, just essays. Or, if you have any feedback,",
      "summary": "4-minute read",
      "url": "https://quarter--mile.com/One-Man-Armies?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 783,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "The SignalFire State of Talent Report - 2025",
      "content": "’s talent gap widens: new grad hiring plummets, Texas loses startup talent, and Anthropic's 80% retention dominance. Over the last couple of years, we’ve seen companies rewrite the playbook for hiring AI and tech talent. In a landscape defined by fierce competition and rapid change, our latest tech talent report zeroes in on the bold moves, early trends, and strategic pivots reshaping how companies attract and retain top-tier talent in 2025 and beyond. At SignalFire, our Beacon AI platform tracks over 650 million professionals and 80 million organizations to give us a front-row seat to the talent transformations reshaping the industry. Our latest report reveals some sharp shifts: - Entry-level hiring is collapsing - A generational hiring shift is leaving new graduates behind - Elite AI labs are locking in top talent, and retaining them - Geographic power centers are evolving fast While headlines continue to highlight labor market shifts amidst political change and intense AI rivalry among tech giants, our data reveals a more nuanced story. This isn’t just about layoffs or remote work. It’s about a fundamental reset in how, where, and who companies are hiring to build the next generation of technology. #1 Tech's lost generation? New grad hiring drops 50% compared to pre-pandemic levels The tech world has long been synonymous with innovation, breakneck growth, and boundless opportunities. The door to tech once swung wide open for new grads. Today, it’s barely cracked. The industry's obsession with hiring bright-eyed grads right out of college is colliding with new realities: smaller funding rounds, shrinking teams, fewer new grad programs, and the rise of AI. Everyone took a hit in 2023, but while hiring bounced back in 2024 for mid- and senior-level roles, the cut keeps getting deeper for new grads: Big Tech: New grads now account for just 7% of hires, with new hires down 25% from 2023 and over 50% from pre-pandemic levels in 2019. Startups: New grads make up under 6% of hires, with new hires down 11% from 2023 and over 30% from pre-pandemic levels in 2019. ‎As budgets tighten and AI capabilities increase, companies are reducing their investment in new grad opportunities. The latest data from the Federal Reserve Bank of New York shows that the unemployment rate for new college grads has risen 30% since bottoming out in September 2022, versus about 18% for all workers. The perception gap isn’t helping— 55% of employers say Gen Z struggles with teamwork, and 37% of managers said they’d rather use AI than hire a Gen Z employee. Even top computer science grads aren’t spared. As demand for junior roles declines, even highly credentialed engineering grads are struggling to break into tech, especially at the Big Tech companies. The share of new graduates landing roles at the Magnificent Seven (Alphabet, Amazon, Apple, Meta, Microsoft, NVIDIA and Tesla) has dropped by more than half since 2022. ‎The experience paradox It’s not just a hiring slowdown, it’s a shift in expectations. Today’s tech employers aren’t looking for potential, they’re looking for proof. That’s left new grads stuck in a Catch-22: you need experience to get the job, but you need the job to get experience. In a world of leaner teams and tighter budgets, there’s little room for investment in training. As the pipeline for new talent shrinks at breakneck speed, it’s creating fierce competition for the few entry-level roles that remain. The cruel irony? Companies are posting junior roles but filling them with senior individual contributors (ICs)—a phenomenon known as the experience paradox . Series A squeeze or AI hype: What’s really stalling Gen Z hiring? AI gets a lot of blame for wiping out junior roles post-2022, but the real story is more nuanced, as you can see from the 200+ comments on this Hacker News post and this recent article from The Atlantic . Yes, automation is already replacing some routine tasks , but the bigger driver may be the end of the “free money madness” driven by low interest rates that we saw in 2020-2022, along with the overhiring and inflation it led to. Now, with tighter budgets and shorter runways, companies are hiring leaner and later. Carta data shows that Series A tech startups are 20% smaller than they were in 2020. This shift isn’t just about hiring less—it’s a hiring reset. As AI tools take over more routine, entry-level tasks, companies are prioritizing roles that deliver high-leverage technical output. Big Tech is doubling down on machine learning and data engineering, while non-technical functions like recruiting, product, and sales keep shrinking, making it especially tough for Gen Z and early-career talent to break in. But despite the hype, AI hasn’t wiped out entire job categories—yet. So far, the fallout has hit new grads hardest, while demand for experienced engineers is still rising. As AI capabilities continue to grow, the real differentiator will be talent, and no one’s playing the game harder than the top AI labs . #2 AI Labs: Anthropic sets the pace in the talent race November 2022 didn’t just mark the launch of ChatGPT—it kicked off the AI talent race. We analyzed retention across top AI labs, and one clear leader emerged: Anthropic. An impressive 80% of employees hired at least 2 years ago at Anthropic were still at the company at the end of their second year—a figure that stands out in an industry known for high turnover. DeepMind follows closely at 78%, while OpenAI’s retention trails at 67% but remains on par with large FAANG companies like Meta (64%). With outsized salary packages at DeepMind and even Senate scrutiny over Big Tech talent poaching, the AI talent war is real, and the spoils go to those who can attract and keep the best minds. Anthropic’s winning formula and where they’re hiring from: Anthropic’s 80% retention isn’t just impressive—it’s a strategic advantage. In a field where turnover means lost time and money, keeping top talent consistently is a game-changer. And it’s not just about keeping talent, it’s about winning it. Talent poaching with precision: Anthropic is siphoning top talent from two of its biggest rivals: OpenAI and DeepMind. Engineers are 8 times more likely to leave OpenAI for Anthropic than the reverse. From DeepMind, the ratio is nearly 11:1 in Anthropic’s favor. Some of that’s expected—Anthropic is the hot new startup, while DeepMind’s larger, tenured team is ripe for movement. But the scale of the shift is striking. Beyond the salary: While Big Tech leans on big paychecks and brand cachet, Anthropic’s edge is its unique culture that embraces unconventional thinkers and gives employees true autonomy to drive impact. Flexible work options, no title politics or forced management tracks—just clear paths for career growth. Employees say Anthropic embraces intellectual discourse and researcher autonomy, making it a magnet for AI talent stifled by bureaucracy elsewhere. Claude is quickly emerging as a favorite among developers by several informal measures , and that kind of affinity can influence career decisions. Engineers often gravitate toward companies whose products they admire and use, and that perceived product resonance may be giving Anthropic an edge in recruiting. It’s not just rival AI labs feeling the pull— Big Tech has become a prime hunting ground, too . Google, Meta, Microsoft, Amazon, and Stripe serve as primary talent pools for AI labs, and Anthropic has been particularly successful at poaching senior researchers and engineers from these companies.‎ #3 Hot spots, cooling zones, and America’s new tech hubs The tech talent map continues to shift. While San Francisco and New York remain dominant, rising hubs like Toronto, Miami, and San Diego are gaining ground. Meanwhile, Texas cities like Austin are losing momentum, prompting founders to rethink hiring and compensation strategies. More companies are embracing hub-and-spoke models and tailoring compensation philosophies to ensure they secure the right talent mix across diverse locations. Key trends shaping the new geographic playbook Hiring is slowing in most metros, but we’re seeing a reconsolidation of tech jobs around major tech hubs like the Bay Area, Seattle, and NYC. Here are the key geographic trends we saw in 2024: 1. Silicon Valley and New York City hold strong: - High talent, high cost: SF and NYC still anchor the AI ecosystem, with over 65% of AI engineers based in these two metros. Despite rising housing costs, shrinking salary advantages, and remote work flexibility, SF and NY continue to attract more tech talent. 2. The sunshine surge - Miami and San Diego are rising fast: These two cities are attracting tech talent not with massive budgets, but with sun, lifestyle, and lower living costs. - Miami’s mix of tax perks and quality of life has fueled a 12% jump in AI roles, according to recent hiring data. - San Diego saw a 7% rise in Big Tech roles, even as startups in the region lost 3.5% of their workforce in 2024, suggesting talent is being poached upward. In 2024, San Diego County startups raised $5.7 billion in venture capital, marking one of the region's best-performing years on record. 3. Texas cools off: Are Austin and Houston losing their luster? Once the darlings of tech growth, Austin and Houston have been losing startup talent. Lagging infrastructure, a cultural mismatch, fluctuating housing costs, and a renewed emphasis on hybrid RTO policies are motivating startup employees to live closer to traditional hubs. Last year, Austin, which was a post-pandemic growth leader, saw a 6% drop in headcount at VC-backed startups. Houston's drop is even steeper at 10.9%. This isn’t just a story about regional reshuffling—it’s a signal of something bigger. The media’s return-to-office narrative is overstated, and companies are rethinking what really matters. The new model? Proximity over presence. For many tech companies, it’s not about clocking in five days a week—it’s about being close enough for hybrid schedules and anchor days. The result: a surge in in-state hiring as employers strike a new balance between flexibility and face time. #4 Hits and misses from 2024 and predictions for 2025 Some of our predictions from last year’s report held strong while others veered off course. Here’s what we got right—and what we predict is next. 2024: What we saw coming: Fractional work stuck around: Fractional roles are thriving—even at the top. CMOs, CFOs, and CTOs are increasingly working as consultants. Whether this trend holds in a stronger market remains to be seen, but for now, this model seems to be working for companies and execs alike. Cybersecurity kept growing: AI-powered threats are on the rise , and the demand for cybersecurity talent has continued to grow as predicted. Compensation is up, roles are harder to fill, and hiring urgency is higher than ever. Remote work didn’t die—it evolved: The RTO debate rages on, but the reality is more nuanced. Companies are increasingly adopting hybrid models while talent continues to demand flexibility. We won’t know the long-term balance until the next economic cycle resets the supply-demand equation. Our predictions for 2025: The rise of the generalist engineer: Specialists won the last decade. Generalists may win the next. With tools like Copilot, Replit, and Cursor maturing, engineers don’t need deep ML expertise to build with AI. Companies will prioritize flexible, collaborative, generalist engineers who can move fast and collaborate effectively with powerful tools—no PhD required. The year of the equity advisor: With lean startups hesitant to hire for both junior and C-suite roles, founders will tap seasoned experts as equity advisors. Carta data shows these roles are more affordable than before, offering startups a low-cost way to gain experience and mentorship without inflating their burn rate. New jobs—not just disappearing ones: While headlines warn of jobs lost to AI, we see another shift: new roles emerging. Expect to see titles like AI governance lead, AI ethics and privacy specialists, agentic AI engineers, and non-human security ops specialists become commonplace. It’ll take time to scale, but these are some of the roles new grads should be paying attention to. People, not just technology, will shape 2025 and beyond The past year made one thing clear: technology alone doesn’t build the future, people do. Anthropic’s retention edge and the reshaping of talent hubs prove that the real advantage lies in how you hire, grow, and keep great talent. What it means for the road ahead: - For new grads: The training wheels are gone. With fewer entry-level roles, the path forward will rely on bootcamps, open-source, freelancing, and creative projects. It’s not enough to just master the latest AI tools; learn to fix their flaws—debugging messy, machine-generated code may be your superpower. - For employers: AI might reduce the short-term need for junior hires, but skipping them entirely risks breaking the long-term talent pipeline. The industry’s future depends on equipping the next generation with skills that grow alongside the evolving technology landscape. ( Read our article on how to build a multi-generational company here .) ________________________ Methodology Note: This report uses data from our proprietary Beacon AI platform, an intelligence engine that tracks 650+ million individuals and 80+ million organizations. We analyze millions of data points on hiring trends, geographic movements, and more to spot emerging talent and help our portfolio companies build teams and products faster. Here's the approach we used to analyze the data for this report: - “Big tech” represents the top 15 technology companies by market cap. - “Startups” represents companies funded by the Top 100 VC firms that closed a Seed through Series C round in the previous 4 years. - “Top computer science graduates” represents graduates from the top 20 engineering programs in the U.S. according to the U.S. News’ Best Undergraduate Engineering Programs Rankings. - For the AI Labs retention analysis, we excluded some newer AI labs (like DeepSeek and xAI/Grok) because they had not been operating over the whole time period we looked at (2023-2024). We focused on entities with distinct",
      "summary": "17-minute read",
      "url": "https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 2308,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 2308/3400 words (67.9%)"
    },
    {
      "title": "AI's metrics question",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://www.ben-evans.com/benedictevans/2025/6/9/generative-ais-metrics-question?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
    },
    {
      "title": "The Code Review Frustration",
      "content": "Vibe code isn't meant to be reviewed: how to stay in control of codebase and not lose vibe code productivity boost 76 views > Disclaimer: The views and opinions expressed in this post are those of the author and do not necessarily reflect the official position of Monadical. Any content provided is for informational purposes only. ## The Code Review Frustration Another day, another slop. Chasing 10x productivity, you run several Claude code agents simultaneously and push code with blazing fast ferocity. No juniors left on the team — they just couldn't catch up. This morning, walking in the park with his dog, your team lead wrote and deployed a 100-story-point system just by casually talking on his phone to a remote agent. Meanwhile, you're stuck with a code review: ![](https://docs.monadical.com/uploads/042fee53-f1f2-40ba-9e3a-0e65fed1ee4a.png) \"It's fucking vibe code, I don't care!\" — this sentiment is just the tip of the iceberg of a current industry problem: <center><big>Treating all code the same when it fundamentally isn't</big></center> ## The Ownership Trap Here's the brutal reality: **the moment you start treating AI-generated code as \"precious,\" you lose AI's biggest superpower.** Once you've spent time reviewing, fixing, and improving that generated code, you become invested. You're going to be extra careful about AI \"breaking\" it. And it's ok, some code should be like this! But many times you just want to vibe and have your 10x productivity dream come true. Treating vibe code as precious is the productivity killer nobody talks about. You get the initial speed boost, then gradually slide back to normal development velocity as your codebase fills up with \"improved vibe code\" that you're reluctant to throw away. And you aren't ready to regenerate it from scratch anymore — an LLM skill that it excels at sometimes when it's stuck with a loop of never ending edits. Meanwhile, every code review becomes a battle between two conflicting mental models: - Reviewer: \"This could be cleaner\" (treating it like human code) - Author: \"It works, who cares\" (treating it like disposable vibe code) **The industry needs a way to keep these two types of code separate.** ## The Modular Solution: Giving Form to Chaos The solution is neither to abandon AI coding nor to accept messy codebases. It's to **architect the separation explicitly.** Think of it like this: Human code provides the \"form\" or \"mold\" that vibe code must fill. Just like the inpainting/outpainting feature in image generation. The human code contains your domain knowledge, business logic, and architectural decisions. The vibe code is just the implementation details that make it work. When AI code is clearly separated and constrained by interfaces, tests, and clear boundaries, you can also regenerate it fearlessly while keeping your valuable insights intact. ### The Overseer Package Approach #### The High-Level View Before scaring managers and leads with implementation details, here's the conceptual framework: 1. **Interface packages** - Define contracts, data shapes, and the most important tests (human-written) 2. **Implementation packages** - Fulfill those constraints (Vibe-generated, marked as @vibe-coded in README or in files) 3. **Clear dependency direction** - Implementation depends on interfaces, never the reverse 4. **Regeneration freedom** - Any @vibe-coded package can be deleted and rewritten without fear This creates a \"constraint sandwich\" - your domain knowledge stays protected while AI handles the tedious implementation work. > Technical implementation example awaits you in one of the last paragraphs. ## Two Flavors of Review: Classic vs YOLO With the modular approach and vibe code as \"second class citizen,\" we can now reframe code review practices: ### **Classic review: High standards, educational, expertise-building** Continue your nitpicky craftsmanship, talk about loops vs. combinators, and document your findings into project code style for LLMs to conform to. Share knowledge, discover new details about the system. ### **YOLO review**: \"Does it work? Does it pass tests? **Doesn't it sneak around the overseer package requirements**? Does it look safe enough? Ship it.\" <center><big>Clear separation of code \"types\" eliminates misunderstanding and friction</big></center> ## The Control Paradox Solved There are deeper psychological and sociological nth-order benefits to this approach. ### Professional Confidence When someone asks about a feature, you want to give an answer. \"I don't know, I vibed it\" destroys professional credibility. \"Cursor can't find the answer today, try again tomorrow\" makes you incompetent. With explicit separation, you can confidently say: \"The business logic is in the interface packages - here's exactly how it works. The implementation details are auto-generated, but the core logic is solid.\" ### Competitive Advantage While others choose between \"fast and messy\" or \"slow and clean,\" you get both. Your company's competitors using 100% vibe coding will hit complexity walls. Your company's competitors avoiding AI will be slower. You'll maintain AI productivity gains while building systems that actually scale. ### Better \"Boomer Business\" Adoption I believe that this approach could tip the scale for businesses who are still indecisive about vibe coding due to reliability and security concerns. ## Looking Forward: The Tooling Evolution I strongly believe that in the near future, the distinction between vibe code and human code will be admitted by industry and integrated into existing tools. ### Git-Level Integration Git commits automatically tagged as vibed. GitHub PRs showing clear visual distinction between human and AI contributions, up to the code line level. ### AI Agent Constraints Future coding agents will have an option to respect \"human code zones\" - like content-aware inpainting for image generation, but for code. Agents could regenerate entire implementations, not only files or packages, but code line-wise, leaving human guidance code untouched. ### IDE Evolution Syntax highlighting that dims unchecked vibe code while emphasizing human code. Folding options that hide implementation details. Search that prioritizes guidance, domain, architectural code. ### Corporate Adoption This separation makes AI coding auditable and controllable - exactly what the bloody enterprise needs. CTOs can require that all business logic lives in human-controlled packages while allowing rapid development in implementation packages. **So the idea of vibe code separation isn't just about individual productivity. It's about making AI coding enterprise-ready.** ## Technical Implementation in TypeScript One of the ways to split vibe and human code using current tooling that I found is a per-package approach. It's easy to do with a monorepo, but another structural or file-naming convention could work well too. For the case of monorepo, I used the \"-interface\" packages that contain concentrated domain knowledge and shape-setting code (tests, interfaces). I used dependency injection to draw more explicit frontiers between modules. ```ts export type GenerateAndSyncTasksDeps = { taskmaster: { generateTasks: ReturnType<GenerateTasksF>; }; tasktracker: { syncTasks: ReturnType<SyncTasksF>; }; }; export const generateAndSyncTasks = (di: GenerateAndSyncTasksDeps) => async (prd: PrdText) => { const tasks = await di.taskmaster.generateTasks(prd); return await di.tasktracker.syncTasks(tasks.tasks); }; ``` This is the \"entry point\" of a module that receives a PRD document, uses https://github.com/eyaltoledano/claude-task-master to generate tasks, and then syncs them to a task tracker. You can tell the coding agent to pick up from there, but it won't have enough guidance yet. Therefore, \"we need to go deeper.\" That's an example of how I defined the interface for taskmaster.generateTask, in its own package: ```ts export type GenerateTasksDeps = { savePrd: (path: NonEmptyString, prd: PrdText) => Promise<AsyncDisposable>; cli: { generate: ( prdPath: NonEmptyString, tasksJsonPath: NonEmptyString ) => Promise<TasksFileContent>; }; readTasksJson: (tasksJsonPath: NonEmptyString) => Promise<TasksFileContent>; }; export type GenerateTasksF = ( deps: GenerateTasksDeps ) => ( prd: PrdText, current: Option.Option<TasksFileContent> ) => Promise<TasksFileContent>; export const generateTasks: GenerateTasksF = (deps) => async (prd, current) => { if (Option.isSome(current)) { throw new Error(\"panic! PRD update not implemented\"); } const prdPath = castNonEmptyString(\"scripts/prd.txt\"); // not obvious: taskmaster CLI wants the prd first saved in file system await using _letFileGo = await deps.savePrd(prdPath, prd); const outputPath = castNonEmptyString(\"tasks/tasks.json\"); await deps.cli.generate(prdPath, outputPath); // don't clean up here // we read file system after CLI ran to return parsed tasks.json return await deps.readTasksJson(outputPath); }; ``` Past this point, it's already possible to tell Claude Code to generate the `GenerateTasksDeps` providing code that calls the CLI, saves and reads from the file system. Important details that we want to be preserved - \"PRD file is temporary and we want to have it in the file system before calling CLI,\" \"we also want to read the result of CLI call from the file system\" are well-preserved as strong contextual harness for LLM code. Data shape definitions are also a great candidate to use as controlling code: ```ts export const TaskFileContent = Schema.Struct({ id: TaskId, title: Schema.NonEmptyString, description: Schema.String, status: TaskStatus, dependencies: Schema.Array(TaskId), priority: Schema.optional(Schema.String), details: Schema.String, testStrategy: Schema.String, subtasks: Schema.Array(SubtaskFileContent), }); ``` Interfaces too: ```ts export interface TasksService { list: (filters?: { project?: ProjectId; status?: StatusId; user_story?: UserStoryId; }) => Promise<readonly TaskDetail[]>; create: (task: CreateTaskRequest) => Promise<TaskDetail>; get: (id: TaskId) => Promise<TaskDetail>; update: (id: TaskId, task: UpdateTaskRequest) => Promise<TaskDetail>; delete: (id: TaskId) => Promise<void>; } ``` And of course, unit tests are a great candidate for putting into controlling packages, especially [property-based tests](https://monadical.com/posts/property-based-testing-for-temporal-graph-storage.html). Also, you can put there all the code that you *could* 100% vibe but better *don't* - that you're supposed to know, if not by heart, then at least \"at some point when you wrote/reviewed it.\" Tell the agent to conform to those interfaces, test and shape-setting functions, writing the \"-implementation\" package counterpart to your \"-interface.\" An example system query: https://github.com/Monadical-SAS/taiga-taskmaster/blob/master/.llm-docs/PACKAGE_ORGANISATION_AND_CODE_SEPARATION_STANDARDS.md To me, it worked handsomely. The agent was very strong at regenerating code anew if I didn't like something and wanted to add more context to the \"control packages.\" And it never loses any context because of strict conformance to controlling code. And for the PR process, it now becomes clear which code is worth more attention and which you can just glance over, which unlocks much more of vibe power without compromising quality. Igor Loskutov is a Slop Enchanter of Monadical Recent posts Vibe code isn't meant to be reviewed * The Scraping-With-Cookies Dilemma Conversations are the New Oil Don't Give Big Tech Your Papaya View more posts... Back to top Let's transform your organization. BOOK A FREE AI STRATEGY CALL",
      "summary": "7-minute read",
      "url": "https://monadical.com/posts/vibe-code-how-to-stay-in-control.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 1660,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Silicon Valley's quest to remove friction from our lives",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
      "summary": "20-minute read",
      "url": "https://www.strangeloopcanon.com/p/notes-on-friction?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "A bit more on Twitter/X's new encrypted messaging",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
      "summary": "24-minute read",
      "url": "https://blog.cryptographyengineering.com/2025/06/09/a-bit-more-on-twitter-xs-new-encrypted-messaging/?utm_source=tldrnewsletter",
      "published_date": "2025-06-10T00:00:00",
      "category": "crypto",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Everything Apple Plans to Show at Its iOS 26-Focused WWDC 2025 Event",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
      "summary": "10-minute read",
      "url": "https://www.bloomberg.com/news/articles/2025-06-06/apple-wwdc-2025-preview-ios-26-macos-26-new-ai-features-ipados-26-redesigns?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc0OTQzOTIzMCwiZXhwIjoxNzUwMDQ0MDMwLCJhcnRpY2xlSWQiOiJTWEZUS0REV1gyUFMwMCIsImJjb25uZWN0SWQiOiI2NTc1NjkyN0UwMkM0N0MwQkQ0MDNEQTJGMEUyNzIyMyJ9.aZlWc8n-NMVZ4gSXwhmPWcqj9IHqbMYmKd4pnWNbVGI&utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Microsoft and Asus announce two Xbox Ally handhelds with new Xbox full-screen experience",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.theverge.com/news/682009/microsoft-asus-rog-xbox-ally-devices-new-windows-xbox-experience?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Breakthrough in search for HIV cure leaves researchers ‘overwhelmed'",
      "content": "A cure for HIV could be a step closer after researchers found a new way to force the virus out of hiding inside human cells. The virus’s ability to conceal itself inside certain white blood cells has been one of the main challenges for scientists looking for a cure. It means there is a reservoir of the HIV in the body, capable of reactivation, that neither the immune system nor drugs can tackle. Now researchers from the Peter Doherty Institute for Infection and Immunity in Melbourne, have demonstrated a way to make the virus visible, paving the way to fully clear it from the body. It is based on mRNA technology, which came to prominence during the Covid-19 pandemic when it was used in vaccines made by Moderna and Pfizer/BioNTech. In a paper published in Nature Communications , the researchers have shown for the first time that mRNA can be delivered into the cells where HIV is hiding, by encasing it in a tiny, specially formulated fat bubble. The mRNA then instructs the cells to reveal the virus. Globally, there are almost 40 million people living with HIV, who must take medication for the rest of their lives in order to suppress the virus and ensure they do not develop symptoms or transmit it. For many it remains deadly, with UNAids figures suggesting one person died of HIV every minute in 2023. It was “previously thought impossible” to deliver mRNA to the type of white blood cell that is home to HIV, said Dr Paula Cevaal, research fellow at the Doherty Institute and co-first author of the study, because those cells did not take up the fat bubbles, or lipid nanoparticles (LNPs), used to carry it. The team have developed a new type of LNP that those cells will accept, known as LNP X. She said: “Our hope is that this new nanoparticle design could be a new pathway to an HIV cure.” When a colleague first presented test results at the lab’s weekly meeting, Cevaal said, they seemed too good to be true. “We sent her back into the lab to repeat it, and she came back the next week with results that were equally good. So we had to believe it. And of course, since then, we’ve repeated it many, many, many more times. “We were overwhelmed by how [much of a] night and day difference it was – from not working before, and then all of a sudden it was working. And all of us were just sitting gasping like, ‘wow’.” Further research will be needed to determine whether revealing the virus is enough to allow the body’s immune system to deal with it, or whether the technology will need to be combined with other therapies to eliminate HIV from the body. The study is laboratory based and was carried out in cells donated by HIV patients. The path to using the technology as part of a cure for patients is long, and would require successful tests in animals followed by safety trials in humans, likely to take years, before efficacy trials could even begin. “In the field of biomedicine, many things eventually don’t make it into the clinic – that is the unfortunate truth; I don’t want to paint a prettier picture than what is the reality,” stressed Cevaal. “But in terms of specifically the field of HIV cure, we have never seen anything close to as good as what we are seeing, in terms of how well we are able to reveal this virus. “So from that point of view, we’re very hopeful that we are also able to see this type of response in an animal, and that we could eventually do this in humans.” Dr Michael Roche of the University of Melbourne and co-senior author of the research, said the discovery could have broader implications beyond HIV, with the relevant white blood cells also involved in other diseases including cancers. Dr Jonathan Stoye, a retrovirologist and emeritus scientist at the Francis Crick Institute, who was not involved in the study, said the approach taken by the Melbourne team appeared be a major advance on existing strategies to force the virus out of hiding, but further studies would be needed to determine how best to kill it after that. He added: “Ultimately, one big unknown remains. Do you need to eliminate the entire reservoir for success or just the major part? If just 10% of the latent reservoir survives will that be sufficient to seed new infection? Only time will tell. “However, that does not detract from the significance of the current study, which represents a major potential advance in delivery of mRNA for therapeutic purposes to blood cells.” Prof Tomáš Hanke of the Jenner Institute, University of Oxford, disputed the idea that getting RNA into white blood cells had been a significant challenge. He said the hope that all cells in the body where HIV was hiding could be reached in this way was “merely a dream”.",
      "summary": "2-minute read",
      "url": "https://www.theguardian.com/global-development/2025/jun/05/breakthrough-in-search-for-hiv-cure-leaves-researchers-overwhelmed?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 834,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "BYD's Five-Minute Charging Puts China in the Lead for EVs",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://spectrum.ieee.org/byd-megawatt-charging?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Field Notes From Shipping Real Code With Claude",
      "content": "June 7, 2025 Field Notes From Shipping Real Code With Claude Vibe Coding Isn’t Just a Vibe Shimmering Substance - Jackson Pollock Note : This post comes with a NotebookLM podcast ( 1 linked at the bottom), and three generated audio recordings. You can read the conversation I had with ChatGPT while preparing drafts of this post. Comments and discussion on the related HN post . Audio Think of this post as your field guide to a new way of building software. By the time you finish reading, you’ll understand not just the how but the why behind AI-assisted development that actually works. Here’s What You’re Going to Learn First, we’ll explore how to genuinely achieve a 10x productivity boost—not through magic, but through deliberate practices that amplify AI’s strengths while compensating for its weaknesses. Next, I’ll walk you through the infrastructure we use at Julep to ship production code daily with Claude’s help. You’ll see our CLAUDE.md templates, our commit strategies, and guardrails. Most importantly, you’ll understand why writing your own tests remains absolutely sacred, even (especially) in the age of AI. This single principle will save you from many a midnight debugging sessions. This is the main insight: Good development practices aren’t just nice-to-haves—they’re the difference between AI that amplifies your capabilities versus your chaos. The research bears this out. 2 Teams using rigorous practices deploy 46 times more frequently and are 440 times faster from commit to deployment. This effect is even more pronounced when you add capable AI assistants into the mix. Why This Post Exists: From Meme to Method Let me take you back to when this all started. 3 Andrej Karpathy 4 tweeted about “vibe-coding”—this idea of letting AI write your code while you just vibe. The developer community had a good laugh. It sounded like the ultimate developer fantasy: kick back, sip coffee, let the machines do the work. The birth of “vibe coding” Then Anthropic released Sonnet 3.7 and Claude Code , and something unexpected happened. The joke stopped being funny because it started being… possible? Of course, our trusty friend Cursor had been around awhile but this new interface finally felt like true vibe coding . At Julep , we build AI workflow orchestration. Our backend has years of accumulated decisions, patterns, and occasional technical debt. We have taken the utmost care to keep code quality high, and ample documentation for ourselves. However, the sheer size, and historical context of why different parts of the code are organized the way they are takes weeks for a good engineer to grok. Without proper guardrails when using Claude, you’re basically playing whack-a-mole with an overeager intern. Understanding Vibe-Coding ‘pls fix’ 5 Steve Yegge brilliantly coined the term CHOP —Chat-Oriented Programming in a slightly-dramatic-titled post “The death of the junior developer” . It’s a perfect, and no-bs description of what it’s like to code with Claude. Think of traditional coding like sculpting marble. You start with a blank block and carefully chisel away, line by line, function by function. Every stroke is deliberate, every decision yours. It’s satisfying but slow. Vibe-coding is more like conducting an orchestra. You’re not playing every instrument—you’re directing, shaping, guiding. The AI provides the raw musical talent, but without your vision, it’s just noise. There are three distinct postures you can take when vibe-coding, each suited to different phases in the development cycle: AI as First-Drafter : Here, AI generates initial implementations while you focus on architecture and design. It’s like having a junior developer who can type at the speed of thought but needs constant guidance. Perfect for boilerplate, CRUD operations, and standard patterns. AI as Pair-Programmer : This is the sweet spot for most development. You’re actively collaborating, bouncing ideas back and forth. The AI suggests approaches, you refine them. You sketch the outline, AI fills in details. It’s like pair programming with someone who has read every programming book ever written but has never actually shipped code. AI as Validator : Sometimes you write code and want a sanity check. AI reviews for bugs, suggests improvements, spots patterns you might have missed. Think of it as an incredibly well-read code reviewer who never gets tired or cranky. Instead of crafting every line, you’re reviewing, refining, directing. But—and this cannot be overstated—you remain the architect. Claude is your intern with encyclopedic knowledge but zero context about your specific system, your users, your business logic. The Three Modes of Vibe-Coding: A Practical Framework After months of experimentation and more than a few production incidents, I’ve settled on three distinct modes of operation. Each has its own rhythm, its own guardrails, and its own use cases. Mode 1: The Playground Lighter Fluid When to use it : Weekend hacks, personal scripts, proof-of-concepts, and those “I wonder if…” moments that make programming fun. In Playground Mode , you embrace the chaos. Claude writes 80-90% of the code while you provide just enough steering to keep things on track. It’s liberating and slightly terrifying. Pro Tip: check out claude-composer for going full-YOLO mode. Here’s what Playground Mode looks like: You have an idea for a script to analyze your Spotify history. You open Claude, describe what you want in plain English, and watch as it generates a complete solution. No CLAUDE.md file, no careful prompting—just raw, unfiltered AI-written code. The beauty of Playground Mode is its speed. You can go from idea to working prototype in minutes. The danger is that this cowboy coding style is absolutely inappropriate for anything that matters. Use it for experiments, never for production. Trust me, while the amazing folks preaching otherwise, good engineering principles still matter, now more than ever . Mode 2: Pair Programming Compiling When to use it : Projects under ~5,000 lines of code , side projects with real users, demos (you don’t want to break), or well-scoped small services in larger systems. This is where vibe-coding starts to shine. You need structure, but not so much that it slows you down. The key innovation here is the CLAUDE.md file—custom documentation that Claude automatically reads when invoked. From Anthropic’s Best practices for Claude Code : CLAUDE.md is a special file that Claude automatically pulls into context when starting a conversation: Common bash commands Core files and utility functions Code style guidelines Testing instructions Repository etiquette (e.g., branch naming, merge vs. rebase, etc.) Other information you want Claude to remember Instead of repeatedly explaining your project’s conventions, you document them once. Here’s a real example from a recent side project: ## Project: Analytics Dashboard This is a Next.js dashboard for visualizing user analytic: ### Architecture Decisions - Server Components by default, Client Components only when necessary - tRPC for type-safe API calls - Prisma for database access with explicit select statements - Tailwind for styling (no custom CSS files) ### Code Style - Formatting: Prettier with 100-char lines - Imports: sorted with simple-import-sort - Components: Pascal case, co-located with their tests - Hooks: always prefix with 'use' ### Patterns to Follow - Data fetching happens in Server Components - Client Components receive data as props - Use Zod schemas for all external data - Error boundaries around every data display component ### What NOT to Do - Don't use useEffect for data fetching - Don't create global state without explicit approval - Don't bypass TypeScript with 'any' types Audio With this context, Claude becomes remarkably effective. It’s like the difference between explaining your project to a new hire every single day versus having them read the onboarding docs once. But Pair Programming Mode requires more than just documentation. You need to actively guide the AI with what I call “anchor comments”—breadcrumbs that prevent Claude from wandering into the wilderness: // AIDEV- NOTE: This component uses virtual scrolling for performance // See: https://tanstack.com/virtual/latest // Don't convert to regular mapping—we handle 10k+ items export function DataTable ( { items }: DataTableProps ) { // Claude, when you edit this, maintain the virtual scrolling ... } These comments serve a dual purpose: they guide the AI and document your code for humans. It’s documentation that pays dividends in both directions. The key distinction between such “anchor comments” and regular comments: these are written , maintained , and meant to be used by Claude itself. Here’s an actual snippet from our project’s CLAUDE.md : ## Anchor comments Add specially formatted comments throughout the codebase, where appropriate, for yourself as inline knowledge that can be easily `grep` ped for. ### Guidelines: - Use `AIDEV-NOTE:` , `AIDEV-TODO:` , or `AIDEV-QUESTION:` (all-caps prefix) for comments aimed at AI and developers. - Keep them concise (≤ 120 chars). - **Important:** Before scanning files, always first try to **locate existing anchors** `AIDEV-*` in relevant subdirectories. - **Update relevant anchors** when modifying associated code. - **Do not remove `AIDEV-NOTE`s** without explicit human instruction. Example: # AIDEV-NOTE: perf-hot-path; avoid extra allocations (see ADR-24) async def render _feed(...): ... Mode 3: Production/Monorepo Scale RTFM When to use it : Large codebases, systems with real users, anything where bugs cost money or reputation. Claude can generate tremendous amounts of code, but integrating it into a complex system requires careful orchestration. Let me start with a big caveat: vibe coding at this scale does NOT scale very well, yet. I definitely do see these systems getting significantly better at handling larger codebases but , for them to be effective, significant effort is needed to help them navigate, understand, and safely hack on them without getting lost in a maze. Generally speaking, it’s better to section them into individual services, and 6 sub modules when possible. As a universal principle, good engineering practices apply to large-scale projects, vibe coded or not. For example, at production scale, boundaries become critical. Every integration point needs explicit documentation: # AIDEV- NOTE: API Contract Boundary - v2.3.1 # ANY changes require version bump and migration plan # See: docs/api-versioning.md @router.get( \"/users/{user_id}/feed\" ) async def get_user_feed ( user_id: UUID ) -> FeedResponse: # Claude: the response shape here is sacred # Changes break real apps in production ... Without these boundaries, Claude will happily “improve” your API and break every client in production. Bottom line: larger projects should definitely start adopting vibe coding in parts, and adopt methodologies that enhance that experience but, don’t expect to land large features reliably just yet. (as of June 7, 2025 / AI epoch ) Infrastructure: The Foundation of Sustainable AI Development CLAUDE.md : Your Single Source of Truth Let me be absolutely clear about this: CLAUDE.md is not optional documentation. Every minute you spend updating it saves an hour of cleanup later. Think of CLAUDE.md as a constitution for your codebase. It establishes the fundamental laws that govern how code should be written, how systems interact, and what patterns to follow or avoid. Organizations that invest in developing the skills and capabilities of their teams get better outcomes—and your CLAUDE.md is that investment crystallized into documentation. Here’s an abridged version of our production CLAUDE.md structure, refined over thousands of AI-assisted commits: # `CLAUDE.md` - Julep Backend Service ## The Golden Rule When unsure about implementation details, ALWAYS ask the developer. ## Project Context Julep enables developers to build stateful AI agents using declarative workflows. ## Critical Architecture Decisions ### Why Temporal? We use Temporal for workflow orchestration because: 1. Workflows can run for days/weeks with perfect reliability 2. Automatic recovery from any failure point ### Why PostgreSQL + pgvector? 1. ACID compliance for workflow state (can't lose user data) 2. Vector similarity search for agent memory ### Why TypeSpec? Single source of truth for API definitions: - OpenAPI specs - TypeScript/Python clients - Validation schemas ## Code Style and Patterns ### Anchor comments Add specially formatted comments throughout the codebase, where appropriate, for yourself as inline knowledge that can be easily `grep` ped for. ### Guidelines: - Use `AIDEV-NOTE:` , `AIDEV-TODO:` , or `AIDEV-QUESTION:` (all-caps prefix) for comments aimed at AI and developers. - **Important:** Before scanning files, always first try to **grep for existing anchors** `AIDEV-*` in relevant subdirectories. - **Update relevant anchors** when modifying associated code. - **Do not remove `AIDEV-NOTE`s** without explicit human instruction. - Make sure to add relevant anchor comments, whenever a file or piece of code is: * too complex, or * very important, or * confusing, or * could have a bug ## Domain Glossary (Claude, learn these!) - **Agent** : AI entity with memory, tools, and defined behavior - **Task** : Workflow definition composed of steps (NOT a Celery task) - **Execution** : Running instance of a task - **Tool** : Function an agent can call (browser, API, etc.) - **Session** : Conversation context with memory - **Entry** : Single interaction within a session ## What AI Must NEVER Do 1. **Never modify test files** - Tests encode human intent 2. **Never change API contracts** - Breaks real applications 3. **Never alter migration files** - Data loss risk 4. **Never commit secrets** - Use environment variables 5. **Never assume business logic** - Always ask 6. **Never remove AIDEV- comments** - They're there for a reason Remember: We optimize for maintainability over cleverness. When in doubt, choose the boring solution. This document becomes the shared context between you and Claude. It’s like having a senior developer whispering guidance in Claude’s ear throughout the coding session. Anchor Comments: Breadcrumbs at Scale As your codebase grows, CLAUDE.md alone isn’t enough. You need inline guidance—what I call anchor comments. These serve as local context that prevents AI from making locally bad decisions. Think of your codebase as a city and anchor comments as street signs. Without them, even smart visitors get lost. Here’s how we use them effectively: # AIDEV- NOTE: Critical performance path - this serves 100k req/sec # DO NOT add database queries here def get_user_feed ( user_id: UUID, cached_data: FeedCache ) -> List[FeedItem]: # We need to avoid mutating the cached data items = cached_data.items[:] # AIDEV- TODO: Implement pagination (ticket: FEED-123) # Need cursor-based pagination for infinite scroll # AIDEV-QUESTION: Why do we filter private items here instead of in cache? # AIDEV-ANSWER: Historical context: Privacy rules can change between cache updates filtered = [item for item in items if user_has_access(user_id, item)] return filtered These comments create a narrative that helps both AI and humans understand not just what the code does, but why it does it that way. Git Workflows for AI Development One of the most underappreciated aspects of AI-assisted development is how it changes your git workflow. You’re now generating code at a pace that can quickly pollute your git history if you’re not careful. It really only applies to very large codebases because it is not a very straightforward tool, but I recommend using git worktrees to create isolated environments for AI experiments: # Create an AI playground without polluting main git worktree add ../ai-experiments/cool-feature -b ai/cool-feature # Let Claude go wild in the isolated worktree cd ../ai-experiments/cool-feature # ... lots of experimental commits ... # Cherry-pick the good stuff back to main cd ../main-repo git cherry-pick abc123 # Just the commits that worked # Clean up when done git worktree remove ../ai-experiments/cool-feature Pro tip : Read about how to use worktrees , and check out the nifty wt tool. This approach gives you the best of both worlds: Claude can experiment freely while your main branch history stays clean and meaningful. For commit messages, we’ve standardized on tagging AI-assisted commits: feat: implement user feed caching [AI] - Add Redis-based cache for user feeds - Implement cache warming on user login - Add metrics for cache hit rate AI-assisted: core logic generated, tests human-written This transparency helps during code review—reviewers know to pay extra attention to AI-generated code. The Sacred Rule: Humans Write Tests Now we come to the most important principle in AI-assisted development. It’s so important that I’m going to repeat it in multiple ways until it’s burned into your memory: Never. Let. AI. Write. Your. Tests. Tests are not just code that verifies other code works. Tests are executable specifications. They encode your actual intentions, your edge cases, your understanding of the problem domain. High performers excel at both speed and stability—there’s no trade-off. Tests are how you achieve both. Beware… Let me illustrate why this matters with an example. Let’s say we asked Claude to implement a rate limiter: class RateLimiter : def __init__ ( self, max_requests: int , window_seconds: int ): self.max_requests = max_requests self.window_seconds = window_seconds self.requests = defaultdict( list ) def is_allowed ( self, user_id: str ) -> bool: now = time.time() user_requests = self.requests[user_id] # Clean old requests self.requests[user_id] = [ req_time for req_time in user_requests if now - req_time < self.window_seconds ] if len (self.requests[user_id]) < self.max_requests: self.requests[user_id].append(now) return True return False Looks reasonable, right? Claude even helpfully generated tests: def test_rate_limiter (): limiter = RateLimiter(max_requests= 3 , window_seconds= 60 ) assert limiter.is_allowed( \"user1\" ) == True assert limiter.is_allowed( \"user1\" ) == True assert limiter.is_allowed( \"user1\" ) == True assert limiter.is_allowed( \"user1\" ) == False # Limit reached But here’s what Claude’s tests missed—what only a human who understands the business requirements would test: Claude’s implementation has a memory leak. Users who hit the API once and never return leave their data in memory forever. The AI-generated tests check the happy path but miss this critical production concern. Vibe coding at its best This is why humans write tests. We understand the context, the production environment, the edge cases that matter. At Julep, our rule is absolute: ## Testing Discipline | What | AI CAN Do | AI MUST NOT Do | |------|-----------|----------------| | Implementation | Generate business logic | Touch test files | | Test Planning | Suggest test scenarios | Write test code | | Debugging | Analyze test failures | Modify test expectations | If an AI tool touches a test file, the PR gets rejected. No exceptions. Your tests are your specification. They’re your safety net. They’re the encoded wisdom of every bug you’ve fixed and every edge case you’ve discovered. Guard them zealously. Scaling Without Drowning: Token Economics and Context Management One of the most counterintuitive lessons in AI-assisted development is that being stingy with context to save tokens actually costs you more. It’s like trying to save money on gas by only filling your tank halfway—you just end up making more trips to the gas station. Token budgets matter. Provide focused prompts, reduce diff length, and avoid large-file bloat by summarizing intent in advance. But “focused” doesn’t mean “minimal”—it means “relevant and complete.” Let me show you the false economy of starved prompts: Starved Prompt Attempt: \"Add caching to the user endpoint\" Claude’s Response: Implements caching… but: Uses in-memory cache (won’t work with multiple servers) No cache invalidation strategy No metrics or monitoring No consideration of cache stampede Result: 3 more rounds of fixes, 4x the tokens spent . Proper Context-Rich Prompt: Add Redis caching to the GET /users/{id} endpoint. Context: - This endpoint serves 50k requests/minute - We run 12 API servers behind a load balancer - User data changes infrequently (few times per day) - We already have Redis at cache.redis.internal:6379 - Use our standard cache key pattern: \"user:v1:{id}\" - Include cache hit/miss metrics (we use Prometheus) - Implement cache-aside pattern with 1 hour TTL - Handle cache stampede with probabilistic early expiration See our caching guide: docs/patterns/caching.md The lesson? Front-load context to avoid iteration cycles. Think of tokens like investing in good tools—the upfront cost pays for itself many times over. In fact, I recommend that all projects should routinely ask Claude to look through the codebase changes, and add context to CLAUDE.md Fresh Sessions and Mental Models Here’s another counterintuitive practice: use fresh Claude sessions for distinct tasks. It’s tempting to keep one long-running conversation, but this leads to context pollution. Think of it like this: you wouldn’t use the same cutting board for vegetables after cutting raw chicken. Similarly, don’t use the same Claude session for database migrations after discussing frontend styling. The context bleeds through in subtle ways. Our rule: One task, one session. When the task is done, start fresh. This keeps Claude’s “mental model” clean and focused. Audio Case Study: Shipping Structured Errors in Production Let me walk you through a real refactoring we did at Julep that showcases production-scale vibe-coding. We needed to replace our ad-hoc error handling with a structured error hierarchy across 500+ endpoints. The Human Decisions (The Why): First, we had to decide on our error taxonomy. This is pure architectural work—Claude can’t make these decisions because they involve understanding our business, our users, and our operational needs: # SPEC.md - Error Hierarchy Design (Human-Written) ## Error Philosophy - Client errors (4xx) must include actionable feedback - System errors (5xx) must include trace IDs for debugging - All errors must be JSON-serializable - Error codes must be stable (clients depend on them) ## Hierarchy BaseError ├── ClientError (4xx) │ ├── ValidationError │ │ ├── SchemaValidationError - Request doesn't match schema │ │ ├── BusinessRuleError - Valid schema, invalid business logic │ │ └── RateLimitError - Too many requests │ └── AuthError │ ├── AuthenticationError - Who are you? │ └── AuthorizationError - You can't do that └── SystemError (5xx) ├── DatabaseError - Connection, timeout, deadlock ├── ExternalServiceError - APIs, webhooks failing └── InfrastructureError - Disk full, OOM, etc. ## Error Response Format { \"error\": { \"code\": \"VALIDATION_FAILED\", // Stable code for clients \"message\": \"Email already exists\", // Human-readable \"details\": { ... }, // Structured data \"trace_id\": \"abc-123-def\" // For debugging } } The AI Execution (The How): With the specification clear, we unleashed Claude on the mechanical refactoring: ### Prompt to Claude: Refactor our error handling to match SPEC.md. Current state: - raise ValueError(\"Invalid email\") - return {\"error\": \"Something went wrong\"}, 500 Target state: - Use error hierarchy from SPEC.md - Include proper error codes - Add trace _id to all 5xx errors Start with the auth module. Show me the plan before implementing. Claude’s plan was solid: 1. Create error hierarchy in `common/errors.py` 2. Create error response formatter 3. Update each module systematically 4. Add error handling middleware Claude was able to handle the tedious work of finding and updating 500+ error sites, while we focused on reviewing: # Before (Claude found these patterns): if not user: raise Exception( \"User not found\" ) # After (Claude's refactoring): if not user: raise AuthenticationError( message= \"User not found\" , code= \"USER_NOT_FOUND\" , details={ \"identifier\" : email} ) Combined with our carefully written CLAUDE.md file, meticulous docs, regularly updated anchor comments, and clear instructions, results: Time: 4 hours instead of 2 days Coverage: All 500+ error sites updated Leadership and Culture in the AI Era Your role as a senior engineer has fundamentally shifted. You’re no longer just writing code—you’re curating knowledge, setting boundaries, and teaching both humans and AI systems how to work effectively. Lean management and continuous delivery practices help improve software delivery performance, which in turn improves organizational performance—and this includes how you manage AI collaboration. The New Onboarding Checklist When new developers join our team, they get two onboarding tracks: one for humans, one for working with AI. Here’s our combined checklist: Week 1: Foundation □ Read team `CLAUDE.md` files (start with root, then service-specific) □ Set up development environment □ Make first PR (human-written, no AI) Week 2: Guided AI Collaboration □ Set up Claude with team templates □ Complete \"toy problem\" with AI assistance □ Practice prompt patterns □ Create first AI-assisted PR (with supervision) Week 3: Independent Work □ Ship first significant AI-assisted feature □ Write tests for another developer's AI output □ Lead one code review session Building a Culture of Transparency One cultural shift that’s essential: normalize disclosure of AI assistance. We’re not trying to hide that we use AI—we’re trying to use it responsibly. Every commit message that includes AI work gets tagged: # Our .gitmessage template # feat/fix/docs: <description> [AI]? # # [AI] - Significant AI assistance (>50% generated) # [AI-minor] - Minor AI assistance (<50% generated) # [AI-review] - AI used for code review only # # Example: # feat: add Redis caching to user service [AI] # # AI generated the cache implementation and Redis client setup. # I designed the cache key structure and wrote all tests. # Manually verified cache invalidation logic works correctly. This transparency serves multiple purposes: Reviewers know to pay extra attention Future debuggers understand the code’s provenance No one feels shame",
      "summary": "37-minute read",
      "url": "https://diwank.space/field-notes-from-shipping-real-code-with-claude?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 4071,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 4071/7400 words (55.0%)"
    },
    {
      "title": "MCP vs API",
      "content": "The HTTP API Problem MCP: A Wire Protocol, Not Documentation Why Not Just Use OpenAPI? Five Fundamental Differences Runtime Discovery vs Static Specs Deterministic Execution vs LLM-Generated Calls Bidirectional Communication Single-Request Human Tasks Local-First by Design The Training Advantage They're Layers, Not Competitors Real-World Example The Bottom Line Bonus: MCP vs API video Bonus: Existing Reddit discussions Every week a new thread emerges on Reddit asking about the difference between MCP and API. I've tried summarizing everything that's been said about MCP vs API in a single post (and a single table). Aspect Traditional APIs (REST/GraphQL) Model Context Protocol (MCP) What it is Interface styles (REST, GraphQL) with optional spec formats (OpenAPI, GraphQL SDL) Standardized protocol with enforced message structure Designed for Human developers writing code AI agents making decisions Data location REST: Path, headers, query params, body (multiple formats) Single JSON input/output per tool Discovery Static docs, regenerate SDKs for changes 1 2 Runtime introspection ( tools/list ) Execution LLM generates HTTP requests (error-prone) LLM picks tool, deterministic code runs Direction Typically client-initiated; server-push exists but not standardized Bidirectional as first-class feature Local access Requires port, auth, CORS setup Native stdio support for desktop tools Training target Impractical at scale due to heterogeneity Single protocol enables model fine-tuning I am making several broad generalizations to keep the article length reasonable. I will continue to update this article with feedback from the community. If you have any suggestions, please email me at frank@glama.ai . The HTTP API Problem HTTP APIs suffer from combinatorial chaos. To send data to an endpoint, you might encode it in: URL path ( /users/123 ) Request headers ( X-User-Id: 123 ) Query parameters ( ?userId=123 ) Request body (JSON, XML, form-encoded, CSV) OpenAPI/Swagger documents these variations, but as a specification format, it describes existing patterns rather than enforcing consistency. Building automated tools to reliably use arbitrary APIs remains hard because HTTP wasn't designed for this—it was the only cross-platform, firewall-friendly transport universally available from browsers. MCP: A Wire Protocol, Not Documentation Model Context Protocol (MCP) isn't another API standard—it's a wire protocol that enforces consistency. While OpenAPI documents existing interfaces with their variations, MCP mandates specific patterns: JSON-RPC 2.0 transport, single input schema per tool, deterministic execution. Key architecture: Transport : stdio (local) or streamable HTTP Discovery : tools/list , resources/list expose capabilities at runtime Primitives : Tools (actions), Resources (read-only data), Prompts (templates) There is more than the above. Refer to the MCP specification for complete overview. Why Not Just Use OpenAPI? The most common question: \"Why not extend OpenAPI with AI-specific features?\" Three reasons: OpenAPI describes; MCP prescribes . You can't fix inconsistency by documenting it better—you need enforcement at the protocol level. Retrofitting fails at scale . OpenAPI would need to standardize transport, mandate single-location inputs, require specific schemas, add bidirectional primitives—essentially becoming a different protocol. The ecosystem problem . Even if OpenAPI added these features tomorrow, millions of existing APIs wouldn't adopt them. MCP starts fresh with AI-first principles. Five Fundamental Differences 1. Runtime Discovery vs Static Specs API : Ship new client code when endpoints change MCP : Agents query capabilities dynamically and adapt automatically // MCP discovery - works with any server client.request('tools/list') // Returns all available tools with schemas 2. Deterministic Execution vs LLM-Generated Calls API : LLM writes the HTTP request → hallucinated paths, wrong parameters MCP : LLM picks which tool → wrapped code executes deterministically This distinction is critical for production safety. With MCP, you can test, sanitize inputs, and handle errors in actual code, not hope the LLM formats requests correctly. 3. Bidirectional Communication API : Server-push exists (WebSockets, SSE, GraphQL subscriptions) but lacks standardization MCP : Bidirectional communication as first-class feature: Request LLM completions from server Ask users for input ( elicitation ) Push progress notifications 4. Single-Request Human Tasks REST APIs fragment human tasks across endpoints. Creating a calendar event might require: POST /events (create) GET /conflicts (check) POST /invitations (notify) MCP tools map to complete workflows. One tool, one human task. 5. Local-First by Design API : Requires HTTP server (port binding, CORS, auth headers) MCP : Can run as local process via stdio—no network layer needed Why this matters: When MCP servers run locally via stdio, they inherit the host process's permissions. This enables: Direct filesystem access (read/write files) Terminal command execution System-level operations A local HTTP server could provide the same capabilities. However, I think the fact that MCP led with stdio transport planted the idea that MCP servers are meant to be as local services, which is not how we typically think of APIs. The Training Advantage MCP's standardization creates a future opportunity: models could be trained on a single, consistent protocol rather than thousands of API variations. While models today use MCP through existing function-calling capabilities, the protocol's uniformity offers immediate practical benefits: Consistent patterns across all servers: Discovery: tools/list , resources/list , prompts/list Execution: tools/call with single JSON argument object Errors: Standard JSON-RPC format with numeric codes Reduced cognitive load for models: // Every MCP tool follows the same pattern: { \"method\": \"tools/call\", \"params\": { \"name\": \"github.search_prs\", \"arguments\": {\"query\": \"security\", \"state\": \"open\"} } } // Versus REST APIs with endless variations: // GET /api/v2/search?q=security&type=pr // POST /graphql {\"query\": \"{ search(query: \\\"security\\\") { ... } }\"} // GET /repos/owner/repo/pulls?state=open&search=security This standardization means models need to learn one calling convention instead of inferring patterns from documentation. As MCP adoption grows, future models could be specifically optimized for the protocol, similar to how models today are trained on function-calling formats. They're Layers, Not Competitors Most MCP servers wrap existing APIs: [AI Agent] ⟷ MCP Client ⟷ MCP Server ⟷ REST API ⟷ Service The mcp-github server translates repository/list into GitHub REST calls. You keep battle-tested infrastructure while adding AI-friendly ergonomics. Real-World Example Consider a task: \"Find all pull requests mentioning security issues and create a summary report.\" With OpenAPI/REST : LLM reads API docs, generates: GET /repos/{owner}/{repo}/pulls?state=all Hopes it formatted the request correctly Parses response, generates: GET /repos/{owner}/{repo}/pulls/{number} Repeats for each PR (rate limiting issues) Generates search queries for comments Assembles report With MCP : LLM calls: github.search_issues_and_prs({query: \"security\", type: \"pr\"}) Deterministic code handles pagination, rate limits, error retry Returns structured data LLM focuses on analysis, not API mechanics The Bottom Line HTTP APIs evolved to serve human developers and browser-based applications, not AI agents. MCP addresses AI-specific requirements from the ground up: runtime discovery, deterministic execution, and bidirectional communication. For AI-first applications, MCP provides structural advantages—local execution, server-initiated flows, and guaranteed tool reliability—that would require significant workarounds in traditional API architectures. The practical path forward involves using both: maintaining APIs for human developers while adding MCP for AI agent integration. Bonus: MCP vs API video During my research, I found this video to be one of the easiest to digest the differences between MCP and API. Bonus: Existing Reddit discussions During my research, I found these Reddit discussions to be helpful in understanding the differences between MCP and API. https://www.reddit.com/r/mcp/comments/1kyeu26/why_mcp_protocol_vs_openapi_docs/ https://www.reddit.com/r/mcp/comments/1iztbrc/whats_the_difefrence_of_using_an_api_vs_an_mcp/ https://www.reddit.com/r/mcp/comments/1jr8if3/i_cant_understand_the_hype/ Footnotes GraphQL offers schema introspection, but it lacks task-level descriptions or JSON-schema-style validation, so SDKs still regenerate for new fields. ↩ OpenAPI 3.1+ supports runtime discovery through the OpenAPI document endpoint. The key difference is that MCP mandates runtime discovery while OpenAPI makes it optional. ↩ Written by Frank Fiegel ( @ punkpeye )",
      "summary": "6-minute read",
      "url": "https://glama.ai/blog/2025-06-06-mcp-vs-api?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 1218,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Welcome to Campus. Here's Your ChatGPT",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~1600 words, got 435. URL: https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
      "summary": "8-minute read",
      "url": "https://www.nytimes.com/2025/06/07/technology/chatgpt-openai-colleges.html?unlocked_article_code=1.Nk8.Z6qI.moAMRJaHP7t6&smid=url-share&utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 435,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 435/1600 words (27.2%)"
    },
    {
      "title": "The hidden time bomb in the tax code that's fueling mass tech layoffs",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
      "summary": "14-minute read",
      "url": "https://qz.com/tech-layoffs-tax-code-trump-section-174-microsoft-meta-1851783502?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "magic namerefs",
      "content": "namerefs (introduced in bash 4.0) act as aliases for other variables var=meow declare -n ref=var echo $ref # prints meow ref=moo echo $var # prints moo they can also reference a specific element in an array, using declare -n ref='array[1234]' using this, i have been playing with a neat nameref trick: tmp=() declare -n var= ' tmp[tmp[0]=some_expression_here, 0] ' it uses the auxiliary array tmp to force an arithmetic context, in which it assigns the result of any arbitrary expression to an element of that array, then expands that same element. we can now create magic variables that evaluate any arbitrary expression. here's a basic counter: tmp=() x=0 declare -n counter= ' tmp[tmp[0]=x++,0] ' for i in {1..10} ; do echo $counter done # prints 0 1 2 3 4 5 6 7 8 9 here's an example that computes the fibonacci numbers: f=(0 1) declare -n fib= ' f[f[2]=f[0], f[0]+=f[1], f[1]=f[2], 0] ' for i in {1..10} ; do echo $fib done # prints 1 1 2 3 5 8 13 21 34 55 this is already very powerful, as it can do many magic things with numbers. but as it turns out, we can do even more: we can use dollar expansions too! here's a silly clock with magic variables that show the current date and time: # \\D{your-format-here} passes that format to strftime # but it only works in prompts like $PS1 # ${var@P} expands $var as if it was in your $PS1 date=( ' \\D{% ' {Y,m,d,H,M,S}}) # the formats we'll use months=(- jan feb mar apr may jun jul aug sep oct nov dec) numbers=({00..60}) tmp=() declare -n year= ' tmp[tmp=${date[0]@P},0] ' declare -n month= ' months[10#${date[1]@P}] ' declare -n day= ' numbers[10#${date[2]@P}] ' declare -n hour= ' numbers[10#${date[3]@P}] ' declare -n minute= ' numbers[10#${date[4]@P}] ' declare -n second= ' numbers[10#${date[5]@P}] ' while : ; do echo $year / $month / $day $hour : $minute : $second sleep 1 done # 2025/jun/06 09:54:13 # 2025/jun/06 09:54:14 # 2025/jun/06 09:54:15 # 2025/jun/06 09:54:16 this is probably one of the coolest things i've ever seen in bash. honestly i am a bit horrified that this works at all, but the resulting code is just so simple and elegant. and it feels like i'm just scratching the surface, there's so much potential. previous: recursive expansions",
      "summary": "2-minute read",
      "url": "https://gist.github.com/izabera/e4717562e20eb6cfb6e05f8019883efb?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 387,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "SaaS Is Just Vendor Lock-In with Better Branding",
      "content": "SaaS Is Just Vendor Lock-In with Better Branding Developers are told \"to focus on the product\" and let SaaS vendors handle the rest, but integrating third-party services, whether it's auth, queuing, file storage, or image optimization, comes at a cost. Not just in dollars but in time, friction, and mental overhead. There are five hidden taxes you pay every time you integrate a SaaS into your stack. 1. The Discovery Tax Before you can integrate anything, you first have to figure out what they're actually selling? What problems are they solving? Is it compatible with your stack? Is their price sane at your scale? a Are their docs clear and do they reveal any implementation weirdness? This unpaid research work is usually non-transferable. What you learn about \"Uploady\" or \"MegaQueue\" doesn't help you next time when you're evaluating something else. It's also subjective. It's marketing, and does the marketing message resonate with you? 2. The Sign-Up Tax You've decided on a service, and this is the moment when you hand over your email and credit card. Do they support usage-based pricing or only lock-in tiers? Can your team members access the dashboard, or do you have to pay more for that functionality? Despite only using the service the same amount! Can you even test the product without hitting a paywall? You're now on the hook, even if you haven't written a single line of code. 3. The Integration Tax Now the real work begins. You read the docs. You install the libraries You wire it into your framework. And figure out the edge cases that the docs don't mention, because docs are marketing! Often you're left fighting your own tooling. They're aiming for the lowest common denominator, and you're bleeding edge. Or the other way around! 4. The Local Development Tax You need the SaaS service to work locally. Does it even offer a local emulator? Can you stub it out in tests? Do you need to tunnel to the cloud just to test one feature? Now you've got branching configuration logic, one for production, one for staging, one for local… If you're lucky. 5. The Production Tax This is the part where you're \"done,\" except you're not. Can you use this in your staging environment? What about pull request previews? You need to securely manage the API keys. Monitoring, logging, and alerting Wondering why something worked in your laptop but fails in production? You've integrated the service, but now you're on the hook for its reliability in production. Conclusion The pitch of modern SaaS is \"don't reinvent the wheel.\" But every wheel you bolt on comes with some friction. It's not just a service: It's a contract. It's a dependency. It's a subtle architectural shift, and it comes with taxes. No matter what choice you make, it's always going to be vendor-locked in. Switching out something, even if it's open source and self-hosted, means that you're rewriting a lot of code. So, my argument is, don't make those decisions. Just pick a platform. The thing that matters is the software that you want to write, not the framework or the services that it runs on. Platforms like Cloudflare or Supabase shine. Where your database, queue, image service, and storage all live within the same platform and speak the same language. You avoid paying these taxes repeatedly. You simply pick the product that's already there. No context switching between vendors. No API key wrangling. No compatibility hacks or configuration forks. Just fast, local feeling integrations that work the same in dev and production. It feels like everything is running on the same machine, and in a way it kind of is. That's the hidden superpower of integrated platforms. They collapse the distance between your code and your services. And in doing so, they give you back the one thing no SaaS vendor can sell you: \"Flow.\"",
      "summary": "3-minute read",
      "url": "https://rwsdk.com/blog/saas-is-just-vendor-lock-in-with-better-branding?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "dev",
      "word_count": 646,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Reverse Engineering Cursor's LLM Client",
      "content": "Reverse Engineering Cursor's LLM Client June 5, 2025 · Viraj Mehta, Aaron Hill, Gabriel Bianconi What happens under the hood at Cursor? We wired TensorZero between Cursor and the LLMs to see every token fly by… and bend those API calls to our own will. TensorZero is an open-source framework that helps engineers optimize LLM applications with downstream feedback signals (e.g. production metrics, human feedback, user behavior), and we figured it would be interesting to see whether we could use TensorZero on the LLM application we use most heavily ourselves: Cursor. With our gateway between Cursor and the LLM providers, we can observe the LLM calls being made, run evaluations on individual inferences, use inference-time optimizations, and even experiment with and optimize the prompts and models that Cursor uses. Cursor is optimized for its population of users as a whole — beyond .cursorrules , is there room for improvement by diving deeper and tailoring it to individual users? What would it look like to be able to empirically experiment with and optimize Cursor for your individual usage patterns? Beyond optimization, Cursor still operates as a black box. Wouldn’t it be interesting to see what’s actually being sent to the models? Nearly all LLM optimization, evaluation, and experimentation techniques require data on what inferences were made and their real-world consequences. In this post, we’ll focus on the former and dive into how we set up TensorZero as a self-hosted proxy between Cursor and the LLMs that it calls. If you’d like to try it yourself, check out the example in our repository. We’ll follow up with a blog post on how we collect feedback and close the optimization loop. Wiring Things Together The first thing we noticed was that Cursor lets you override the OpenAI base URL and model names. Perfect. TensorZero exposes an OpenAI-compatible inference endpoint, so we can easily configure Cursor to call TensorZero instead of OpenAI. In the TensorZero configuration, we define a TensorZero function cursorzero so that we can automatically experiment with different models and prompts while storing provider-agnostic inference and feedback data in our database for observability and optimization. The First Roadblock: Cursor’s Servers Cursor was initially unable to connect to TensorZero running locally. It turns out that Cursor first sends a request to its own servers, where additional processing happens before making the LLM call, so it couldn’t connect to our gateway on localhost . (This also means that your credentials must be forwarded to Cursor’s servers, allowing them to collect data on your inferences and codebase.) As a test that our plan could work in the first place, we pointed Cursor to OpenRouter and realized we could use its models for the Ask, Agent, and Cmd+K interactions in Cursor. We were also still able to use the normal Cursor Tab completions, which rely on a proprietary model Cursor serves for inline completions (it is very good, so we’re glad to be able to keep it). The solution was to set up a reverse proxy to expose a public endpoint that would forward requests back to our machine. We used Ngrok to keep things simple. Since we were exposing a gateway with LLM credentials to the public Internet, we added Nginx in front of our gateway to authenticate requests. We set Cursor’s base URL to our Ngrok endpoint, added the credentials we set up in Nginx, turned off built-in models, and finally added our new TensorZero function under the model name tensorzero::function_name::cursorzero . In the end, the workflow looked like this: Cursor → Ngrok → Nginx (self-hosted) → TensorZero (self-hosted) → LLM providers But it didn’t work. The Second Roadblock: CORS The authentication process had failed. Nginx logs showed that there was an OPTIONS request hitting our endpoint, so we configured Nginx to return headers on OPTIONS requests and incrementally added headers we saw in responses from the OpenAI API. This is the initial verification request that comes from the local Cursor IDE. The CORS requirement likely comes from Electron. After the initial verification, all requests come from Cursor’s servers. Our Nginx Configuration to handle CORS headers # --- CORS helper macro --- set $CORS_ALLOW_ORIGIN $http_origin; # reflect the caller's origin set $CORS_ALLOW_HEADERS \"Authorization,Content-Type\"; location / { # --- pre-flight --- if ($request_method = OPTIONS) { add_header Access-Control-Allow-Origin $CORS_ALLOW_ORIGIN always; add_header Access-Control-Allow-Credentials \"true\" always; add_header Access-Control-Allow-Methods \"GET,POST,OPTIONS\" always; add_header Access-Control-Allow-Headers $CORS_ALLOW_HEADERS always; add_header Access-Control-Max-Age 86400 always; return 204; # 204 (No Content) is conventional for pre-flight } } You can find the entire codebase for “CursorZero” on GitHub . It finally worked! Finally: Observability for Cursor We could finally see everything coming in and out of Cursor — including its prompts. For example, here’s what we saw after asking Cursor “how do I get nginx to get the key from an environment variable in my .env?” System Prompt You are a an AI coding assistant, powered by tensorzero::function_name::cursorzero. You operate in Cursor You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide. Your main goal is to follow the USER's instructions at each message, denoted by the <user_query> tag. <communication> When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use ( and ) for inline math, [ and ] for block math. </communication> <search_and_reading> If you are unsure about the answer to the USER's request or how to satiate their request, you should gather more information. This can be done by asking the USER for more information. Bias towards not asking the user for help if you can find the answer yourself. </search_and_reading> <making_code_changes> The user is likely just asking questions and not looking for edits. Only suggest edits if you are certain that the user is looking for edits. When the user is asking for edits to their code, please output a simplified version of the code block that highlights the changes necessary and adds comments to indicate where unchanged code has been skipped. For example: ```language:path/to/file // ... existing code ... {{ edit_1 }} // ... existing code ... {{ edit_2 }} // ... existing code ... ``` The user can see the entire file, so they prefer to only read the updates to the code. Often this will mean that the start/end of the file will be skipped, but that's okay! Rewrite the entire file only if specifically requested. Always provide a brief explanation of the updates, unless the user specifically requests only the code. These edit codeblocks are also read by a less intelligent language model, colloquially called the apply model, to update the file. To help specify the edit to the apply model, you will be very careful when generating the codeblock to not introduce ambiguity. You will specify all unchanged regions (code and comments) of the file with \"// ... existing code ...\" comment markers. This will ensure the apply model will not delete existing unchanged code or comments when editing the file. You will not mention the apply model. </making_code_changes> <user_info> The user's OS version is darwin 24.3.0. The absolute path of the user's workspace is /Users/viraj/tensorzero/tensorzero/examples/cursorzero. The user's shell is /bin/zsh. </user_info> You MUST use the following format when citing code regions or blocks: ```12:15:app/components/Todo.tsx // ... existing code ... ``` This is the ONLY acceptable format for code citations. The format is ```startLine:endLine:filepath``` where startLine and endLine are line numbers. User Prompt <additional_data> Below are some potentially helpful/relevant pieces of information for figuring out to respond <current_file> Path: nginx/nginx.conf Line: 1 Line Content: `events {}` </current_file> <attached_files> <file_contents> ```path=nginx/nginx.conf, lines=1-46 events {} http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\"'; access_log /var/log/nginx/access.log main; map $http_authorization $is_authorized { default 0; \"~*^Bearer your-token-here$\" 1; } server { listen 80; # --- CORS helper macro --- set $CORS_ALLOW_ORIGIN \"$http_origin\"; # reflect the caller's origin set $CORS_ALLOW_HEADERS \"Authorization,Content-Type\"; location / { # --- pre-flight --- if ($request_method = OPTIONS) { add_header Access-Control-Allow-Origin \"$CORS_ALLOW_ORIGIN\" always; add_header Access-Control-Allow-Credentials \"true\" always; add_header Access-Control-Allow-Methods \"GET,POST,OPTIONS\" always; add_header Access-Control-Allow-Headers \"$CORS_ALLOW_HEADERS\" always; add_header Access-Control-Max-Age 86400 always; return 204; # 204 No Content is conventional for pre‑flight } # --- authenticated requests --- if ($is_authorized = 0) { return 401; } proxy_pass http://gateway:3000; proxy_set_header Host $host; # expose CORS headers on the real response too add_header Access-Control-Allow-Origin \"$CORS_ALLOW_ORIGIN\" always; add_header Access-Control-Allow-Credentials \"true\" always; } } } ``` </file_contents> <manually_added_selection> ```path=nginx/nginx.conf, lines=1-16 events {} http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\"'; access_log /var/log/nginx/access.log main; map $http_authorization $is_authorized { default 0; \"~*^Bearer your-token-here$\" 1; } server { listen 80; ``` </manually_added_selection> </attached_files> </additional_data> <user_query> how do I get nginx to get the key from an environment variable in my .env? </user_query> Response I see you're trying to use an environment variable for your authentication token in the Nginx configuration instead of hardcoding \"your-token-here\" in the `nginx.conf` file. Unfortunately, Nginx doesn't directly read environment variables from a `.env` file. However, there are a few approaches you can take: ### Option 1: Use Nginx's environment variable support with a template 1. Create a template file (e.g., `nginx.conf.template`) with a placeholder: ``` map $http_authorization $is_authorized { default 0; \"~*^Bearer ${API_TOKEN}$\" 1; } ``` 2. Then use a script to replace the placeholder with the actual environment variable before starting Nginx: ```bash #!/bin/bash # Load environment variables from .env source .env # Replace placeholder with actual token envsubst '${API_TOKEN}$' < /path/to/nginx.conf.template > /path/to/nginx.conf # Start Nginx nginx -g 'daemon off;' ``` ### Option 2: Use Docker environment variables (if using Docker) If you're running Nginx in Docker, you can pass environment variables to the container and use `envsubst`: ```dockerfile FROM nginx:alpine COPY nginx.conf.template /etc/nginx/templates/default.conf.template CMD [\"/bin/sh\", \"-c\", \"envsubst '${API_TOKEN}$' < /etc/nginx/templates/default.conf.template > /etc/nginx/conf.d/default.conf && nginx -g 'daemon off;'\"] ``` Then run with: ```bash docker run -e API_TOKEN=your-secret-token -p 80:80 your-nginx-image ``` ### Option 3: Use a separate authentication service Instead of hardcoding the token in Nginx, you could proxy authentication requests to a separate service that has access to environment variables: ``` location /auth { proxy_pass http://auth-service:3001; # This service can read environment variables } ``` For your specific case, I'd recommend Option 1 or 2 depending on your deployment setup. This way, you can keep your token in your `.env` file and have it securely injected into the Nginx configuration at startup time. Prompt Engineering Cursor This example paints a picture of what prompt engineering looks like for a state-of-the-art AI coding assistant. This prompt and others like it (there were different prompts used for asking questions, editing source code, and working in the shell) are all Cursor uses to answer questions and generate code. We find it remarkable that there isn’t more here and that all of software engineering has been internalized enough by the current generation of LLMs that you can get the brains behind Cursor with a 642-token system prompt. This must be due to extensive post-training efforts by the big labs. An Explicit AI Hierarchy? We find the following snippet particularly intriguing: These edit codeblocks are also read by a less intelligent language model, colloquially called the apply model, to update the file. To help specify the edit to the apply model, you will [...]. You will not mention the apply model. It’s common to mix different models to optimize cost and latency, but Cursor explains this hierarchy to the models themselves? Interesting. A/B Testing Models With TensorZero in place, we have the full Cursor experience with control over observability and experimentation of our LLM requests. We’ve been running CursorZero for days of heavy software engineering: it’s been stable and there has been no noticeable additional latency. We’re currently running an even random split of Claude 4.0 Sonnet, GPT-4.1, o4 Mini, and Gemini 2.5 Pro — and it feels as good as Cursor ever has. Try It Yourself Interested in analyzing your own usage of AI coding assistants? You can find instructions on GitHub to reproduce this work with Cursor and OpenAI Codex . TensorZero helps you understand — and optimize — your LLM agents, even if you didn’t build them yourself! But Wait… There’s More! This blog post demonstrates how we successfully reverse-engineered Cursor’s LLM client by setting up TensorZero as a self-hosted proxy service, enabling us to observe, analyze, and experiment with different LLM models while maintaining the full Cursor experience. In our next post of this series, we’ll explain how we’re evaluating real-world usage of AI coding assistants, along with the results of our internal A/B testing between models. Sneak peek: git hooks, tree-sitter, and more. Then, we’ll explore if this feedback signal might be able to improve Cursor by optimizing models and inferences through your individual usage patterns. Stay tuned by subscribing below. Start building today. Check out our GitHub and Quick Start .",
      "summary": "17-minute read",
      "url": "https://www.tensorzero.com/blog/reverse-engineering-cursors-llm-client/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 2185,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 2185/3400 words (64.3%)"
    },
    {
      "title": "Meta reportedly in talks to invest billions of dollars in Scale AI",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
      "summary": "1-minute read",
      "url": "https://techcrunch.com/2025/06/08/meta-reportedly-in-talks-to-invest-billions-of-dollars-in-scale-ai/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Specification Engineering",
      "content": "Specification Engineering reading time: 2.90 mins published: 2025-03-18 updated: 2025-03-29 ... is a bet on better code gen and more complexity AI engineering wants to be declarative. Ultimately, the algorithms in language model attention heads which devs interleave with their Python and Typescript are fuzzy and inscrutable. Their natural abstractions are guarantees, predicates on co-images. We can’t know what they do, only what they will have done. This is fine when AI software is 99% Python logic and 1% AI. Most software accesses SQL databases, but that’s not a problem - the interaction is tightly scoped and managed. But what happens when the software is 99% AI and 1% Python, at least in terms of complexity/headspace? At that point, the software is poorly served by the imperative paradigm. There may be plenty of LoC, but as far as it’s owner is concerned, there’s nothing imperative about it. At that point, much of the actual logic of the program lives in … the heads of its developers. The prompts won’t speak for themselves - each maintainer will, as a byproduct of hours of whiteboarding and painful trial-and-error, have reams and reams of knowledge regarding how different AI components interrelate with one another that simply cannot be safely deduced from the code itself, possibly even by a superintelligence. Storing critical system information solely in human minds - and doing so more often as time goes on and AI becomes a bigger part of software - is not a good idea. The I/O bandwidth is low, the information degrades quickly, and collaboration scales poorly. Its a structural trend directly counter to the massive productivity gains the rest of software is seeing - and it’s holding AI software development back. Evals can sometimes help introduce some structure and legibility, but they’re too fragmented - the requirements your engineers care about are distributed across hundreds of test cases you will never read and likely struggle to version and update. As time goes on, teams and engineers will want AI systems like Synth to help them - and, to be most effective in controlling and intervening on the software, those systems will need a legible and durable source of truth. Finding the right abstractions will take time, but now is the time to start. Every abstraction is leaky, and so directly maintaining imperative Python in AI software will be a necessity for the foreseeable future. But, great engineering teams will use processes and tools to ensure that the system specification is syncronized and takes precedence. PRs and prompt updates can be compiled up into spec diffs, and rejected if they introduce breaking changes. Syncronization in the other direction is where the abstraction starts paying for itself. Add a requirement -> AI spools up 1k LoC and Synth stress-tests two new prompts and a sub-agent, with 5 new evals to boot. Evals just become a way to check guarantees and create impetus for the compiler to update prompts/code/LoRAs. Naturally, syncronization will sometimes go both ways. Adding a better model might require simpler code with fewer prompts, depending on how preferences are outlined, and so we might go models -> evals -> spec -> code -> evals -> spec. Suddenly equilibrium becomes a more apt description than compilation. But don’t let that scare you away. Declarative specs with guarantees aren’t new . They’ve been used as long as software’s been written. Engineering teams benefit greatly from clearly communicating system level guarantees, and maintained them even back when it took precious human-hours to do so. What is new is intelligence that helps us consistently and cheaply transpile between them and software in a git commit hook. Let’s use it.",
      "summary": "3-minute read",
      "url": "https://www.joshuapurtell.com/posts/spec_eng/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 610,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Maintaining an Android app is a lot of work",
      "content": "There was recent news about 47% decline in the number of apps on Google Play Store. As a hobby Android developer, who has been developing MusicSync , a Google Play Music + Podcast replacement for the last five years, I thought I would share my experience of maintaining an Android app. And why this reduction in the number of apps is not surprising to me. I have several side-projects that run on a backend server with a limited web UI, and it is much less effort to maintain them. However, maintaining an Android app as a side-project is a more involved affair. And here are some of the problems I have faced. Java vs Kotlin # Kotlin is clearly the preferred language of development if you are starting a new Android project in 2025. But what if you are maintaining a hobby project written in Java? You will start seeing incompatibility when your dependencies are re-written in Kotlin. If you depend on a library that uses Kotlin’s coroutines or relies on Kotlin’s suspend functions , then you will have to work around it, or rewrite your app in Kotlin as well! Jetpack Compose, an official Google UI library for Android is entirely unusable from Java. I would imagine that if you started with Kotlin first then a big chunk of StackOverflow questions written for Java audiences require you translate them to corresponding Kotlin code as well To their credit, Android documentation still gives code samples in both Java and Kotlin. Google makes breaking changes to its libraries # Google has a habit of making breaking changes to its Android libraries. Here’s a list of some of the libraries that I have used in my app and the issues I have faced. Media 3 # Android ships with MediaPlayer . Google recommends its open-source library ExoPlayer . ExoPlayer V1 was last released in 2017. It was replaced with backward-incompatible ExoPlayer V2 which was last released in July 2024. And now, it has now been replaced with backward-incompatible media3 . The Google provided migration script is far from being complete. Further, media3 does not follow semantic versioning, minor version upgrades has resulted in breaking API changes. Google Auth library # Google’s own Auth library had a bug and sign-in was broken for API 26 and lower for months . Java 1 2 3 4 5 6 7 8 9 10 java.lang.NoSuchMethodError: No virtual method getAndSetObject (Ljava / lang / Object;JLjava / lang / Object;)Ljava / lang / Object; in class Lsun / misc / Unsafe; or its super classes (declaration of 'sun.misc.Unsafe' appears in / system / framework / core - libart.jar) E at com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper.gasWaiters(AbstractFuture.java:1394) E at com.google.common.util.concurrent.AbstractFuture.releaseWaiters(AbstractFuture.java:1110) E at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1000) E at com.google.common.util.concurrent.AbstractFuture.set(AbstractFuture.java:783) E at com.google.auth.oauth2.OAuth2Credentials$RefreshTask.access$400(OAuth2Credentials.java:600) E at com.google.auth.oauth2.OAuth2Credentials$RefreshTask$1.onSuccess(OAuth2Credentials.java:617) ... Dropping support for older Android versions # Google Ads library v24 dropped support for Android API 21. According to official Google statistics, API 21 is used by 0.1% (~4 million) users. The rationale behind this has been left unexplained. Upgrades for the sake of it # Material 2 was deprecated for Material 3. No clear migration guide was provided. I tried to upgrade and some components like Sliders won’t look good. Why? I don’t know, and I was never able to figure out the mystic. It does not help that most documentation now refers to Jetpack Compose which I cannot use! So, for the near term, Java-based codebase are likely stuck with Material 2. The UI design guidelines for Android evolve unpredictably # Bottom bar, a featured popular on iOS was discouraged and then became a standard feature in Material design. Back and up buttons used to behave differently and now they are supposed to behave the same . I only learnt about it last year when I posted about it on Reddit. You might think that you can just use Material Design components and be done with it. But migrating from one version of Material Design to another is not trivial either. And before you migrate from Material 1 to Material 2, Google deprecates it for Material 3. Google makes breaking changes to Android platform # Every major release of Android makes breaking changes that requires developer effort Toasts use to work for quick notifications, now, after API 31, it only works if the app is foreground. How to know if you app in foreground? You have to use ActivityLifecycleCallbacks for that and write ton of code and even then there are confusions about onStart vs onResume . Displaying notifications didn’t require permissions, now after API 33, it requires POST_NOTIFICATIONS . Storage permissions were either all or none, now API 33 onwards , they can be fine-grained at the level of audio, video, and images. Background code execution restrictions keeps changing subtly in every release. Media notifications were changed in a backward-incompatible in API 33 onwards. This long thread explains the pain of a lot of developers. Crucial third-party libraries have been deprecated # Several popular third-party have been deprecated or are no longer maintained. Picasso # Picasso was great for image loading and has been deprecated . It has been replaced with coil but the upgrade is not trivial. Glide # Glide an alternative to Picasso was last released in Sep 2023. OkHttp # OkHttp which even Android uses internally for implementing HttpURLConnection has not seen a stable release since Oct 2023, the last stable release was 4.12.0 and even the last alpha release was in April 2024. OkHttp 4.12.0 does not support Happy Eyeballs which is a major issue with IPv6 networks. EventBus # EventBus was the de-facto event passing library for Android. And it is unmaintained now. RateThisApp # RateThisApp was good to get app ratings, and then it was abandoned. I don’t blame the maintainers here. If you use an open-source library, you have to be prepared for the fact that it may not be maintained. I am just pointing out, how some of the obvious boilerplate tasks that one requires for building an Android app are suddenly in a limbo. Two different versioning schemes for everything # Android has two versioning schemes , Android API version is for developers and Android version for marketing. For example, Android 11 is API 30, Android 12 is API 31 as well as 32(!), Android 13 is API 33, Android 14 is API 34. The developer documents would reference one scheme or the other or sometimes both! And you are supposed to memorize the mappings while trying to debug issues using GitHub issues or StackOverflow. It just adds unnecessary friction and confusion. Forced upgrades # There are multiple versions in an Android app, all tightly coupled with each other. minSdkVersion and targetSdkVersion of the app Java sourceCompatibility and targetCompatibility version of dependencies version of Android build tool chain version of Gradle version of Android Studio You might think that all updates are optional, but they aren’t Gradle and Android Studio must be upgraded together for version-compatibility Upgrading Java sourceCompatibility and targetCompatibility requires upgrading Gradle (and hence, Android Studio) Upgrading Android build tool chain requires upgrading minSdkVersion and targetSdkVersion Upgrade Android build tool chain requires upgrading Gradle version Also, if you want to stay on an old library like Exoplayer V2, sooner or later, it will become incompatible with other dependencies, and you will be forced to upgrade to media3! You see how you are forced to upgrade almost everything or nothing? And what if you decide to not upgrade any of these? Well, your app will get delisted if the minSdkVersion is too old. Conclusion # Compared to server-side development, Android development requires a bit more efforts to maintain. So, if you are planning to build an Android app as a hobby, keep the ongoing maintenance cost in mind. Update # After this article ended up on the front page of Hacker News , Two new items came out during those discussions that are worth mentioning. App published for the first time after 2021, have to hand over their private signing keys to Google Play Store Further, multiple people suggested that I should use F-droid to publish apps. The problem is that F-droid not only have very little reach but also it cannot solve for backward-incompatible changes to the underlying platform, abandoned libraries, and backward-incompatible changes to Android libraries.",
      "summary": "7-minute read",
      "url": "https://ashishb.net/programming/maintaining-android-app/?utm_source=tldrnewsletter",
      "published_date": "2025-06-09T00:00:00",
      "category": "ai",
      "word_count": 1380,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Google starts testing ‘Search Live' in AI Mode",
      "content": "Skip to main content Apps & Updates Google AI Mode Google starts testing ‘Search Live’ in AI Mode Abner Li | Jun 5 2025 - 7:35 pm PT After previewing at I/O 2025 , Google is beginning to test AI Mode’s new “Search Live” experience. Powered by Project Astra (just like Gemini Live ), it lets you have a real-time conversation with Google. If rolled out to you, the Google app will show a waveform badged by a sparkle underneath the Search bar. (That is curiously the same icon used by Gemini Live. As such, this must be Google’s icon for “Live” conversational experiences.) It replaces the left Google Lens shortcut that immediately opened your gallery/screenshots. Another way to launch Search Live is from the new circular button to the right of the text field in AI Mode conversations. The fullscreen interface has a light or dark background with the new ‘G’ logo in the top-left corner. There’s a curved waveform in the Google colors, while pill-shaped buttons let you “Mute” and get a “Transcript.” Currently, that second button just opens the AI Mode text chat (ending the Live conversation) instead of showing you real-time captions . Tap the three-dot overflow menu for Voice settings, with four options available: Cosmo, Neso, Terra, and Cassini. After you ask a question, Search Live will surface sites used to inform the answer with a scrollable carousel. Google can ask you clarifying questions to refine your query, while you’re free to ask follow-ups. You can exit the Google app and continue your conversation in the background.(The iOS app makes use of Live Activities.) As of today, Search Live’s camera capability that lets you stream video is not yet available. It’s similar to how Gemini Live first rolled out the voice experience before getting camera sharing. Search Live is currently rolling out to the Google app Android and iOS (we have it on the latter platform). Our account is enrolled in Search Labs . Add 9to5Google to your Google News feed. You’re reading 9to5Google — experts who break news about Google and its surrounding ecosystem, day after day. Be sure to check out our homepage for all the latest news, and follow 9to5Google on",
      "summary": "2-minute read",
      "url": "https://9to5google.com/2025/06/05/google-search-live-testing/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 369,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Amazon ‘testing humanoid robots to deliver packages'",
      "content": "Amazon is reportedly developing software for humanoid robots that could perform the role of delivery workers and “spring out” of its vans. The $2tn (£1.47tn) technology company is building a “humanoid park” in the US to test the robots, said the tech news site the Information, citing a person who had been involved in the project. The Information reported that the robots could eventually take the jobs of delivery workers. It is developing the artificial intelligence software that would power the robots but will use hardware developed by other companies. The indoor obstacle course being used for the tests at an Amazon office in San Francisco is about the size of a coffee shop, the report said, with the company hoping the robots will be able to travel in Amazon’s Rivian vans and make deliveries from them. Even with a human driver behind the wheel, a robot could theoretically speed up drop-off times by visiting one address while the human employee delivers to another. Amazon also has an interest in self-driving vehicles through its Zoox unit. Amazon has more than 20,000 Rivians in the US and has placed one of the vans in the humanoid testing zone, according to the report. Once it has tested the robots in the humanoid park, it plans to take them on “field trips” in the real world where they will attempt to deliver packages to homes. Amazon has already conducted trials with humanoid robots, deploying devices developed by the US company Agility Robotics in its warehouses. The chief executive of Agility, Peggy Johnson, told the Guardian last year that the company’s Digit robot allowed employees to hand off work to humanoids and become a “robot manager”. Last year Amazon was given permission to test-fly drones beyond a human controller’s line of sight in the UK, paving the way for using the technology in home delivery . Prof Subramanian Ramamoorthy, the chair of robot learning and autonomy at the University of Edinburgh, said Amazon had a respected robotics team and its reported focus on “last mile” delivery was not a surprise. The humanoid robot hardware capable of carrying out such a task is becoming available and the field is developing rapidly, he said. However, he added, the challenge was to achieve reliable performance outside highly constrained environments such as the reported “humanoid park”. skip past",
      "summary": "3-minute read",
      "url": "https://www.theguardian.com/technology/2025/jun/05/amazon-testing-humanoid-robots-to-deliver-packages?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 390,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 390/600 words (65.0%)"
    },
    {
      "title": "Walmart and Wing expand drone delivery to five more US cities",
      "content": "Wing, the on-demand drone delivery company owned by Alphabet, is spreading its commercial wings with help from Walmart. The two companies announced Thursday plans to roll out drone delivery to more than 100 Walmart stores in five new cities: Atlanta, Charlotte, Houston, Orlando, and Tampa. Walmart is also adding Wing drone deliveries to its existing market in the Dallas-Fort Worth area. The expansion signals Walmart’s growing confidence in drone delivery. Greg Cathey, who is senior vice president of Walmart’s U.S. Transformation and Innovation department, said drone delivery would remain a key part of its “commitment to redefining retail.” “We’re pushing the boundaries of convenience to better serve our customers, making shopping faster and easier than ever before,” Cathey said in a blog posted Thursday. The expansion also marks a turning point for Wing, from Alphabet X graduate to commercial enterprise. Wing partnered with Walmart in 2023 and launched a pilot program to test on-demand drone delivery at two stores in the Dallas metro area that reached about 60,000 homes. It has since grown to 18 Walmart Supercenters in Dallas-Fort Worth. The expansion announced Thursday is nearly a five-fold increase of Wing’s operations with Walmart. “We’re decidedly out of the pilot and trial phase and into scaling up this business,” Wing CEO Adam Woodworth told TechCrunch in a recent interview. “We’ve always been the type of company that wants to do something well and stay focused. And so this is the next big bite at the apple. It’s a much bigger bite than we’ve taken before.” Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW Woodworth said the pilot program in Dallas-Fort Worth, and specifically how it scaled, helped form Wing’s drone delivery strategy in the retail sector. “We figured out how the expansion worked out and looked in DFW, and now we’re sort of copy-pasting that across more markets,” he added. Woodworth wouldn’t say whether Wing was profitable yet or when it would be. But he did say the company is focused on how to scale its deliveries while keeping its expenses in check. Wing’s hypothesis is to build a business centered on small, lightweight, automated, low-cost airplanes — aka drones. There are fixed operational costs tied to those physical assets such as flight operations and training. The crux, and what Wing is trying to navigate, is how to scale the number of drones and flights without adding even more personnel. “The more places you can be operating, the more you can be flying, the more you can defray those costs. This is a meaningful step in that direction,” he said, adding that Wing is trying to keep its resources flat as the scale continues to go up. Wing is also pushing into the restaurant food delivery sector through its partnership with DoorDash . The two companies paired up in 2022 to launch drone deliveries in Australia and have since worked together in Dallas-Forth Worth and more recently in Charlotte .",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/05/walmart-and-wing-expand-drone-delivery-to-five-more-u-s-cities/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 563,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Longevity Is Now a Factor When Picking an Embryo for IVF",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/health/embryo-ivf-screening-longevity-2b1f096a?st=WfueaT&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.wsj.com/health/embryo-ivf-screening-longevity-2b1f096a?st=WfueaT&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "science",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Google rolling out upgraded Gemini 2.5 Pro preview",
      "content": "Skip to main content Apps & Updates Gemini Google rolling out upgraded Gemini 2.5 Pro preview Abner Li | Jun 5 2025 - 9:01 am PT Google today released an “ upgraded preview ” of Gemini 2.5 Pro that will soon be generally available in the coming weeks. This model update builds off 2.5 Pro’s I/O Edition (05-06) last month that brought coding upgrades. That coding lead continues with 06-05 today across benchmarks like AIDER Polyglot, while there’s “top-tier performance on GPQA and Humanity’s Last Exam (HLE), highly challenging benchmarks that evaluate a model’s math, science, knowledge and reasoning capabilities.” Meanwhile, Google notes how 06-05 today has a “24-point Elo score jump on LMArena” at 1470 today, while there’s a “35-point Elo jump to lead on WebDevArena at 1443.” With the last few model upgrades, some people noticed performance declines outside of coding, especially compared to 03-25 . Google is now addressing that feedback with improved “style and structure,” so that Gemini 2.5 Pro “can be more creative with better-formatted responses.” This Gemini 2.5 Pro model update is still in preview, but you can expect general availability and a stable version “in a couple of weeks.” It is rolling out now to the Gemini API via Google AI Studio and Vertex AI, with developers able to control cost and latency with thinking budgets (as introduced with 2.5 Flash ). The Gemini app is also getting this 2.5 Pro preview upgrade today. L-R: 06-05, 05-06, 03-25 More on Gemini: Gemini app doubles recently introduced 2.5 Pro query limit Google rolls out Gemini Live camera and screen sharing to everyone Gemini app ‘Pro’ users getting Veo 3 trial on Android & iOS, AI Ultra now in 73 countries Add 9to5Google to your Google News feed. You’re reading 9to5Google — experts who break news about Google and its surrounding ecosystem, day after day. Be sure to check out our homepage for all the latest news, and follow 9to5Google on",
      "summary": "1-minute read",
      "url": "https://9to5google.com/2025/06/05/gemini-2-5-pro-preview-06-05/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 326,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "AMC Networks Inks Deal With AI Company Runway",
      "content": "By Steven Zeitchik Plus Icon Steven Zeitchik Senior Editor, Technology and Politics View All June 4, 2025 6:00am Scott M. Gimple, David Zabel, Greg Nicotero and Norman Reedus speak onstage at AMC and AMC+ Present The Walking Dead: Daryl Dixon at Jacob Javits Center on October 12, 2023 in New York City. The company is placing its chips on AI in a new deal with upstart Runway Photo by Jason Mendez/Getty Images for AMC AMC Networks is the latest entertainment entity — and the first cable player — to formally make a deal with emerging Hollywood AI player Runway, THR has learned. AMC will use the New York firm’s tech both to generate marketing images and help pre-visualize shows that have yet to be produced. “It’s kind of a natural transition for every entertainment and media company; they need to think through what AI means for them,” Runway co-founder Cristóbal Valenzuela told THR when asked about the AMC partnership. “This is the most important tech of their lifetime — it’s a make-or-break moment.” Related Stories Movies A New York Startup Just Threw a Splashy Event to Hail the Future of AI Movies TV 'The Summer I Turned Pretty' Season 3 Trailer Sees Lola Tung's Belly Getting Married With AMC and BBC America in its portfolio along with streaming services Shudder, Acorn TV and AMC+, AMC Networks has a notable range of platforms, in addition of course to bringing to the world Peak TV mainstays like Mad Men , The Walking Dead and Breaking Bad to the world. But with cord-cutting long a fact of life and even far larger streamers struggling for mindshare, the company has been hit hard. In its most recent earnings report, profits at AMC Networks nearly halved to $63 million, and the company is believed to be looking for ways to produce content less expensively without suffering a drop in quality. Still, AMC’s streaming services boasts more than ten million",
      "summary": "3-minute read",
      "url": "https://www.hollywoodreporter.com/business/business-news/amc-networks-runway-ai-deal-1236255108/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 324,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 324/600 words (54.0%)"
    },
    {
      "title": "Google's Chrome Browser Gets 'Highest Score Ever' on Speedometer Performance Test",
      "content": "Google's Chrome Browser Gets 'Highest Score Ever' on Speedometer Performance Test Thursday June 5, 2025 3:02 pm PDT by Juli Clover Google today announced that its Chrome browser has received the \"highest ever score\" on the Speedometer 3 benchmarking test. Speedometer 3 is designed to measure browser performance, and it was created collaboratively by Google, Apple, Intel, Microsoft, and Mozilla. The test is designed to measure web responsiveness through several workloads like HTML parsing, JavaScript and JSON processing, pixel rendering, CSS application, and more. Optimizations that Google has implemented over the last year have brought a 10 percent improvement in performance since August 2024, which Google says leads to better browser experiences for end users. The team heavily optimized memory layouts of many internal data structures across DOM, CSS, layout, and painting components. Blink now avoids a lot of useless churn on system memory by keeping state where it belongs with respect to access patterns, maximizing utilization of CPU caches. Where internal memory was already relying on garbage collection in Oilpan, e.g. DOM, the usage was expanded by converting types from using malloc to Oilpan. This generally speeds up the affected areas as it packs memory nicely in Oilpan's backend. On an M4 MacBook Pro with macOS 15, Chrome 139 achieved a score of 52.35 on the benchmarking test. More detail on the optimizations that were added can be found in Google's blog post . Apple has not recently shared its maximum Speedometer 3 test results for Safari so there isn't a direct comparison available, and it is worth noting that Google appears to have used Speedometer 3, and not the newer Speedometer 3.1 test. [ 81 comments ] Get weekly top MacRumors stories in your inbox. Popular Stories Monday June 9, 2025 9:00 am PDT by Jordan Golson Apple's Worldwide Developers Conference (WWDC) starts today with the traditional keynote kicking things off at 10:00 a.m. Pacific Time. MacRumors is on hand for the event and we'll be sharing details and our thoughts throughout the day. We're expecting to see a number of software-related announcements led by a design revamp across Apple's platforms that will also see the numbering of all of... • 1212 comments Monday June 9, 2025 5:21 pm PDT by Juli Clover At today's WWDC 2025 keynote event, Apple unveiled a new design that will inform the next decade of iOS, iPadOS, and macOS development, so needless to say, it was a busy day. Apple also unveiled a ton of new features for the iPhone, an overhauled Spotlight interface for the Mac, and a ton of updates that make the iPad more like a Mac than ever before.",
      "summary": "2-minute read",
      "url": "https://www.macrumors.com/2025/06/05/chrome-speedometer-improvements/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 441,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "The founder of DeviantArt is making a $22,000 display for digital art",
      "content": "Angelo Sotira started the online digital art platform DeviantArt when he was just a teenager, growing a formative community for millions of artists in the 2000s. Twenty-five years later, Sotira wants to change digital art again, but with a focus on the way it’s displayed. On Thursday, Sotira revealed his new venture, Layer , a screen specifically designed to showcase digital art in the best quality possible. “The way that the canvas needs to perform and behave in your life is quite different than other types of displays,” Sotira told TechCrunch. “It needs to blend into beautiful environments.” The closest point of reference that the average consumer would have for a product like this is Samsung’s The Frame TV, which looks like a painting hung on the wall when it’s not turned on. But Layer takes that kind of feel to an even more premium level — unlike The Frame, Layer is not a consumer product, and it’s not trying to emulate static paintings or photographs. “They’re $22,000, so that kind of tells you a lot about who that’s for,” Sotira said. “We spared no expense and we spared no effort. We made no compromise in producing what is actually, in our opinion, the very best way to display digital art on a wall.” Image Credits: Layer When Sotira talks about digital art, he isn’t talking about digital photography or videos. Layer is working with hundreds of artists like Casey Reas , who makes generative art — no, not the kind of generative AI art like you’d get from ChatGPT, which is created with LLMs that use other artists’ work without their consent. Instead, many of these artists are writing their own software to create digital artworks that change over time according to what the code says. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW But these artworks can require a lot of computing power to execute. That’s part of why Layer is so pricey — it needs the technological capacity to display these new kinds of works. “You’re looking at an over 35-year history of extraordinary artists developing the medium of code-based art and essentially, the pixels on the display are being governed by the code that’s been written that runs live on that GPU, rendering it in full resolution,” Sotira said. “It’s actually controlling every pixel, so it’s not going through any compression algorithms.” Sotira is well aware that he’s not the first entrepreneur to try to create a better way to display digital art — when he was at DeviantArt, he was pitched on products like Layer all the time. But because of this, he knows what was missing from the products that were pitched to him in the past. “One of the driving principles is that you can plug it in, turn it on, and leave it alone, and it should know how to sequence art for you,” he said. In his experience, he enjoys tinkering with these devices for a few weeks, but then it becomes tedious to continue updating the display, so he wanted his own canvas to be more self-sustaining. “It’s going to be on your wall for five years, so it has to play really, really well in your life.” Image Credits: Layer Layer seems like a highly expensive and very niche product, but some venture capitalists and entrepreneurs are betting on it. While in stealth, the startup raised $5.7 million in funding from Expa Ventures, Human Ventures, and Slauson & Co., plus angels like",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/05/the-founder-of-deviantart-is-making-a-22000-display-for-digital-art/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 656,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple says its App Store helped generate $1.3T in billings and sales, most without a commission",
      "content": "Ahead of its Worldwide Developers Conference on June 9, Apple on Thursday offered a new update on its global App Store business, sharing that developers generated $1.3 trillion in billings and sales in 2024. The company stressed that 90% of those billings and sales did not involve paying Apple a commission. The study also noted that developer billings and sales for digital goods and services in 2024 totaled $131 billion, driven by mobile games, photo and video editing apps, and other enterprise tools. Physical goods and services, meanwhile, topped $1 trillion, thanks to increased demand for online food delivery and pickup, and online grocery apps. In-app advertising revenue was $150 billion last year. Spending across digital goods and services, physical goods and services, and in-app advertising has more than doubled since 2019, with physical goods and services seeing the strongest growth at more than 2.6 times, Apple said. The numbers are intended to highlight how the App Store creates financial opportunities for mobile developers that extend beyond sales from in-app purchases. The storefront provides a place for developers to have their apps discovered by consumers, and Apple provides the technical infrastructure required to run an app business. This position ignores the fact that the App Store is now a mature ecosystem, and apps are a selling point for the iPhone itself. Developers today have a number of tools at their disposal to host, distribute, and manage their own applications, if they choose, but Apple’s policies prevent this. That’s starting to change, however. In a recent court ruling in favor of Epic Games in the U.S., Apple was required to let developers link to their own websites for processing in-app purchases without having to pay Apple a commission. In Europe, the tech giant is fighting against the rules proposed by the Digital Markets Act (DMA), which, among other things, directs Apple to give developers the right to inform their customers about alternative payment mechanisms. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The new data comes from an Apple-funded study by economists Professor Andrey Fradkin from the Boston University Questrom School of Business and Dr. Jessica Burley from Analysis Group. The latter has been working with Apple for years throughout its antitrust legal battles to document the App Store’s success in a more positive light for the company. The study highlights other regional growth trends, like how the billings and sales facilitated by the App Store more than doubled over the past five years in the U.S., China, and Europe. Digital payment spending, meanwhile, also grew by more than seven times in the U.S. since 2019, thanks to the broad adoption of mobile payments. The report also reiterated other metrics, like how the App Store attracts 813 million average weekly visitors worldwide, and pointed to the various investments Apple has made in tools and technologies to support developers, like coding and distribution platforms, frameworks, analytics, anti-fraud systems, developer support, and more.",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/05/apple-says-its-app-store-helped-generate-1-3t-in-billings-and-sales-most-without-a-commission/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 563,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Perplexity received 780 million queries last month, CEO says",
      "content": "Perplexity received 780 million queries in May, CEO Aravind Srinivas shared onstage at Bloomberg’s Tech Summit on Thursday. Srinivas said that the AI search engine is seeing more than 20% growth month-over-month. “Give it a year, we’ll be doing, like, a billion queries a week if we can sustain this growth rate,” Srinivas said. “And that’s pretty impressive because the first day in 2022, we did 3,000 queries, just one single day. So from there to doing 30 million queries a day now, it’s been phenomenal growth.” Srinivas went on to note that the same growth trajectory is possible, especially with the new Comet browser that it’s working on. “If people are in the browser, it’s infinite retention,” he said. “Everything in the search bar, everything on the new tab page, everything you’re doing on the sidecar, any of the pages you’re in, these are all going to be extra queries per active user, as well as seeking new users who just are tired of legacy browsers, like Chrome. I think that’s going to be the way to grow over the coming year.” Srinivas said the reason Perplexity is developing Comet is to shift the role of AI from simply providing answers to actually completing actions on your behalf. He explained that when you get an AI-powered answer, it’s essentially four or five searches in one. On the other hand, AI performing an action would be getting an entire browsing session done with one prompt. “You really need to actually have a browser and hybridize the compute on the client and the server side in the most seamless way possible,” he said. “And that calls for rethinking the whole browser.” He went on to explain that Perplexity isn’t thinking of Comet as “yet another browser,” but as a “cognitive operating system.” Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW “It’ll be there for you every time, anytime, for work or life, as a system on the side, or like, just going and doing browsing sessions for you,” Srinivas said. “And I think that’ll fundamentally make us rethink how we even think about the internet. Like, earlier we would browse the internet, but now people are increasingly living on the internet. Like a lot of our life actually exists there. And if you want to build a proactive, personalized AI, it needs to live together with you, and that’s why we need to rethink the browser entirely.” While the company hasn’t revealed too much about the browser, Srinivas said in April that one reason Perplexity is developing its own browser is to track user activity beyond its own app so that it can sell premium ads, which would essentially mirror what Google quietly did to become the giant it is today. It’s currently unknown when exactly Comet will launch, but Srinivas previously said on X that it will launch in the coming weeks. Comet will have a native virtual meets recording, transcription and searches over them. Won’t be part of the first release, but very fast follow up. As for release date: it’s going to take a min of three weeks and a max of five weeks. Reliability and latency have improved over… — Aravind Srinivas (@AravSrinivas) May 13, 2025",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/05/perplexity-received-780-million-queries-last-month-ceo-says/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 612,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "What a developer needs to know about SCIM",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~3800 words, got 256. URL: https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim?utm_source=tldrnewsletter",
      "summary": "19-minute read",
      "url": "https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "ai",
      "word_count": 256,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 256/3800 words (6.7%)"
    },
    {
      "title": "X Is Testing a Way To Take Community Notes to the Next Level",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.adweek.com/media/x-is-testing-a-way-to-take-community-notes-to-the-next-level/?utm_source=tldrnewsletter",
      "summary": "3-minute read",
      "url": "https://www.adweek.com/media/x-is-testing-a-way-to-take-community-notes-to-the-next-level/?utm_source=tldrnewsletter",
      "published_date": "2025-06-06T00:00:00",
      "category": "dev",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "AirPods to get camera control, sleep detection, new gestures, more",
      "content": "Skip to main content AirPods Exclusive Exclusive: AirPods to get camera control, sleep detection, new gestures, more Marcus Mendes | Jun 4 2025 - 12:31 pm PT Yesterday, 9to5Mac exclusively reported a handful of iOS 26 features that Apple has been working on and may announce on WWDC for its native apps. Today, we are sharing five previously unreported AirPods features that may also be announced as early as Monday, during the keynote. Control AirPods with head gestures Last year, Apple introduced head gestures so AirPods Pro 2 and AirPods 4 users can nod “up and down or side to side to accept or decline Announce Calls and to interact with or dismiss Announce Notifications without using your hands”. 9to5Mac has learned that Apple is working on new head gestures, including extending a Conversation Awareness volume adjustment before returning users to their original noise-control settings. Currently, you can end a Conversation Awareness volume adjustment by “pressing and holding on the AirPods stem or swiping up on the stem”, but soon, a head gesture might also do the trick. Sleep auto-pause Many people tend to fall asleep with their AirPods on. Some doze off while listening to music, and some do so while listening to podcasts. Currently, users must resort to sleep timers, a feature not all music or podcast apps support. Luckily, our sources say Apple has been working on detecting when a user has fallen asleep while wearing their AirPods and automatically pausing playback. It is not clear whether this will be a standalone feature or if it will work in tandem with the Apple Watch, which already has sleep detection. Still, it’ll sure be a welcome feature. Camera control Years later than anyone would have initially guessed, Apple is finally poised to allow for AirPods Camera Control. Like with the old EarBuds, this new feature will let users trigger the iPhone or iPad camera with a click on the AirPods stem. While touching one’s ear in every remotely taken photo might look weird, I do hope Apple supports this feature even if the user is just casually holding one AirPod. Audio Mix Last year, Apple introduced the Audio Mix alongside iPhone 16, which “uses machine learning to identify and separate background elements from speech and lets you choose from three voice options,” In-frame, Studio and Cinematic. Now, 9to5Mac has learned that Apple has been developing a similar “studio quality” mic mode for AirPods , turning them into a strong competitor in the creator lav mic market. Wider classroom support Apple has been working on a feature aimed at improving the pairing experience in shared iPads, which will be especially welcome in classrooms where multiple students use the same device. Top comment by Ryan W Liked by 6 people AirPods Pro are probably the most satisfying product experience I've ever had. Most companies stop adding major features after releasing a product. With the AirPods, Apple gives us new stuff year after year. The AirPods I own now are significantly more capable than the ones I bought three years ago. View all comments The goal is to streamline how AirPods connect to iPadOS in these settings, likely with less friction and fewer manual steps. As always, Apple may choose to delay or tweak some of these features before making them public. But if even a few of them make it to the WWDC25 keynote on Monday, it would mark yet another year that great features are added to current models, rather than new features being associated with a newly announced hardware. WWDC25: What to expect Apple is expected to unveil major updates across all its platforms, including iOS 26 , macOS 26 , iPadOS 26 , watchOS 26 , and more. With AI reportedly playing a smaller role this year, the spotlight will likely fall on design refreshes, and long-time wishlist picks like better window management support on the iPad. The keynote kicks off Monday, June 9, and 9to5Mac will feature full coverage before, during, and after the event. See you then! Best AirPods deals currently on Amazon AirPods Pro 2, USB-C Charging : 20% off at $199 AirPods (3rd Generation) : 19% off at $136 AirPods 4, USB-C and Wireless Charging : 6% off at $168 AirPods 4 USB-C Charging : 8% off at $119 AirPods Max Wireless, USB-C Charging, Midnight : 9% off at $499 Add 9to5Mac to your Google News feed. You’re reading 9to5Mac — experts who break news about Apple and its surrounding ecosystem, day after day. Be sure to check out our homepage for all the latest news, and follow 9to5Mac on",
      "summary": "3-minute read",
      "url": "https://9to5mac.com/2025/06/04/exclusive-airpods-ios-26-wwdc-features/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "dev",
      "word_count": 767,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "ChatGPT introduces meeting recording and connectors for Google Drive, Box, and more",
      "content": "OpenAI’s ChatGPT is adding new features for business users, including integrations with different cloud services, meeting recordings, and MCP connection support for connecting to tools for deep research. As part of the launch, ChatGPT is gaining connectors for Dropbox, Box, SharePoint, OneDrive, and Google Drive. This allows ChatGPT to look for information across users’ own services to answer their questions. For instance, an analyst could use the company’s slide deck and documents to build out an investment thesis. OpenAI said that the new feature will follow an organization’s access control hierarchy. Recording and transcription of meetings, now a table-stakes feature of the productivity suites, is also now available. The feature can generate notes with time-stamped citations and suggest actions. Users will be able to query for information in their meeting notes, as they can with documents and files across the integrated services. Plus, users can convert action items into a Canvas document, OpenAI’s tool for writing and coding projects . The feature competes with ClickUp, Zoom, and, more recently, Notion , which have all added some type of transcription and meeting summarization features to their products. In addition, the company is introducing deep research connectors for HubSpot, Linear, and select Microsoft and Google tools in beta. OpenAI said that users can now prepare detailed research reports through Deep Research using knowledge and data from these sources, along with web information. OpenAI said connectors are available to all paid users. Customers can also use MCP (model context protocol) to connect to other tools for deep research. MCP support will be available to Pro, Team, and Enterprise users, the company said. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The addition of these features indicates that OpenAI wants businesses to use ChatGPT even more than they already might be in their enterprise workspaces. The company is likely to succeed here. While startups like Notion and Lux Capital-backed Context are also trying to build AI-powered office tools that provide research and analysis features, OpenAI has a first-mover advantage. Three million customers already",
      "summary": "2-minute read",
      "url": "https://techcrunch.com/2025/06/04/chatgpt-introduces-meeting-recording-and-connectors-for-google-drive-box-and-more/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 412,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Scientists in Japan develop plastic that dissolves in seawater within hours",
      "content": "WAKO, Japan, June 4 (Reuters) - Researchers in Japan have developed a plastic that dissolves in seawater within hours, offering up a potential solution for a modern-day scourge polluting oceans and harming wildlife. While scientists have long experimented with biodegradable plastics, researchers from the RIKEN Center for Emergent Matter Science and the University of Tokyo say their new material breaks down much more quickly and leaves no residual trace. Sign up here. At a lab in Wako city near Tokyo, the team demonstrated a small piece of plastic vanishing in a container of salt water after it was stirred up for about an hour. While the team has not yet detailed any plans for commercialisation, project lead Takuzo Aida said their research has attracted significant interest, including from those in the packaging sector. Scientists worldwide are racing to develop innovative solutions to the growing plastic waste crisis, an effort championed by awareness campaigns such as World Environment Day taking place on June 5. Item 1 of 11 A researcher shows a sample of ocean-degradable plastic at the Center for Emergent Matter Science (CEMS) of Japanese research institution Riken in Wako, Saitama Prefecture, Japan May 27, 2025. REUTERS/Manami Yamada [1/11] A researcher shows a sample of ocean-degradable plastic at the Center for Emergent Matter Science (CEMS) of Japanese research institution Riken in Wako, Saitama Prefecture, Japan May 27, 2025. REUTERS/Manami Yamada Purchase Licensing Rights , opens new tab Plastic pollution is set to triple by 2040, the UN Environment Programme has predicted, adding 23-37 million metric tons of waste into the world's oceans each year. \"Children cannot choose the planet they will live on. It is our duty as scientists to ensure that we leave them with best possible environment,\" Aida said. Aida said the new material is as strong as petroleum-based plastics but breaks down into its original components when exposed to salt. Those components can then be further processed by naturally occurring bacteria, thereby avoiding generating microplastics that can harm aquatic life and enter the food chain. As salt is also present in soil, a piece about five centimetres (two inches) in size disintegrates on land after over 200 hours, he added. The material can be used like regular plastic when coated, and the team are focusing their current research on the best coating methods, Aida said. The plastic is non-toxic, non-flammable, and does not emit carbon dioxide, he added. Reporting by Irene Wang; Editing by John Geddie and Lincoln Feast. Our Standards: The Thomson Reuters Trust Principles. , opens new tab Share Purchase Licensing Rights Sustainability UK economy shrinks by the most since 2023 as US tariffs hit Sustainable Finance & Reporting category · June 12, 2025 · 6:50 AM UTC · ago Britain's economic output fell sharply in April. Boards, Policy & Regulation category UK accounting watchdog fines KPMG $1.7 million for Carr’s Group audit breaches 6:50 AM UTC · Updated ago Sustainability category South Africa's $5.8 billion Hive project aims to lead low-cost ammonia output 6:04 AM UTC Sustainable Finance & Reporting category Polish fashion retailer LPP's Q1 profit rises 20%, beats forecast 5:54 AM UTC Society & Equity category Kenya's budget to weigh revenue growth against public outrage 4:02 AM UTC",
      "summary": "2-minute read",
      "url": "https://www.reuters.com/sustainability/climate-energy/scientists-japan-develop-plastic-that-dissolves-seawater-within-hours-2025-06-04/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 536,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "One thing Tesla and Comma.ai overlooked in self-driving",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://deepdream.substack.com/p/one-thing-tesla-and-commaai-overlooked?utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://deepdream.substack.com/p/one-thing-tesla-and-commaai-overlooked?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Mistral releases a vibe coding client, Mistral Code",
      "content": "French AI startup Mistral is releasing its own “vibe coding” client, Mistral Code, to compete with incumbents like Windsurf, Anysphere’s Cursor, and GitHub Copilot. Mistral Code, a fork of the open source project Continue, is an AI-powered coding assistant that bundles Mistral’s models, an “in-IDE” assistant, local deployment options, and enterprise tools into a single package. A private beta is available as of Wednesday for JetBrains development platforms and Microsoft’s VS Code. “Our goal with Mistral Code is simple: deliver best-in-class coding models to enterprise developers, enabling everything from instant completions to multi-step refactoring through an integrated platform deployable in the cloud, on reserved capacity, or air-gapped, on-prem GPUs,” Mistral wrote in a blog post provided to TechCrunch. AI programming assistants are growing increasingly popular. While they still struggle to code quality software , their promise to boost coding productivity is pushing companies and developers to adopt them rapidly. One recent poll found that 76% of developers have used or were planning to use AI tools in their development processes last year. The Mistral Code client and dashboards Image Credits: Mistral Mistral Code is said to be powered by a combination of in-house models including Codestral (for code autocomplete), Codestral Embed (for code search and retrieval), Devstral (for “agentic” coding tasks), and Mistral Medium (for chat assistance). The client supports more than 80 programming languages and a number of third-party plug-ins, and can reason over things like files, terminal outputs, and issues, the company said. Mistral claimed that customers including consulting firm Capgemini, Spanish and Portuguese bank Abanca, and French national railway company SNCF are using Mistral Code in production. “Customers can fine-tune or post-train the underlying models on private repositories or distill lightweight variants,” Mistral explained in its blog post. “For IT managers, a rich admin console exposes granular platform controls, deep observability, seat management, and usage analytics.” Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW Mistral said it plans to continue making improvements to Mistral Code and contribute at least a portion of those upgrades to the Continue open source project. Image Credits: Mistral Founded in 2023, Mistral is a frontier model lab building a range of AI-powered services, including a chatbot platform, Le Chat, and mobile apps. It is backed by venture investors like General Catalyst, and has raised over €1.1 billion (roughly $1.24 billion) to date. A few weeks ago, Mistral launched the aforementioned Codestral, Devstral, and Mistral Medium models. Around the same time, the company rolled out Le Chat Enterprise, a corporate-focused chatbot service that offers tools like an AI agent builder, and integrates Mistral’s models with third-party services like Gmail, Google Drive, and SharePoint.",
      "summary": "2-minute read",
      "url": "https://techcrunch.com/2025/06/04/mistral-releases-a-vibe-coding-client-mistral-code/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 513,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "LLMs are mirrors of operator skill",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://ghuntley.com/mirrors/?utm_source=tldrnewsletter",
      "summary": "9-minute read",
      "url": "https://ghuntley.com/mirrors/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content extraction failed - no content found using site-specific, adaptive, or generic strategies"
    },
    {
      "title": "Nintendo Switch 2 draws long lines and excited gamers",
      "content": "IE 11 is not supported. For an optimal experience visit our site on another browser. Skip to Content Get more news Live on June 4, 2025, 7:40 PM EDT By Brian Cheung , Steve Kopack and Kalhan Rosenblatt Listen to this article / For the crowds outside Nintendo’s store in New York’s Rockefeller Center on Wednesday, a long wait was well worth getting their hands on the new Switch 2 console. Some camped out for hours, bringing their own makeshift waterproof tents and lawn chairs. Others stood in the line, which stretched several city blocks. A handful even dressed up as iconic Nintendo characters, including Luigi, the beloved brother and sidekick in the video game “Super Mario.” “Really, it’s just all about the community and sort of sharing that love of Nintendo with each other,” said Lauren Hyde, who has been in line since she traveled from Maryland on Monday. The Japanese gaming giant is releasing the coveted console Thursday, eight years after the original version hit shelves. The first iteration of the Switch was a major hit for Nintendo, selling roughly 150 million units, according to the company , making it the third-best-selling console of all time, behind the PS2 and the Nintendo DS. Nintendo Switch 2, the successor to the Nintendo Switch system, will be released in 2025. Nintendo The hype for the Switch 2 has been building for months, with Nintendo teasing a brief first look at it in January . With a bigger screen, better graphics and larger controls, the next iteration of the console has drummed up enough enthusiasm among gamers that many showed up early at stores like Best Buy and GameStop to pick up their units at midnight. “We have a lot of wonderful products that create buzz,” said Jason Bonfig, Best Buy’s senior executive vice president in charge of customer offerings and fulfillment. “Switch is very, very special. They’re a passionate community.” Bonfig estimated that 70% of Best Buy’s customers indicated they wanted to pick up their Switch 2s in-store, “and the vast majority of those customers will be there at midnight to do so.” “Something about consumer electronics in general with that instant gratification is really important,” he added. Meanwhile, the Nintendo Store is selling units at midnight only to those who secured reservations on its website, meaning many lining up outside its New York City location were doing so mostly for fun. Nintendo even warned in a message on its website that “delivery by June 5 is not guaranteed” for customers who had not already secured pre-order invitations. Though consumers have been fatigued by high prices amid the ongoing global trade war, Nintendo said in April that it would not hike the price of the $449.99 device at launch. There is also hope among some in the gaming world that the Switch 2 could spur the creation of new games and therefore new jobs. The industry has taken a hit in recent years after widespread layoffs affected many studios, and a performers strike has continued into 2025. While some companies release consoles to reviewers early, Nintendo has remained quiet about many of the details of the Switch 2, holding off on review copies for gaming news sites like IGN . Nintendo has not commented on how many customers have pre-ordered the Switch 2. Brian Cheung Brian Cheung is a business and data correspondent for NBC News. Steve Kopack Steve Kopack is a producer at NBC News covering business and the economy. Kalhan Rosenblatt Kalhan Rosenblatt is a reporter covering youth and internet culture for NBC News, based in New York.",
      "summary": "3-minute read",
      "url": "https://www.nbcnews.com/business/business-news/nintendo-switch-2-release-lines-excited-gamers-rcna210910?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 600,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "“AI will replace all the jobs!” Is Just Tech Execs Doing Marketing",
      "content": "Over the weekend, I went digging for evidence that AI can, will, or has replaced a large percent of jobs. It doesn’t exist. Worse than that, actually, there’s hundreds of years of evidence and sophisticated analyses from hundreds of sources showing the opposite is true: AI will almost certainly create more jobs than it displaces, just like thousands of remarkable technologies before it. I don’t want anyone to think I’m raining on this parade without first attempting to convince myself that the opposite was true, and that AI really would be the first technology in 120 years to displace a massive portion of the workforce. So, dry though it may be, let’s walk through the logic together. The majority of statements that have received press (and there have been dozens in the last 5 years) center on the claim that AI will destroy 20-50% of the current need for human labor. I’ll attempt to address each of the most robust points inherent in those arguments, rather than trying to argue that any one innovation or upgrade to a model’s capability hasn’t done it yet (or won’t): If AI is going to make such a huge percentage of jobs redundant, there must be historical analogies–i.e. other technologies that massively upended labor markets. What are these and how have they affected jobs in the past? MIT’s Technology Review noted that this “fear of new tech taking jobs,” is far from new. The automation of farm work is the most notable and most labor-impacting example we have from history, rapidly unemploying a huge portion of human beings in the developing economies of the late 19th and 20th centuries. And yet, at the conclusion of this era (~1940s/50s), the conclusion was that “technological unemployment is a myth,” because “technology has created so many new industries” and has expanded the market by “lowering the cost of production to make a price within reach of large masses of purchasers.” In short, technological advances had created more jobs overall. Last year, Quarterly Journal of Economics published a groundbreaking study on how technological innovations have impacted labor forces across industries since 1980. MIT did a nice summarization : “the number of studies that support the labour replacement effect is more than offset by the number of studies that support the labour-creating/reinstating and real income effects.” This 2023 paper looked at 127 previous studies of technology supposedly replacing labor forces from the 18th century to the present, concluding that “the labor displacing effect of technology appears to be more than offset by compensating mechanisms that create or reinstate labor.” The Economic Policy Institute did a deep dive into what drives labor market demand and unemployment, concluding: “Productivity growth (which technology sometimes enables and other times drives) has not historically been associated with higher unemployment or higher inequality,” and that “Anxieties over widespread technology-driven unemployment lack an empirical base.” Perhaps the closest analogy to AI is the personal computer revolution of the 1980s. Millions of jobs in communication, documentation, research, analysis, and engineering became obsolete within a decade, and yet, the McKinsey Global Institute concluded in 2018 that “We tallied up all the jobs destroyed in the US since 1980 as a result of the rise of personal computing and the Internet, and it’s about 3.5 million,” but “When we add up all the jobs created, we find that over 19 million jobs have been created as a result of the personal computer and Internet. We see a net gain of 15.8 million jobs in the US over the last few decades. That’s about 10 percent of the civilian labor force today.” If AI is going to have these massive impacts but hasn’t yet, why not? Folks who claim AI will destroy the labor market have claimed this radical change is “only a few years away,” “on the immediate horizon,” or “imminent,” for the last 5 years, yet we’re at historically low unemployment (yes, even accounting for underemployment and the way the BLS counts employment ). The US labor market is within a single percentage point of its post-war unemployment low, measured in 1953 at 3.4% . If AI is killing jobs, it’s doing so at an imperceptibly slow rate; why could that be? Is it still too early? Did other technologies take a long time to show their impacts on labor markets? The broad consensus from rational industry observers, analysts, economists, and even AI-hyped technologists is that the end of cheap money (i.e. higher US interest rates) has driven most of the lower-than-pre-pandemic-demand for entry-level talent (just as it has in times of inflation-fighting interest hikes of the past). Machine-learning, the technology underpinning AI, has been around for decades , with widespread adoption in tech companies between 2006-2013. The current generative-AI era, based on the transformer architecture model, kicked off in 2017, with significant public examples and tech adoption from 2018-2020. Most of the current, press-driven AI hype cycle, however, skyrocketed in late 2021 with OpenAI’s release of GPT-3 (longtime readers here will recall that Britney Muller showed off techniques extremely similar to what’s now associated with modern LLMs back in July 2018 ). We’ve had 15-20 years of robust machine learning development and adoption, and another 5-10 years of broad LLM/generative AI adoption , improvement, and usage, yet labor market fluctuation has been far more dependent on other factors: the Covid pandemic itself, the post-pandemic surge and decline in tech hiring, inflation-fighting tactics by government banks, and (most recently), a renewal of early-20th-century-style tariffs and trade wars. When controlling for these events. The effects of previous technological advancements also took time, but the most salient examples (of farm equipment in the 1910-1920 era and the personal computer in the 1980s) showed millions of displaced workers within 5 years . AI’s slower changes bode poorly for the argument that it will have a larger impact than those events. Even if one assumes that AI was the only contributor to labor market changes between 2021-2025, the change has been incredibly slight, *even* in the software engineering market where it supposedly has the greatest impact. There was a greater loss (nearly 150%) in percentage of software engineering jobs between 2019-2021 than from 2021-2025. I found it particularly revealing that one of the most commonly cited examples of AI killing labor needs in the software field is the death of StackOverflow, and yet, a robust analysis of that site’s usage from 2008-2020 shows that “What really happened is a parable of human community and experiments in self-governance gone bizarrely wrong.” However, it seems likely that the perception of AI and its adoption are slowing hiring in the software engineering market in the post-bubble-popping era (2024-25). This thoughtful analysis by Gergely Orosz concludes a well-visualized, data-driven walkthrough with: “LLMs are a leading cause of the fall in software developer job postings: there’s uncertainty at large companies about whether to hire as fast as previously, given the productivity hype around AI tooling, and businesses are opting to “wait and see” by slowing down recruitment, as a result.” It strains credibility to look at the data, history, and analyses and conclude that AI will eventually kill 20-50% of all jobs, when its largest impact in the prior 5-20 years of adoption (depending on one’s starting point) is ~10% variation in a job sector that employs ~1% of US workers . Assuming AI will have an effect similar to 20th Century farm equipment’s on agriculture, why will that labor force behave differently to their 20th Century counterparts (and either refuse to or be prevented from finding new jobs)? This point is hard to find citations for, given that it’s a future-looking, theoretical assertion. We can, however, compare the impact of the tractor (and farm machinery more broadly) on the economy from 1910-1960. Tractors and farm equipment resulted in the shutdown of a huge number of farms, and a decline in the number of people employed in farming, from ~33% to ~2% of the labor force (notably, even that massive upheaval was less significant than the prognostications by tech company leaders that AI will displace half of all jobs). Nothing like it has happened in the American economy since, and only the industrial revolution of the 18th/19th centuries can compete in scale of transformation. A superb breakdown of farm machinery’s impact on a sector that employed more than a quarter of all Americans comes from Olmstead and Rhode at UC Davis : Is it possible that AI will do to broad sectors of the economy what mechanized farm equipment did to agriculture? Rationally, it’s difficult to fathom generative AI having a greater economic and labor-force impact than the PC revolution of the 1980s. AI makes many tasks more efficient, but evidence that it can wholesale replace entire human functions in a tractor-like way is pure speculation that exists in imagination, not reality. The core assertion by the “AI will replace 20-50% of all jobs” crowd seems to be that the past 20 years of machine learning and generative AI improvements are not indicative of what will happen in the future: a leap in capability that will enable company management to instruct an AI on a job function (“get us press,” or “optimize our marketing campaign,” or “record and audit our financials” ) and rely on machines to correctly determine what needs to be done, how to do it, and then complete all associated tasks with little to no human supervision, intervention, or additional labor. It’s impossible to argue against the assertion that AI will do what’s described above, because it’s based not on objective data, but rather on subjective belief about a possible future. Fighting about what someone believes may come about in the future is generally non-productive, so I’ll avoid that to spare us all a lot of wasted time 😉 I’ll move on from the dry argument analysis and citation process and attempt to summarize (and opine on) what’s really going on here. Leaders of AI companies, and some AI proponents, marketers, journalists, and even critics have found that when they make scary predictions about their field destroying the job market, press and media eat it up. This media coverage, because it’s scary and the AI hype cycle is in full swing, draws clicks. Those clicks lead to employees, managers, and leaders at other businesses being scared into learning and adopting AI in their businesses. Incentive also exists for those who criticize AI, AI companies, or their ethics/models/practices: these folks also benefit directly from the attention they earn when they amplify the message of AI as a job destroying technology. If you’re feeling like the “AI will take all our jobs” discussion is familiar, you’re in good company. Many others have pointed out the similarities to stories like: Source: Jalopnik Source: InsideHook Source: Honest Jobs Mechanization really did take jobs from farm workers. Automation took jobs from manual laborers. The PC took jobs from clerical and communication workers. But, all of these resulted in greater productivity, employment, and more optionality for workers. It’s both anti-historic and anti-evidence that AI will somehow prove to be the exception. Could AI, along with thousands of other impactful technological, political, social, demographic, and black-swan-event changes permanently alter the employment landscape in our lifetimes? Absolutely. In fact, one of my favorite stats from this overly-ambitious weekend of research was MIT’s estimation that 60% of employment in 2018 was in types of jobs that didn’t exist before 1940 . By the time I’m in my 80s, y’all better have destroyed more than half of all the existing jobs, and that’s just to keep up with the 20th Century’s pace of change. But, don’t expect AI to do it for you in the next decade; that’s just marketing. p.s. If you’re looking for the TL;DR, Ed Zitron on Bluesky has got you: p.p.s. I agree there’s evidence that this fear-based marketing campaign has been successful enough to disrupt some hiring, especially for early-stage jobs in a few tech-heavy fields . But squinting at the evidence, it’s <0.1% of jobs (<200,000 total) being affected, and even here, the unbalanced capital vs. labor market is a far more compelling explanation .",
      "summary": "14-minute read",
      "url": "https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 2027,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple Notes Expected to Gain Support for Exporting in Markdown in iOS 26",
      "content": "Apple Notes Expected to Gain Support for Exporting in Markdown in iOS 26 Wednesday June 4, 2025 4:37 am PDT by Tim Hardwick Apple's Notes app is rumored to be getting limited Markdown support in iOS 26 and macOS 26, according to 9to5Mac . The feature would allow users to export text in the markdown format. Markdown is a lightweight markup language that some writers prefer to use over rich text. Rather than using HTML for bold, italics, links, and headers, Markdown uses quick character shortcuts like **bold** or #header. It sounds like the feature will only add support for exporting text with markdown formatting and not writing in markdown directly. If the rumor holds up, it's likely to be unveiled at next week's Worldwide Developers Conference alongside other iOS 26 improvements, including automatic translation and polls in Messages , not to mention a major visual redesign . Related Roundup: macOS 26 Tahoe Tag: Apple Notes [ 74 comments ] Get weekly top MacRumors stories in your inbox. Popular Stories Monday June 9, 2025 9:00 am PDT by Jordan Golson Apple's Worldwide Developers Conference (WWDC) starts today with the traditional keynote kicking things off at 10:00 a.m. Pacific Time. MacRumors is on hand for the event and we'll be sharing details and our thoughts throughout the day. We're expecting to see a number of software-related announcements led by a design revamp across Apple's platforms that will also see the numbering of all of... • 1212 comments Monday June 9, 2025 5:21 pm PDT by Juli Clover At today's WWDC 2025 keynote event, Apple unveiled a new design that will inform the next decade of iOS, iPadOS, and macOS development, so needless to say, it was a busy day. Apple also unveiled a ton of new features for the iPhone, an overhauled Spotlight interface for the Mac, and a ton of updates that make the iPad more like a Mac than ever before.",
      "summary": "1-minute read",
      "url": "https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "ai",
      "word_count": 322,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Apple names 2025 Design Awards winners",
      "content": "Days before its Worldwide Developers Conference 2025 (WWDC), Apple has named the winners and finalists of its 2025 Design Awards for apps. This year’s winners and finalists mostly include indie apps and startups, and contrary to the trend everywhere else, generative AI apps are noticeably missing from the list. This is the second year in a row Apple has skipped AI-centric apps for its awards . The company did highlight some apps with AI features, though, including Speechify (which won in the Inclusivity category), and music creation and editing app Moises (both are venture-backed startups). Unsurprisingly, Apple focuses more on how developers have used its tools to build better experiences. The company named 12 winners (one app and one game in each) across six categories: Delight and Fun, Innovation, Interaction, Inclusivity, Social Impact, and Visuals and Graphics. In its list of finalists this year, Apple highlighted a large number of apps and games from small developers around the world. From @Apple : \"Speechify is more than a great app; it’s a critical resource that helps people live their lives.\" A big thanks to our friends @Apple & proud the @SpeechifyAI team for winning this year's 2025 Apple Design Award in the lead up to WWDC. https://t.co/t7k8nQK7HD — Rohan Pavuluri (@RPavuluri) June 3, 2025 Watch Duty , which helped share information during California wildfires, won the award in the social impact category. Meanwhile, M13-backed developer prototyping tool Play was awarded the best app in the innovation category. Huge news — Play has won the Apple Design Award for Innovation! To be recognized by @Apple — the company whose platforms have inspired so much of what we’re building — is an incredible honor. Immensely proud of our incredible team and so grateful to all the designers, teams,… pic.",
      "summary": "2-minute read",
      "url": "https://techcrunch.com/2025/06/04/apple-names-2025-design-awards-winners/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 296,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Machine Code Isn't Scary",
      "content": "Machine Code Isn't Scary The first programming language I ever learned was ActionScript. Writing code for Macromedia's Flash might be the furthest away from \"bare metal\" as you can possibly get. As I continued learning new languages, this starting heritage stuck with me. I was mostly interested in high-level, \"web languages\". Low-level languages felt impenetrable. Over time, I learned a bit more about them here and there, but for some reason, this notion stuck with me. Low-level things are scary, and machine code epitomized that most directly. When I Googled things asking about writing in \"straight machine code\", I was met with discouraging messages rather than learning. Eventually, I decided I needed to overcome this barrier if I was going to achieve my goals. In doing so, I learned something I didn't expect. Machine code isn't scary. If you can make sure your JSON conforms to a JSON schema, you can write machine code. Which Machine Code? One problem with machine code is that there isn't simply one standard. There are many different \"instruction sets\" depending on the processor. Most modern PCs use x86-64 machine code, but newer Macs, Raspberry Pis, and most mobile devices use ARM. There are other architectures out there, especially as you go back in time. The goal of this article won't be to give you a deep understanding of any particular instruction set, but instead, to give you enough information about how machine code typically works so you cannot be afraid of machine code. So we will start by having our examples be in ARM 64-bit (also written as aarch64). Once we have a decent understanding of that, we will talk a bit about x86-64. Machine Code Basics To understand the basics of machine code, you need three concepts: Instructions Registers Memory Instructions are exactly what they sound like; they are the code that will run. Machine code instructions are just numbers. In fact, in AArch64, every instruction is a 32-bit number. Instructions encode what operation the machine should run (add, move, subtract, jump, etc.) and accept some arguments for what data to operate on. These arguments might be constants (meaning like the number 2; these constants are often called \"immediates\"), but they can also be registers or a memory address. For now, just think of a register as a variable and memory as a list. Arm Instructions Here is an example of the instruction add immediate . 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 sf 0 0 1 0 0 0 1 0 sh imm12 Rn Rd Now this might look a bit confusing, but once you've seen these tables long enough, they start to be fairly straightforward. Each column in this table represents a single bit in a 32-bit number. If the value is a 0 or 1, that just means it is already filled in. If it has a label, it is a variable that needs to be filled in. sf tells us whether the registers we are going to use are 64-bit or 32-bit registers. sh stands for shift. sh goes in conjunction with imm12, which stands for a 12-bit immediate (constant). So if we want to add 42 to something, we would put 000000101010 in for imm12 and set sh to 0 (meaning we aren't shifting the number). But what if we want to represent a number larger than 12 bits? Well, the add instruction doesn't let us represent all such numbers; but setting sh to 1 lets us shift our number by 12 bits. So for example we can represent 172032172032 by leaving our 42 alone and setting sh to 1. This is a clever technique for encoding larger numbers in a small space. Variables that start with R are registers, in this case, Rn is our argument to add, and Rd is our destination. So the above instruction can be thought of like this: struct Add { is_sixty_four_bit : boolean , shift : boolean , immediate : u12 , n : Register , destination : Register , } Our add instruction is really just a data structure where we put the right parts in the right places. Registers Registers are small places to store values. Every instruction set will have a different number of these registers, different sizes of registers, different kinds of registers, and different naming conventions for registers. For AArch64, there are 31 general-purpose registers numbered X0 through X30 for 64-bit registers. Let's say we want to add 42 to register X0 and store the result in X1; we use this binary number. sf operation sh imm12 Rn Rd 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 To encode our registers into our instruction, we just use their number. So register X0 would be 00000 and register X18 would be 10010 . Registers are simply places where we can store values. But by convention, registers can be used for different things. These are called calling conventions and they are how \"higher\" level languages like C encode function calls. Writing out all these binary numbers all the time (or even converting them to hex) can often be tedious. So instead, we usually talk about instructions in a simple text format called assembly. add x1, x0, #0x2a In order to feel cool, people usually write numbers in assembly as hex values. This is just the number 42. You can see that assembly hides some of the details of the encoding we just made. We don't think about sf, sh, what size our number is, that a register is Rn vs Rd. Instead, the destination comes first and the arguments after. Because of this lack of detail, a single assembly instruction add might actually map to many different machine code instructions depending on its arguments. Memory The last piece we have to understand for machine code is memory. To understand what is going on with memory, we will look at an instruction that lets us store things in memory. This instruction is called STR or not written in shorthand, store. 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 1 x 1 1 1 0 0 1 0 0 imm12 Rn Rt Using this instruction, we are going to store some value (RT) into the address (RN) + some offset (imm12). So if we think about memory as a big array, this instruction is like writing into that array. array[offset] = value . The x here is like our sf before, it controls whether we are using 64-bit values or not. If we want to make this concrete, let's say we have a value in X2, we have an address of memory in X1 and we want to store a value 2 bytes offset from that. We would get this structure: x operation imm12 Rn Rt 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 Since writing that all is tedious, we often just write the assembly notation. We are storing the value in x2 based on the address stored in x1 + 2. str x2, [x1, #0x2] X86-64 X86 encoding is a bit different, but it more or less has the same parts. We are still working with instructions, registers, and memory. Some names are a bit different. Instead of the consistent 0-30 naming, we get the historical baggage of the following 64-bit registers: rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8-r15). However, the biggest difference is that x86 is not a fixed width instruction set. We can't simply give a nice little diagram of every instruction using 32 bits. Instead, instructions are assembled from parts. These parts are given different names; when you see an instruction encoding, it tells you how to put the parts together. REX 7 6 5 4 3 2 1 0 0 1 0 0 W R X B The first part is called the REX. This is a prefix that we can use to help us with 64-bit operations. Not sure if there is an official justification for the name REX, but my understanding is that it is the \"Register Extension Prefix\". Unfortunately, because the REX is a prefix, it will only make sense when we see what comes later. REX is there for backward compatibility. The W in REX lets us signal that we are using 64-bit or not for certain operations. The R and B will \"extend\" our registers in certain operations. In other words, it allows more registers than you used to be able to (These are those r8-r15 registers with a different naming convention than the older registers). We need these because, before 64-bit x86, we had fewer registers and our instructions only had 3 bits per register. With 16 registers, we need an extra bit. (X is for the SIB structure, which we don't cover here). ModR/M 7 6 5 4 3 2 1 0 mod reg rm Our next part is ModR/M. ModR/M keeps up with the tradition of naming things incredibly short and confusing names. mod actually means Mode. mod tells us if rm is acting as a register or if it is a pointer to memory. If mod == 11 then rm is being used as a register, otherwise, it is being used as a pointer. reg just is a register. OpCode OpCode is simple, it is a number. It can be 1-3 bytes long. Putting It Together There are other parts, but we won't cover them here. With just these parts, we can build up an instruction. Let's say we want to move a 32-bit signed immediate to a 64-bit register. We can consult a table of instruction encodings and we will get this: REX.W + C7 /0 id So now we can assemble our parts and make our instruction. Let's start with REX.W. This notation just means REX with W set to 1. Then there’s B8, which is just a number written in hex. /0 is yet more shorthand for using the ModR/M but setting the reg to 0. Finally, id means \"immediate doubleword\", in other words, a constant number that is 32 bits long. So given all that, we can write our instruction. So let's move the number 42 to the rbx register. Byte Index Bits Description Byte 0 55–48 01001000 REX.W = 1 Byte 1 47–40 11000111 Opcode C7 Byte 2 39–32 11000011 ModR/M: reg=000, r/m=011 (RBX) Byte 3 31–24 00101010 42 Byte 4 23–16 00000000 the rest of 42 Byte 5 15–8 00000000 ... Byte 6 7–0 00000000 ... Why is RBX 011? Well, because the table says so. Yeah, I did say that x86 is a bit weird. The Rest of It I won't pretend that this is all you need. But I will say that starting here can get you further than you think. There are some other things to learn, like various flags for things like overflow, there’s also calling conventions, which are about which registers you use when for things like function calls. We haven't really talked about the stack here, but that's memory that you write to to keep track of things. Nor have we talked about jumps, or how to encode larger immediates in ARM, but you’ve gotten the basics. It’s easier than you would think to hop on compiler explorer and learn how things are done. Learning machine code and writing things at this low level has unlocked so many things that were mental blocks for me before. Relying on libraries made by others to do these low-level things always left a gap in my knowledge that made me doubt my understanding. Even if I intellectually could explain things, actually doing them has made a huge difference for me. So if you, like me, find low-level things intimidating, I can't recommend enough starting from scratch, at the lowest possible level for your task. What I've found over and over again with low-level details, their not hard, their just poorly documented and poorly explained.",
      "summary": "7-minute read",
      "url": "https://jimmyhmiller.com/machine-code-isnt-scary?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 2077,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Life as a physical process",
      "content": "It’s always hard to define life. Everyone has their favorite definition – some describe it as a struggle against entropy, while others describe it as an emergent property of chemicals. Countless books have been written on the topic, yet we’re far from a consensus. Against the backdrop of the second law of thermodynamics, life seems like an improbable accident. When everything tends to go towards disorder, how come life is able to create cities, computers and space ships? How do we reconcile all the beautiful complexity we see around us with the stupidly simple laws we observe in physics? Recently, I read this beautiful book, named Life as No One Knows It , by Sara Walker and had a minor revelation that maybe we’ve been looking at life from a completely wrong lens. All this while, we have been fixated on different manifestations of life as we find it on Earth. We see bacteria, zebra fish, or fruit fly and start wondering whether we got lucky here on Earth, or whether our universe contains many more similar instances of life. But, is this even the right question to ask? If we’re identifying life as we know it on Earth, we’re blinding ourselves to the only instance of life as we know it (with self-replication, cells, DNA, etc.). But, why should we expect life to be the same way in the rest of the universe? Are cells fundamental? Does life have to be necessarily carbon-based? Sure, we can define life as anything. It’s an overloaded term. But if we keep defining it with traits we observe on Earth, we will be like that guy who only searches for his keys under the street light because that’s the only thing he can see. Of course, to discover alien life, we need to first know what we’re looking for. But defining all life as being Earth-like is too myopic. What should we do? Well, only when we define life as a precise physical phenomena that doesn’t refer back to life on Earth, can we expect to discover it at unexpected places in the universe. What could such a definition look like? My take-home message from the book was this: Life is a physical process that gives rise to systems that are statistically unlikely to assemble and persist (by themselves) Sara, in her book, champions Assembly Theory which posits that each object in the universe has two numbers associated with it: assembly index and copy number. Assembly index is effectively the minimal number of steps required to (recursively) assemble an object from available building blocks. Think of it as analogous to number of steps required to assemble a lego shape. Copy number is how many copies of the object exist in the universe. Once you accept these basic concepts, you can then define life as a process that generates objects with high assembly index and copy number . High assembly index assures that the process to produce it could not go about randomly. You just don’t smash a bunch of lego pieces and make a Taj Mahal. You need knowledge of it, and that knowledge could only come about via a history of selection. That history is what life is. So, if you see a complicated object – one that could not have assembled spontaneously from basic physics – you should assume a Darwinian-like selection process to be underway. High copy number of that complicated object simply means you get more statistical assurance that some creative process is underway that’s producing extremely complicated objects in multiple numbers. Some concepts in the Assembly theory aren’t clear to me. For example, what is an object and what are building blocks? My current understanding is that Assembly theory works wonderfully at the chemical level where you can define objects as molecules, and building blocks as atoms. Then, assembly index can be precisely calculated via the shortest method of synthesising the molecule from atoms. But, outside that application, I’m not sure how will we calculate assembly index of objects like stars, or really complicated ones like kidneys. Nevertheless, v iewed from the lens of statistical impossibility, life suddenly elevates from the realm of Earth-biology to the study of patterns . We can start looking at life not as wet, mushy thing as we know it, but more like a creative force that builds artifacts that could not have come about randomly (because those complicated objects prevent their creative force from dying out.) In fact, with this lens, life doesn’t even remain a binary thing. Instead of saying something is alive or dead, it’s probably better to talk in terms of degree of aliveness. Aliveness is what an object does to its surroundings. Does it create more complicated outputs v/s the inputs it consumes? Then it’s perpetuating life. The further the distance in complicatedness between an object’s outputs v/s inputs, the more alive something can said to be. (I love that this definition aligns well with how we view agency in people. High agency people shape their environment, while low agency people get shaped! ) Overall, I strongly recommend reading the book. It’s a unique take on why we should go beyond life as we know it on Earth, and start looking at it as a physical phenomena that subsumes what we see here on Earth. To me, the shift is a bit like finally recognising Earth revolves around the Sun (and not the other way around). The beautiful thing about rising one level is that it can help us see life at places we never expected it to be. Life is a creative force that refuses to die out by creating objects that can’t assemble spontaneously. Perhaps AIs are alive in that sense? They’re a complicated object, present in multiple copies and are playing a vital role in creating more statistically unlikely objects in the universe. So, why disqualify them? Join 200k followers Follow @paraschopra Get my new essays in your email",
      "summary": "5-minute read",
      "url": "https://invertedpassion.com/life-as-a-physical-process/?utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 994,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "23andMe's Former CEO Pushes Purchase Price Nearly $50 Million Higher",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/articles/23andmes-former-ceo-pushes-purchase-price-nearly-50-million-higher-7ec98421?st=HxjmWW&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "2-minute read",
      "url": "https://www.wsj.com/articles/23andmes-former-ceo-pushes-purchase-price-nearly-50-million-higher-7ec98421?st=HxjmWW&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Reddit Sues Anthropic, Alleges Unauthorized Use of Site's Data",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/tech/ai/reddit-lawsuit-anthropic-ai-3b9624dd?st=zWM5Ru&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "2-minute read",
      "url": "https://www.wsj.com/tech/ai/reddit-lawsuit-anthropic-ai-3b9624dd?st=zWM5Ru&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-05T00:00:00",
      "category": "big_tech",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Musk Says SpaceX Revenue Will Near $16 Billion in 2025",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/business/spacex-revenue-to-exceed-nasas-budget-in-2026-elon-musk-says-3321ea31?st=4dsszH&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "2-minute read",
      "url": "https://www.wsj.com/business/spacex-revenue-to-exceed-nasas-budget-in-2026-elon-musk-says-3321ea31?st=4dsszH&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "big_tech",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Beyond 2nm: Apple's A20 chip to introduce new packaging breakthrough",
      "content": "Skip to main content Apple Silicon Beyond 2nm: Apple’s A20 chip to introduce new packaging breakthrough Marcus Mendes | Jun 3 2025 - 4:02 pm PT Apple is planning to overhaul its chip design for the 2026 iPhones, in a move that could mark the first time it uses advanced multi-chip packaging in a mobile device. It sounds complicated, but here’s what this means. According to analyst Jeff Pu in a new report for GF Securities , the iPhone 18 Pro, 18 Pro Max, and the long-rumored iPhone 18 Fold are expected to debut Apple’s A20 chip, built on TSMC’s second-gen 2nm process (N2). But that’s only part of the story. The more interesting bit is how those chips will be assembled. How those chips will be assembled For the first time, Apple is set to adopt Wafer-Level Multi-Chip Module (WMCM) packaging for its iPhone processors. WMCM allows different components, like the SoC and DRAM, to be integrated directly at the wafer level, before being diced into individual chips. It uses a technique that connects the dies without needing an interposer or substrate, which can bring both thermal and signal integrity benefits. If you have no idea what a silicon interposer is, this video does a great job of explaining it. In other words, Apple’s next-gen chip won’t just be smaller and more power-efficient thanks to N2. It’ll also be physically closer to its onboard memory, enabling better performance and potentially lower power consumption for tasks like AI processing and high-end gaming. Behind the scenes, Pu reports TSMC is building a dedicated production line and expects to quickly ramp up by 2027: “TSMC will establish a dedicated WMCM production line at its AP7, leveraging equipment and process similar to CoWoS-L without on-substrate. We see TSMC is preparing capacity of up to 50KPM by end of 2026 and estimate the capacity to reach 110-120KPM by end-2027, due to the increase in adoption.” What this means For Apple, this is a big leap in chip design, similar to when it adopted 3nm ahead of most of the industry. And for the broader mobile market, it suggests that technologies once reserved for data center GPUs and AI accelerators are making their way into smartphones. And if you’re wondering what this means for the iPhone 18 Fold: it seems Apple isn’t just reserving its most novel hardware for its form factor. It may also be the company’s testbed for next-gen silicon packaging. Add 9to5Mac to your Google News feed. You’re reading 9to5Mac — experts who break news about Apple and its surrounding ecosystem, day after day. Be sure to check out our homepage for all the latest news, and follow 9to5Mac on",
      "summary": "2-minute read",
      "url": "https://9to5mac.com/2025/06/03/apples-a20-chip-packaging-breakthrough/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "dev",
      "word_count": 448,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Human Brain Cells on a Chip for Sale",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://spectrum.ieee.org/biological-computer-for-sale?utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://spectrum.ieee.org/biological-computer-for-sale?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "big_tech",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Neuralink competitor Paradromics completes first human implant",
      "content": "Key Points Paradromics has implanted its brain-computer interface in a human for the first time. The procedure took place at the University of Michigan with a patient who was already undergoing neurosurgery to treat epilepsy. The company said the procedure demonstrated that its system can be safely implanted and record neural activity. Dr. Matthew Willsey working in the operating room. Courtesy of the University of Michigan Neurotech startup Paradromics on Monday announced it has implanted its brain-computer interface in a human for the first time. The procedure took place May 14 at the University of Michigan with a patient who was already undergoing neurosurgery to treat epilepsy. The company's technology was implanted and removed from the patient's brain in about 20 minutes during that surgery. Paradromics said the procedure demonstrated that its system can be safely implanted and record neural activity. It's a major milestone for the nearly 10-year-old startup, as it marks the beginning of its next chapter as a clinical-stage company. Once regulators give it the green light, Paradromics plans to kick off a clinical trial later this year that will study the long-term safety and use of its technology in humans. \"We've shown in sheep that our device is best in class from a data and longevity standpoint, and now we've also shown that it's compatible with humans,\" Paradromics founder and CEO Matt Angle told CNBC in an interview. \"That's really exciting and raises a lot of excitement for our upcoming clinical trial.\" A brain-computer interface, or BCI, is a system that deciphers brain signals and translates them into commands for external technologies. Paradromics' system is called the Connexus Brain-Computer Interface, and the company says it will initially help patients with severe motor impairments such as paralysis speak through a computer. More CNBC health coverage From ‘Cockroach Award’ to the Big Board: Hinge Health’s unlikely path to IPO How UnitedHealthcare became the face of America’s health insurance frustrations When 20,000 devices were paralyzed by a bad update, a Georgia health system turned to Apple Paradromics' BCI has not been cleared by the U.S. Food and Drug Administration, and it still has a long road ahead before it reaches commercialization. But for Angle, who founded the company in 2015, the procedure in May was a success, and one that was years in the making. \"You do all of these steps, you validate the hardware, you have this really high degree of rational certainty that things are going to work,\" he said, \"but still emotionally when it works and when it happens the way you expected it to, it's still very, very gratifying.\" Though Paradromics' BCI has not been officially cleared for use by regulators, organizations like the University of Michigan can use new devices for research as long as they can demonstrate that there is not a significant risk to patients. Dr. Oren Sagher, professor of neurosurgery at the University of Michigan, oversaw the traditional clinical component of the procedure in May. Dr. Matthew Willsey, assistant professor of neurosurgery and biomedical engineering at the University of Michigan, led the research component, including the placement of Paradromics' device. BCIs have been studied in academia for decades, and several other startups, including Elon Musk 's Neuralink , are developing their own systems. Paradromics' Connexus Brain-Computer Interface. Courtesy: Paradromics \"It's absolutely thrilling,\" Willsey said in an interview. \"It's motivating, and this is the kind of thing that helps me get up in the morning and go to work.\" Each company's BCI is slightly different, but Paradromics is designing a BCI that can record brain activity at the level of individual neurons. Angle compared this approach to placing microphones inside vs. outside a stadium. Inside a stadium, microphones would capture more detail, such as individual conversations. Outside a stadium, microphones would only capture the roar of the crowd, he said. Other prominent BCI companies include Synchron , which is backed by Jeff Bezos and Bill Gates, and Precision Neuroscience . Both have implanted their systems in humans. Paradromics has raised nearly $100 million as of February, according to PitchBook. The company announced a strategic partnership with Saudi Arabia's Neom in February, but declined to disclose the investment amount. \"The last demonstration stuff has been shown, and we're really excited about the clinical trial that's coming up,\" Angle said. WATCH: Inside Paradromics, the Neuralink competitor hoping to commercialize brain implants before the end of the decade watch now VIDEO 5:03 05:03 Inside Paradromics, the Neuralink competitor hoping to commercialize brain implants before the end of the decade The Edge",
      "summary": "5-minute read",
      "url": "https://www.cnbc.com/2025/06/02/neuralink-paradromics-human-implant.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "big_tech",
      "word_count": 756,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Why GUIs are built at least 2.5 times",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://patricia.no/2025/05/30/why_lean_software_dev_is_wrong.html?utm_source=tldrnewsletter",
      "summary": "25-minute read",
      "url": "https://patricia.no/2025/05/30/why_lean_software_dev_is_wrong.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "science",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Meta and Yandex are de-anonymizing Android users' web browsing identifiers",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~4200 words, got 299. URL: https://arstechnica.com/security/2025/06/meta-and-yandex-are-de-anonymizing-android-users-web-browsing-identifiers/?utm_source=tldrnewsletter",
      "summary": "21-minute read",
      "url": "https://arstechnica.com/security/2025/06/meta-and-yandex-are-de-anonymizing-android-users-web-browsing-identifiers/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 299,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 299/4200 words (7.1%)"
    },
    {
      "title": "Does AI Progress Have a Speed Limit?",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://asteriskmag.com/issues/10/does-ai-progress-have-a-speed-limit?utm_source=tldrnewsletter",
      "summary": "34-minute read",
      "url": "https://asteriskmag.com/issues/10/does-ai-progress-have-a-speed-limit?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Google's NotebookLM now lets you share your notebook — and AI podcasts — publicly",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/678915/google-notebooklm-share-public-link?utm_source=tldrnewsletter",
      "summary": "1-minute read",
      "url": "https://www.theverge.com/news/678915/google-notebooklm-share-public-link?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Meta Prioritizing Ultralight Headset With Puck For 2026 Over Traditional Quest 4",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.uploadvr.com/meta-prioritizing-puffin-for-2026-pushing-out-quest-4-to-2027/?utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://www.uploadvr.com/meta-prioritizing-puffin-for-2026-pushing-out-quest-4-to-2027/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Epic Games' MetaHuman creation tool launches out of early access",
      "content": "MetaHuman , a game dev tool from Epic Games to quickly create realistic digital human characters, is now out of early access and available to all developers with today’s release of Unreal Engine 5.6. With the new update, developers can now use MetaHuman Animation to generate real-time animations and audio from almost any mono camera (such as webcams and most smartphones). Any camera that works with Unreal Engine LiveLink, including some Android phones, will be able to take advantage of these features. MetaHuman Creator, meanwhile, now has an expanded assortment of character mesh shapes and textures, and enhanced fidelity thanks to improvements to the capturing and processing of scanned data. This will help teams create even more lifelike characters for their projects. Epic Games shows off MetaHuman’s real-time animation features at Unreal Fest 2025. During the State of Unreal keynote at Unreal Fest 2025, Epic Games demonstrated a real-time demo of MetaHuman with an alien character named Blue. Using just a minimal camera setup, an actor backstage was able to speak to the crowd in real time, with MetaHuman accurately capturing his facial expressions and movements without any noticeable delays. Epic also revealed that it updated the Unreal Engine EULA license to include MetaHumans, so now developers can use the tool in any game engine or creative software. Disclosure: Epic Games paid my way to Unreal Fest.",
      "summary": "2-minute read",
      "url": "https://venturebeat.com/gaming-business/epic-games-metahuman-creation-tool-launches-out-of-early-access/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 227,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 227/400 words (56.8%)"
    },
    {
      "title": "Stop Over-thinking AI Subscriptions",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://steipete.me/posts/2025/stop-overthinking-ai-subscriptions?utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://steipete.me/posts/2025/stop-overthinking-ai-subscriptions?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Colossal Labs will release the sounds of dire wolves howling ‘later this year,' founder says",
      "content": "Ben Lamm, the billionaire founder of Colossal Labs, and Sophie Turner spoke about his company’s efforts in reviving the long-extinct dire wolf during a panel at SXSW’s first London outpost. The wolf re-entered the public consciousness because of the television show “Game of Thrones,” in which Turner was a leading character. It turns out that even she thought dire wolves were mythical creatures. “Were you a fan of ‘Game of Thrones?’” Turner asked. “I was a fan of ‘Game of Thrones,’” Lamm said. “When ‘Game of Thrones’ came out, which was a while ago, I never thought, ‘Oh, by the way, one day we’ll make dragons or dire wolves.” Lamm retold the story of how he and his team at Colossal “brought back” the dire wolf species, which had been extinct for more than 10,000 years. He also provided some updates as to what the company is working on next, including a bio-acoustic project, announced just a few weeks ago . Lamm said his dire wolves started howling at 3 weeks old, responding to human singing and the sounds of owls. The first two, Romulus and Remus, are now around 6 months old. Lamm and his team are looking at ways to use AI to help monitor the behavior of the wolves, identifying patterns of their emotions and “wolf linguistics,” as he put it. “What’s crazy about it is the dire wolves have their own unique pitch and inflection,” Lamm told the crowd. “We’re going to release that later this year,” he said, referring to a bio-acoustic project, which will map out the different types of dire wolf howls. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW The wolves live on a 2,000-acre preservation in a top-secret location. He referred to the park as “very Yellowstone.” It has an animal hospital, as well as full-time caretakers and full-time security. The wolves are learning socialization tactics that would help them in the wild. “They’re starting to do some light predation on the preserve,” he said. “They’re becoming more and more true wolf-like every day.” He also said that they are about to officially introduce the two wolves to a newly created dire wolf, a female named Khaleesi, who is around 3 months old. The public met Colossal Labs’ efforts to “de-extinct” animals with fascination and skepticism, much of it focused on whether the dire wolves are merely genetically edited gray wolves. But investors seem quite interested in where Colossal could be headed with its innovation. TechCrunch reported back in March that the company is looking to “revive” the Tasmanian tiger and the dodo, two other extinct animals. The company announced this year a $200 million Series C at a $10.2 billion valuation and said in March it genetically modified mice to grow mammoth-like fur. (Lamm said it impressively took only a month to do.) Turner did ask Lamm a question that he unsurprisingly gets quite often. “What about dinosaurs?” she asked. “Can you bring them back?” His company isn’t focused on that at the moment, as bringing a dinosaur back is quite complicated, Lamm explained. But later in the conversation, he said there might be a new update coming from Colossal soon. “I think maybe this summer, there could be a dodo update that’s interesting,” he said. Update: This story was updated to provide clarity about the skepticism around Colossal’s dire wolf program. Colossal Labs says a link included in this report is inaccurate. “We are in an issue with them because they misquoted Beth Shapiro,” spokesperson Emily Mailaender wrote in an email. “She never said they weren’t dire wolves. We have been working to get it corrected.” TechCrunch will remove the link if the media outlet retracts its story.",
      "summary": "3-minute read",
      "url": "https://techcrunch.com/2025/06/03/colossal-labs-will-release-the-sounds-of-dire-wolves-howling-later-this-year-founder-says/?utm_source=tldrnewsletter",
      "published_date": "2025-06-04T00:00:00",
      "category": "ai",
      "word_count": 692,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "How Much Should the World's Richest Man Get Paid?",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/business/elon-musk-tesla-pay-package-fcd86d99?st=2vtDJs&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "7-minute read",
      "url": "https://www.wsj.com/business/elon-musk-tesla-pay-package-fcd86d99?st=2vtDJs&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Meta Aims to Fully Automate Ad Creation Using AI",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/tech/ai/meta-aims-to-fully-automate-ad-creation-using-ai-7d82e249?st=jA6Fqa&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "4-minute read",
      "url": "https://www.wsj.com/tech/ai/meta-aims-to-fully-automate-ad-creation-using-ai-7d82e249?st=jA6Fqa&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "Artificial Blood That Could Work for All Blood Types in Trials",
      "content": "CLOSE X By Jasmine Laws is a US News Reporter at Newsweek based in London, U.K. Her focus is reporting on health insurance, including Medicare and Medicaid, but she also writes about U.S. politics, crime and global affairs. Jasmine joined Newsweek in 2024 from News UK and had previously written for the Metro, Byline Times, The Human Perspective magazine and The Express. She has previously extensively covered stories within the U.K. art industry, human rights, health, and human trafficking. She was one of the nominees for Best News Piece at the U.K's Anti-Slavery Day 2023 Awards in Parliament. She is a graduate of Durham University and completed her NCTJ at News Associates. You can get in touch with Jasmine by emailing jasmine.laws@newsweek.com. Languages: English. Writers Page Jasmine Laws US News Reporter Newsweek Is A Trust Project Member FOLLOW Speed: 0.5x Speed: 1x Speed: 1.5x Speed: 2x 🎙️ Voice is AI-generated. Inconsistencies may occur. ✖ Clinical trials to explore the use of universal artificial blood are underway in Japan after initial volunteer doses began in March. The research, led by Professor Hiromi Sakai's laboratory, plans to assess artificial blood, usable for all blood types and storable for up to two years, as a potential solution to critical shortages in blood supplies for emergency and chronic health care worldwide. Newsweek has contacted Sakai for comment via an email to Nara Medical University. Why It Matters There have been other trials to explore the use of artificial blood in recent years, including in the U.S. and the U.K. While Japan's efforts are not the first, it comes amid a new push in medical research to find ways of reducing problems caused by blood shortages . The World Health Organization estimates that more than 118 million blood donations are collected each year—with 40 percent coming from high-income countries, home to 16 percent of the world's population. This means a large portion of the global population has limited access to blood transfusion treatment. Universal artificial blood could reduce preventable deaths in injury, surgery and childbirth—settings where supply mismatches or stockouts cost millions of lives annually in low-income nations alone. A stock image of blood units stored in a cold storage room. A stock image of blood units stored in a cold storage room. Sina Schuldt/dpa via AP What To Know Nara Medical University's trial administered 100 to 400 milliliters of the artificial blood to 16 healthy adult volunteers in March, according to the local news outlet Kyodo News . The next stage would be to examine the treatment's efficacy and safety if no side effects were reported. Newsweek has not been able to determine whether the participants experienced any side effects following the March transfusions. Sakai's lab said on its website that the artificial blood would solve problems present in the traditional blood transfusion system, including \"possibility of infection, blood type mismatching, immunological response, and short shelf life which is insufficient for stockpiling for emergency situations.\" Artificial blood could reduce preventable deaths in settings where limited supply costs millions of lives in low-income countries. Artificial blood could reduce preventable deaths in settings where limited supply costs millions of lives in low-income countries. Getty The blood was created by extracting hemoglobin, the oxygen-carrying molecule, from expired donor blood—older than three weeks—and encapsulating it in a lipid shell. Known as hemoglobin vesicles, these particles mimic natural red blood cells and can carry oxygen efficiently. They are also free of any blood type markers, making them universally compatible and virus-free. The research builds on a 2022 trial in Japan, when artificial hemoglobin vesicles were tested to determine if they carry oxygen in the way red blood cells do. Participants in the study experienced minor side effects from the trial, such as fever or rash, that were resolved quickly. What People Are Saying Ash Toye, a professor of cell biology in the School of Biochemistry at the University of Bristol, England, told Newsweek : \"The launch of a new clinical trial in Japan using artificial blood product derived from human hemoglobin marks a potentially exciting step forward in transfusion medicine. While this area has long held promise, previous efforts have faced significant challenges, particularly around safety, stability, and oxygen delivery efficacy. This trial will need to demonstrate not only that the artificial blood is safe in humans but that it can perform as reliably as donor blood under a range of clinical conditions. There are advantages with an artificial blood product as you will not have to worry about compatibility, and as it's usually smaller, it can penetrate areas which are blocked, e.g., by a stroke or clot.\" Toye added: \"However, as it uses human hemoglobin sourced from blood donors as a starting material, it faces the same challenges in terms of infection risk as human blood . But as with normal donor human blood, they will likely mitigate this by testing. The reliance on human blood also restricts the scale that can be achieved here. But I assume at some point they will move to recombinant human blood sources. I look forward to hearing more about the artificial product and the trial.\" What Happens Next Nara Medical University plans to advance from volunteer safety trials to broader efficacy studies, seeking clinical approval and practical rollout of its universal artificial blood by 2030, according to Kyodo News. The outcome could redefine worldwide transfusion medicine and transform the humanitarian response to trauma, surgery and chronic disease care. Request Reprint & Licensing Submit Correction View Editorial & AI Guidelines Top stories Exclusive: Houthis Warn US and Israel of 'War' If Iran Attacked US Embassy in Middle East Prepares to Evacuate After Warning From Iran Mahmoud Khalil Cannot Be Detained or Deported, Judge Rules Federal Agents Involved in Los Angeles Hit and Run Investigation—Police About the writer Jasmine Laws is a US News Reporter at Newsweek based in London, U.K. Her focus is reporting on health insurance, including Medicare and Medicaid, but she also writes about U.S. politics, crime and global affairs. Jasmine joined Newsweek in 2024 from News UK and had previously written for the Metro, Byline Times, The Human Perspective magazine and The Express. She has previously extensively covered stories within the U.K. art industry, human rights, health, and human trafficking. She was one of the nominees for Best News Piece at the U.K's Anti-Slavery Day 2023 Awards in Parliament. She is a graduate of Durham University and completed her NCTJ at News Associates. You can get in touch with Jasmine by emailing jasmine.laws@newsweek.com. Languages: English. Writers Page Jasmine Laws FOLLOW Jasmine Laws is a US News Reporter at Newsweek based in London, U.K. Her focus is reporting on health insurance, including Medicare and Medicaid, but she also writes about U.S. politics, crime and global affairs. Jasmine joined Newsweek in 2024 from News UK and had previously written for the Metro, Byline Times, The Human Perspective magazine and The Express. She has previously extensively covered stories within the U.K. art industry, human rights, health, and human trafficking. She was one of the nominees for Best News Piece at the U.K's Anti-Slavery Day 2023 Awards in Parliament. She is a graduate of Durham University and completed her NCTJ at News Associates. You can get in touch with Jasmine by emailing jasmine.laws@newsweek.com. Languages: English. Jasmine Laws is a US News Reporter at Newsweek based in London, U.K. Her focus is reporting on health insurance, ...",
      "summary": "4-minute read",
      "url": "https://www.newsweek.com/artificial-blood-japan-all-blood-types-2079654?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "science",
      "word_count": 1233,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Blue Origin launches 6 passengers on sub-orbital trip to the edge of space",
      "content": "Blue Origin launches 6 passengers on sub-orbital trip to the edge of space - CBS News Watch CBS News Blue Origin spacecraft carries 6 to edge of space Blue Origin spacecraft carries 6 to edge of space 00:20 An international crew of four men and two women blasted off and rocketed to the edge of space Saturday, enjoying a few minutes of weightlessness and an out-of-this-world view before plunging back to Earth to wrap up Blue Origin's twelfth New Shepard passenger flight. \"It was such an incredible ride, very moving, very spiritual,\" Panamanian attorney Jaime Alemán, a former ambassador to the United States, said after landing. \"Even better than I ever imagined. I've been traveling, thanks to the gods, all my life, this was like the cherry on top of a cake. I mean, to go up in space and see how huge it is, you can only begin to imagine how much more there is still to discover.\" he New Shepard rocket and crew capsule climb away from Blue Origin's West Texas launch site on the company's 12th crewed sub-orbital space flight. Blue Origin webcast Blue Origin's hydrogen-fueled single-stage booster roared to life at 9:39 a.m. EDT and quickly climbed away from the company's West Texas launch site, accelerating to just over 2,000 mph before releasing the New Shepard spacecraft. The crew capsule, equipped with the largest windows of any operational spaceship, continued coasting up to an altitude of nearly 65 miles, just above the internationally recognized boundary between the discernible atmosphere and space, before beginning the descent to landing. At the moment the capsule was released, its three American passengers — Aymette Medina Jorge, Gretchen Green and Paul Jeris — along with Alemán, Canadian Jesse Williams and New Zealand's Mark Rocket, began enjoying about three minutes of weightlessness as they coasted upwards. The New Shepard single-stage rocket flew itself back to landing near its launch pad for refurbishment and work to ready the vehicle for its next flight. Blue Origin webcast The reusable New Shepard booster, meanwhile, also continued upward before slowing and beginning its on tail-first descent. Seven minutes after liftoff, the rocket's BE-3 engine re-ignited, four landing legs deployed and the booster settled to a picture-perfect touchdown on a landing pad near the launch site. The gumdrop-shaped New Shepard capsule took a more leisurely flight home, descending under three large parachutes to touchdown in the West Texas desert 10 minutes after liftoff. \"You guys, we did it!\" one of the passengers exclaimed. Passenger Gretchen Green celebrates her trip to space after touchdown in the West Texas desert. Blue Origin webcast Blue Origin has now launched 12 passenger flights since the sub-orbital spacecraft carried Amazon- and Blue Origin-founder Jeff Bezos and three others aloft in July 2021. The company has now launched 68 passengers, including four who have flown twice. Blue Origin and its passengers do not reveal how much it costs to fly aboard a New Shepard, but it's estimated to cost upwards of $500,000 per seat. Jorge's seat was sponsored by Farmacias Similares, a Mexican company focused on accessible healthcare across Latin America. Her crewmates presumably paid for their own tickets. Blue Origin has had the sub-orbital passenger market to itself since June 2024 when competitor Virgin Galactic, founded by entrepreneur Richard Branson, retired its original rocketplane to focus on building two upgraded Delta-class spacecraft. Virgin is expected to resume flights next year. Elon Musk Jeff Bezos Virgin Galactic Blue Origin Richard Branson William Harwood Bill Harwood has been covering the U.S. space program full-time since 1984, first as Cape Canaveral bureau chief for United Press International and now as a consultant for CBS News. More from CBS News Musk says he regrets some of his posts on Trump, conceding they \"went too far\" Why everyone from Musk to Wall Street is worried about U.S. debt payments Oklahoma's top prosecutor says state plans to retry Richard Glossip Israel is poised to launch operation on Iran, sources say View CBS News In Open Continue Be the first to know Get browser notifications for breaking news, live events, and exclusive reporting. Not Now Turn On",
      "summary": "3-minute read",
      "url": "https://www.cbsnews.com/news/blue-origin-launches-six-passengers-on-sub-orbital-trip-to-the-edge-of-space/?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "science",
      "word_count": 688,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "My AI Skeptic Friends Are All Nuts",
      "content": "Author Name Thomas Ptacek @tqbf @tqbf Image by Annie Ruygt A heartfelt provocation about AI-assisted programming. Tech execs are mandating LLM adoption. That’s bad strategy. But I get where they’re coming from. Some of the smartest people I know share a bone-deep belief that AI is a fad — the next iteration of NFT mania. I’ve been reluctant to push back on them, because, well, they’re smarter than me. But their arguments are unserious, and worth confronting. Extraordinarily talented people are doing work that LLMs already do better, out of spite. All progress on LLMs could halt today, and LLMs would remain the 2nd most important thing to happen over the course of my career. Important caveat : I’m discussing only the implications of LLMs for software development. For art, music, and writing? I got nothing. I’m inclined to believe the skeptics in those fields. I just don’t believe them about mine. Bona fides: I’ve been shipping software since the mid-1990s. I started out in boxed, shrink-wrap C code. Survived an ill-advised Alexandrescu C++ phase. Lots of Ruby and Python tooling. Some kernel work. A whole lot of server-side C, Go, and Rust. However you define “serious developer”, I qualify. Even if only on one of your lower tiers. level setting † (or, God forbid, 2 years ago with Copilot) First, we need to get on the same page. If you were trying and failing to use an LLM for code 6 months ago †, you’re not doing what most serious LLM-assisted coders are doing. People coding with LLMs today use agents. Agents get to poke around your codebase on their own. They author files directly. They run tools. They compile code, run tests, and iterate on the results. They also: pull in arbitrary code from the tree, or from other trees online, into their context windows, run standard Unix tools to navigate the tree and extract information, interact with Git, run existing tooling, like linters, formatters, and model checkers, and make essentially arbitrary tool calls (that you set up) through MCP. The code in an agent that actually “does stuff” with code is not, itself, AI. This should reassure you. It’s surprisingly simple systems code, wired to ground truth about programming in the same way a Makefile is. You could write an effective coding agent in a weekend. Its strengths would have more to do with how you think about and structure builds and linting and test harnesses than with how advanced o3 or Sonnet have become. If you’re making requests on a ChatGPT page and then pasting the resulting (broken) code into your editor, you’re not doing what the AI boosters are doing. No wonder you’re talking past each other. the positive case LLMs can write a large fraction of all the tedious code you’ll ever need to write. And most code on most projects is tedious. LLMs drastically reduce the number of things you’ll ever need to Google. They look things up themselves. Most importantly, they don’t get tired; they’re immune to inertia. Think of anything you wanted to build but didn’t. You tried to home in on some first steps. If you’d been in the limerent phase of a new programming language, you’d have started writing. But you weren’t, so you put it off, for a day, a year, or your whole career. I can feel my blood pressure rising thinking of all the bookkeeping and Googling and dependency drama of a new project. An LLM can be instructed to just figure all that shit out. Often, it will drop you precisely at that golden moment where shit almost works, and development means tweaking code and immediately seeing things work better. That dopamine hit is why I code. There’s a downside. Sometimes, gnarly stuff needs doing. But you don’t wanna do it. So you refactor unit tests, soothing yourself with the lie that you’re doing real work. But an LLM can be told to go refactor all your unit tests. An agent can occupy itself for hours putzing with your tests in a VM and come back later with a PR. If you listen to me, you’ll know that. You’ll feel worse yak-shaving. You’ll end up doing… real work. but you have no idea what the code is Are you a vibe coding Youtuber? Can you not read code? If so: astute point. Otherwise: what the fuck is wrong with you? You’ve always been responsible for what you merge to main . You were five years go. And you are tomorrow, whether or not you use an LLM. If you build something with an LLM that people will depend on, read the code. In fact, you’ll probably do more than that. You’ll spend 5-10 minutes knocking it back into your own style. LLMs are showing signs of adapting to local idiom, but we’re not there yet. People complain about LLM-generated code being “probabilistic”. No it isn’t. It’s code. It’s not Yacc output. It’s knowable. The LLM might be stochastic. But the LLM doesn’t matter. What matters is whether you can make sense of the result, and whether your guardrails hold. Reading other people’s code is part of the job. If you can’t metabolize the boring, repetitive code an LLM generates: skills issue! How are you handling the chaos human developers turn out on a deadline? † (because it can hold 50-70kloc in its context window) For the last month or so, Gemini 2.5 has been my go-to †. Almost nothing it spits out for me merges without edits. I’m sure there’s a skill to getting a SOTA model to one-shot a feature-plus-merge! But I don’t care. I like moving the code around and chuckling to myself while I delete all the stupid comments. I have to read the code line-by-line anyways. but hallucination If hallucination matters to you, your programming language has let you down. Agents lint. They compile and run tests. If their LLM invents a new function signature, the agent sees the error. They feed it back to the LLM, which says “oh, right, I totally made that up” and then tries again. You’ll only notice this happening if you watch the chain of thought log your agent generates. Don’t. This is why I like Zed’s agent mode : it begs you to tab away and let it work, and pings you with a desktop notification when it’s done. I’m sure there are still environments where hallucination matters. But “hallucination” is the first thing developers bring up when someone suggests using LLMs, despite it being (more or less) a solved problem. but the code is shitty, like that of a junior developer Does an intern cost $20/month? Because that’s what Cursor.ai costs. Part of being a senior developer is making less-able coders productive, be they fleshly or algebraic. Using agents well is both a both a skill and an engineering project all its own, of prompts, indices, and (especially) tooling. LLMs only produce shitty code if you let them. † (Also: 100% of all the Bash code you should author ever again) Maybe the current confusion is about who’s doing what work. Today, LLMs do a lot of typing, Googling, test cases †, and edit-compile-test-debug cycles. But even the most Claude-poisoned serious developers in the world still own curation, judgement, guidance, and direction. Also: let’s stop kidding ourselves about how good our human first cuts really are. It’s hard to get a good toolchain for Brainfuck, too. Life’s tough in the aluminum siding business. † (and they surely will; the Rust community takes tooling seriously) A lot of LLM skepticism probably isn’t really about LLMs. It’s projection. People say “LLMs can’t code” when what they really mean is “LLMs can’t write Rust”. Fair enough! But people select languages in part based on how well LLMs work with them, so Rust people should get on that †. I work mostly in Go. I’m confident the designers of the Go programming language didn’t set out to produce the most LLM-legible language in the industry. They succeeded nonetheless. Go has just enough type safety, an extensive standard library, and a culture that prizes (often repetitive) idiom. LLMs kick ass generating it. All this is to say: I write some Rust. I like it fine. If LLMs and Rust aren’t working for you, I feel you. But if that’s your whole thing, we’re not having the same argument. but the craft Do you like fine Japanese woodworking? All hand tools and sashimono joinery? Me too. Do it on your own time. † (I’m a piker compared to my woodworking friends) I have a basic wood shop in my basement †. I could get a lot of satisfaction from building a table. And, if that table is a workbench or a grill table, sure, I’ll build it. But if I need, like, a table? For people to sit at? In my office? I buy a fucking table. Professional software developers are in the business of solving practical problems for people with code. We are not, in our day jobs, artisans. Steve Jobs was wrong: we do not need to carve the unseen feet in the sculpture. Nobody cares if the logic board traces are pleasingly routed. If anything we build endures, it won’t be because the codebase was beautiful. Besides, that’s not really what happens. If you’re taking time carefully golfing functions down into graceful, fluent, minimal functional expressions, alarm bells should ring. You’re yak-shaving. The real work has depleted your focus. You’re not building: you’re self-soothing. Which, wait for it, is something LLMs are good for. They devour schlep, and clear a path to the important stuff, where your judgement and values really matter. but the mediocrity As a mid-late career coder, I’ve come to appreciate mediocrity. You should be so lucky as to have it flowing almost effortlessly from a tap. We all write mediocre code. Mediocre code: often fine. Not all code is equally important. Some code should be mediocre. Maximum effort on a random unit test? You’re doing something wrong. Your team lead should correct you. Developers all love to preen about code. They worry LLMs lower the “ceiling” for quality. Maybe. But they also raise the “floor”. Gemini’s floor is higher than my own. My code looks nice. But it’s not as thorough. LLM code is repetitive. But mine includes dumb contortions where I got too clever trying to DRY things up. And LLMs aren’t mediocre on every axis. They almost certainly have a bigger bag of algorithmic tricks than you do: radix tries, topological sorts, graph reductions, and LDPC codes. Humans romanticize rsync ( Andrew Tridgell wrote a paper about it!). To an LLM it might not be that much more interesting than a SQL join. But I’m getting ahead of myself. It doesn’t matter. If truly mediocre code is all we ever get from LLMs, that’s still huge. It’s that much less mediocre code humans have to write. but it’ll never be AGI I don’t give a shit. Smart practitioners get wound up by the AI/VC hype cycle. I can’t blame them. But it’s not an argument. Things either work or they don’t, no matter what Jensen Huang has to say about it. but they take-rr jerbs So does open source. We used to pay good money for databases. We’re a field premised on automating other people’s jobs away. “Productivity gains,” say the economists. You get what that means, right? Fewer people doing the same stuff. Talked to a travel agent lately? Or a floor broker? Or a record store clerk? Or a darkroom tech? When this argument comes up, libertarian-leaning VCs start the chant: lamplighters, creative destruction, new kinds of work. Maybe. But I’m not hypnotized. I have no fucking clue whether we’re going to be better off after LLMs. Things could get a lot worse for us. LLMs really might displace many software developers. That’s not a high horse we get to ride. Our jobs are just as much in tech’s line of fire as everybody else’s have been for the last 3 decades. We’re not East Coast dockworkers ; we won’t stop progress on our own. but the plagiarism Artificial intelligence is profoundly — and probably unfairly — threatening to visual artists in ways that might be hard to appreciate if you don’t work in the arts. We imagine artists spending their working hours pushing the limits of expression. But the median artist isn’t producing gallery pieces. They produce on brief: turning out competent illustrations and compositions for magazine covers, museum displays, motion graphics, and game assets. LLMs easily — alarmingly — clear industry quality bars. Gallingly, one of the things they’re best at is churning out just-good-enough facsimiles of human creative work. I have family in visual arts. I can’t talk to them about LLMs. I don’t blame them. They’re probably not wrong. Meanwhile, software developers spot code fragments seemingly lifted from public repositories on Github and lose their shit. What about the licensing? If you’re a lawyer, I defer. But if you’re a software developer playing this card? Cut me a little slack as I ask you to shove this concern up your ass. No profession has demonstrated more contempt for intellectual property. The median dev thinks Star Wars and Daft Punk are a public commons. The great cultural project of developers has been opposing any protection that might inconvenience a monetizable media-sharing site. When they fail at policy, they route around it with coercion. They stand up global-scale piracy networks and sneer at anybody who so much as tries to preserve a new-release window for a TV show. Call any of this out if you want to watch a TED talk about how hard it is to stream The Expanse on LibreWolf. Yeah, we get it. You don’t believe in IPR. Then shut the fuck up about IPR. Reap the whirlwind. It’s all special pleading anyways. LLMs digest code further than you do. If you don’t believe a typeface designer can stake a moral claim on the terminals and counters of a letterform, you sure as hell can’t be possessive about a red-black tree. positive case redux When I started writing a couple days ago, I wrote a section to “level set” to the state of the art of LLM-assisted programming. A bluefish filet has a longer shelf life than an LLM take. In the time it took you to read this, everything changed. Kids today don’t just use agents; they use asynchronous agents. They wake up, free-associate 13 different things for their LLMs to work on, make coffee, fill out a TPS report, drive to the Mars Cheese Castle, and then check their notifications. They’ve got 13 PRs to review. Three get tossed and re-prompted. Five of them get the same feedback a junior dev gets. And five get merged. “I’m sipping rocket fuel right now,” a friend tells me. “The folks on my team who aren’t embracing AI? It’s like they’re standing still.” He’s not bullshitting me. He doesn’t work in SFBA. He’s got no reason to lie. There’s plenty of things I can’t trust an LLM with. No LLM has any of access to prod here. But I’ve been first responder on an incident and fed 4o — not o4-mini, 4o — log transcripts, and watched it in seconds spot LVM metadata corruption issues on a host we’ve been complaining about for months. Am I better than an LLM agent at interrogating OpenSearch logs and Honeycomb traces? No. No, I am not. To the consternation of many of my friends, I’m not a radical or a futurist. I’m a statist. I believe in the haphazard perseverance of complex systems, of institutions, of reversions to the mean. I write Go and Python code. I’m not a Kool-aid drinker. But something real is happening. My smartest friends are blowing it off. Maybe I persuade you. Probably I don’t. But we need to be done making space for bad arguments. but i’m tired of hearing about it And here I rejoin your company. I read Simon Willison , and that’s all I really need. But all day, every day, a sizable chunk of the front page of HN is allocated to LLMs: incremental model updates, startups doing things with LLMs, LLM tutorials, screeds against LLMs. It’s annoying! But AI is also incredibly — a word I use advisedly — important. It’s getting the same kind of attention that smart phones got in 2008, and not as much as the Internet got. That seems about right. I think this is going to get clearer over the next year. The cool kid haughtiness about “stochastic parrots” and “vibe coding” can’t survive much more contact with reality. I’m snarking about these people, but I meant what I said: they’re smarter than me. And when they get over this affectation, they’re going to make coding agents profoundly more effective than they are today. Previous post ↓ Using Kamal 2.0 in Production",
      "summary": "16-minute read",
      "url": "https://fly.io/blog/youre-all-nuts/?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 2831,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "9 Lessons From Cursor's System Prompt",
      "content": "[CONTENT EXTRACTION FAILED] Expected ~10400 words, got 2591. URL: https://byteatatime.dev/posts/cursor-prompt-analysis/?utm_source=tldrnewsletter",
      "summary": "52-minute read",
      "url": "https://byteatatime.dev/posts/cursor-prompt-analysis/?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 2591,
      "content_extraction_status": "failed",
      "failure_reason": "Insufficient content: got 2591/10400 words (24.9%)"
    },
    {
      "title": "Google's Pixel 10 phones will reportedly launch on August 13th",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/news/678088/google-pixel-10-release-date-launch-august?utm_source=tldrnewsletter",
      "summary": "2-minute read",
      "url": "https://www.theverge.com/news/678088/google-pixel-10-release-date-launch-august?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Of Course Workers Want a Four-Day Week. Companies Should Too",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.wsj.com/lifestyle/workplace/of-course-workers-want-a-four-day-week-companies-should-too-0837a0a1?st=nmPfYi&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "summary": "5-minute read",
      "url": "https://www.wsj.com/lifestyle/workplace/of-course-workers-want-a-four-day-week-companies-should-too-0837a0a1?st=nmPfYi&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "HTTP 401 - Request failed"
    },
    {
      "title": "What happened in the 2010s?",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.strangeloopcanon.com/p/what-happened-in-the-2010s?utm_source=tldrnewsletter",
      "summary": "12-minute read",
      "url": "https://www.strangeloopcanon.com/p/what-happened-in-the-2010s?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    },
    {
      "title": "Typing 118 WPM Broke My Brain in the Right Ways (A Year-Long Journey)",
      "content": "Typing 118 WPM Broke My Brain in the Right Ways (A Year-Long Journey) Viral on HN • June 2, 2025 • 6 min read Typing became my therapy. Not even kidding. Started at 60 WPM, felt like dragging my feet through mud every time I had to write code comments or documentation. Now? 118 WPM. No home row bullshit. No \"proper finger placement.\" Just pure, chaotic rhythm. Proof here because I know you don't believe me. Update: Posted this on HN and it went absolutely wild - hit for the day with ~150 points and ~200 comments. Guess I'm not the only one obsessed with typing fast. Check out the thread if you want to see the chaos. My Daily Brain Reset Ritual Every morning, before I even think about opening VS Code, before Slack starts destroying my soul, before the daily standup where we pretend we know what we're doing, I just type . Five minutes. That's it. It's like meditation but actually useful. Resets my brain, calms the static from yesterday's debugging session, and gets me in the zone. Way better than staring at my ceiling wondering why I chose this career. Speed is Useless if You're Wrong Here's the thing that took me way too long to figure out: rushing makes everything worse. Every time I tried to go fast, I'd mess up, backspace like crazy, and end up slower than when I started. Classic developer move, honestly. So I slowed down. Focused on hitting every key right. And boom — speed just... happened. 90 WPM, 100 WPM, 118 WPM. All came when I was hitting 100% accuracy consistently. It's like debugging. You can't just randomly change stuff and hope it works. You gotta be methodical, get it right, then optimize. Same energy. Some words still absolutely wreck me though. \"Obviously\" keeps coming out as \"obviousily\" and I want to throw my keyboard every time. So now I just slow down for those demon words and blast through the rest. Strategic speed management or whatever. Screw the Rules, Find Your Flow Plot twist: I don't type \"correctly\" at all. My fingers just go wherever they want. It's like anarchist typing. My left pinky probably hasn't touched the 'A' key in months, but somehow I'm still in the 99.5 percentile. Turns out the \"proper way\" is just a suggestion. Like following PEP 8 or using semicolons in JavaScript. Sure, it's nice, but if your way works better, who cares? When Your Brain is Fried You know those days when you've been staring at the same bug for 3 hours and your brain feels like mush? Or when you've been doomscrolling",
      "summary": "6-minute read",
      "url": "https://balaji-amg.surge.sh/blog/typing-118-wpm-brain-rewiring?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 442,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 442/1200 words (36.8%)"
    },
    {
      "title": "Digg's founders explain how they're building a site for humans in the AI era",
      "content": "The rebooted version of social site Digg aims to bring back the spirit of the old web at a time when AI-generated content is threatening to overwhelm traditional social media platforms, drowning out the voices of real people. This presents an opportunity to build a social site for the AI era, where the people who create content and manage online communities are given a bigger stake in a platform’s success, Digg’s founders think. A Web 2.0-era news aggregation giant, Digg was once valued at $175 million at its height back in 2008 and is now being given new life under the direction of its original founder, Kevin Rose, and Reddit co-founder Alexis Ohanian. The two recently teamed up to announce a new vision for Digg , which will focus on enabling discovery and community, the way that the early internet once allowed for. Speaking at The Wall Street Journal’s Future of Everything conference on Thursday, the founders offered more insight as to how they plan to accomplish that goal with the Digg reboot. Initially, the two touched on problems they encountered in the earlier days of social media, with Ohanian recalling how he chose to resign from Reddit’s board over disagreements about the company’s approach to hate speech that he felt was bad for society and the business. For instance, the company was allowing a forum on Reddit called “r/WatchPeopleDie” to continue operating up until the Christchurch mass shooting, which caught the attention of the media, he said. It was only then that Reddit decided to adjust its policies around violence and gore on the platform. After Reddit, Ohanian went on to found venture capital firm Seven Seven Six , where he says he’s focused on building businesses that are more “values-aligned.” He said he sees Digg as another step in that direction. Techcrunch event Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $200+ on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | July 15 REGISTER NOW Rose reflected on the early days of machine learning, where the technology was often used to reward posts on which people would rant about the “most obscure, kind of fringe-y weirdness,” he said. “Sometimes that can be good, but oftentimes it’s pushing really weird agendas. And that’s not even getting into the whole bot and AI side of things that are also pushing those agendas,” Rose said. With Digg, the founders want to create a new community focused on serving real people, not AI or bots, they said. Alexis Ohanian. Image Credits: WSJ's Future of Everything conference “I’ve long",
      "summary": "8-minute read",
      "url": "https://techcrunch.com/2025/06/02/diggs-founders-explain-how-theyre-building-a-site-for-humans-in-the-ai-era/?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 483,
      "content_extraction_status": "partial",
      "failure_reason": "Partial extraction: got 483/1600 words (30.2%)"
    },
    {
      "title": "AI didn't kill Stack Overflow",
      "content": "by Matthew Tyson Contributing Writer AI didn’t kill Stack Overflow feature May 28, 2025 9 mins Developer Generative AI Technology Industry How the groundbreaking developer forum moderated itself into oblivion. Credit: PeopleImages.com - Yuri A / Shutterstock It would be easy to say that artificial intelligence killed off Stack Overflow, but it would be truer to say that AI delivered the final blow. What really happened is a parable of human community and experiments in self-governance gone bizarrely wrong. Stack Overflow once was the internet’s senior engineer, the backstop where developers turned with problems that stymied us. It was a space for technical sharing where the ethos and attitude of open source programming thrived. Stack Overflow was not the first or only site of its kind. It was part of a wave of next-generation programming forums that appeared at the turn of the millennium, some of which still exist . And these themselves were the descendants of user groups and Usenet. As we enter the era of large language models (LLMs), all such forums are facing an existential crisis. Do we even need them anymore? Before we get too far into answering that question, let’s take a closer look at Stack Overflow: What made it great, how it stumbled, and how much of a role AI plays in its current decline. A long pattern of declining usage To start, we can look at this chart of declining monthly questions on Stack Overflow . The chart goes back as far as when the site began in 2008. As it shows, the site once reached a fairly sustained height of 200,000 new questions a month. After that, we see a pattern of gradual decline beginning in 2014. There is a COVID-19 bump in 2020, when everyone was working remotely, followed by a continuing collapse. By the beginning of 2023, about a year after ChatGPT erupted onto the scene, the bottom really drops out. It’s clear that ChatGPT and similar tools were the last straw for Stack Overflow. The site’s diminishing engagement almost exactly mirrors the rise of ChatGPT. But generative AI just put an exclamation mark on something that was already happening. What launched Stack Overflow into the stratosphere was human interaction and the vibrant culture that rose up around it. More than any other site like it, Stack Overflow captured the interactive component of software development. But then the experiment in self-moderation took on an oppressive tone, as its leaders systematically dismantled the very quality that made the platform great. By the time LLMs came along, Stack Overflow was already operating on an arid vision of transactional Q&A. When generative AI came for Stack Overflow, the one thing that could have saved it—the human element—had already been stripped out. The rep game: How Stack Overflow won, and then lost, the ‘net Stack Overflow’s most revolutionary aspect was its reputation system. That is what elevated it above the crowd. The brilliance of the rep game allowed Stack Overflow to absorb all the other user-driven sites for developers and more or less kill them off. On Stack Overflow, users earned reputation points and badges for asking good questions and providing helpful answers. In the beginning, what was considered a good question or answer was not predetermined; it was a natural byproduct of actual programmers upvoting some exchanges and not others. The reputation game was always imperfect: People could and did game the game. But whatever; it was fun, and most users found it helpful. So, what happened? Stack Overflow evolved toward being a so-called self-governing platform, where the power to govern (or moderate) was granted by reputation. Users with enough reputation were empowered to manage various aspects of the platform. Most importantly, they became responsible for moderating questions and answers for “quality.” For Stack Overflow, the new model, along with highly subjective ideas of “quality” opened the gates to a kind of Stanford Prison Experiment . Rather than encouraging a wide range of interactions and behaviors, moderators earned reputation by culling interactions they deemed irrelevant. Suddenly, Stack Overflow wasn’t a place to go and feel like you were part of a long-lived developer culture. Instead, it became an arena where you had to prove yourself over and over again . The downside of gamification Initially, gamification was a booster rocket. It took a beautiful aspect of software development culture—the mysterious joy of giving and receiving help for its own sake—and added a fun new way to score reputation. But what drove that helping culture? I remember a non-programmer looking over my shoulder once when I was on Stack Overflow. “Why do people help? Just for nothing?” The joy of being able to help someone by sharing what you’ve learned is something you must experience yourself to understand. Possibly the best analogy is seeing someone whose car has broken down on the side of the road. You pull over to help because you’ve been there; you know what being broken down on the side of the road feels like. Maybe you can help, and even if you can’t, at least the stranded driver knows someone cares. And then there is the boost of discovering the source of the problem: “Look, here’s a loose coolant clamp.” That shared thrill is what we lost when Stack Overflow let the reputation game win. Software development and the culture of helping Whether the culture of helping each other will survive in this new age of LLMs is a real question. Is human helping still necessary? Or can it all be reduced to inputs and outputs? Maybe there’s a new role for humans in generating accurate data that feeds the LLMs. Maybe we’ll evolve into gardeners of these vast new tracts of synthetic data. But returning to Stack Overflow and the community it once represented: Is there some radical resurrection in its future? Before AI entered the scene, it was clear Stack Overflow needed to back out of a dead-end street of its own creation. It’s possible the site could have returned to greatness by embracing what once made it great: the community and culture of software development. That culture thrives on making people feel welcome. Practically speaking, it means letting people with foolish or off-topic questions interact with those with more experience . Someday, they’ll become the ones with experience. Maybe they’ll come back to return the favor. It’s also clear that developers still want and appreciate community, even in the age of AI. We see that ethos alive and well in spaces like dev.to , and also in the success of GitHub’s open source, social coding model. GitHub is possibly the center of the coding universe, the true heir to the Iron Throne of the old user groups. Of course, it’s also just another useful tool, one that remains essential even in an AI-centric software universe. This perhaps boils down to the kernel at the heart of coding for coding’s sake. By nature, software developers will always create code, just like musicians are always producing music. Even if AI could produce great music, musicians would still do it. After all, we didn’t get to Bach, Beethoven, or the Beatles and just say, “Okay, we’re good, we have music now.” Humans have an inherent need to create, and for software developers, coding is how we do it. There is a way of writing, building, and doing software that is joyful, challenging, and rewarding. AI can be a part of that. But if it is allowed to wholesale replace it, the act of coding for coding’s sake starts to look more like an enthusiast’s hobby. It becomes akin to handcrafting wooden furniture pieces in the age of mass-produced furniture products. Don’t lose the human element Where does a site like Stack Overflow fit into this picture? Matt Asay recently offered some interesting ideas , like tying reputation to AI model contributions. But to truly make a comeback, Stack Overflow would have to believe in the future of human programmers and their culture. It would have to fundamentally say: This is a place where the human side of software development lives, and everything that happens here is in support of that basic mission. The rise and fall of Stack Overflow is a poignant reminder that platforms built for humans thrive on genuine community, not just generating content. Its genius was harnessing the enthusiasm of developers. That energy was gradually diminished by a bizarre twist where a working democracy sprouted an aristocracy, and that aristocracy killed democracy. The arrival of sophisticated AI happened in parallel, but it didn’t cause the collapse; it merely exposed the extent to which the community had already lost its spark. AI will continue to reshape the technological landscape, and the ramifications will continue to unfold before our very eyes. The lesson of Stack Overflow is even more important in the new world that is coming: Humans are the drivers of meaning and purpose. Subtract that human element at your own peril. by Matthew Tyson Contributing Writer Matthew Tyson is a founder of Dark Horse Group, Inc. He believes in people-first technology. When not programming or playing guitar, Matt explores the backcountry and the philosophical hinterlands. He has written for InfoWorld , CSOonline and other Foundry publications since 2007. More from this author how-to 10 JavaScript concepts you need to succeed with Node Jun 11, 2025 15 mins analysis JavaScript innovation and the culture of programming Jun 6, 2025 3 mins how-to JavaScript promises: 4 gotchas and how to avoid them Jun 4, 2025 8 mins feature Putting agentic AI to work in Firebase Studio May 21, 2025 14 mins feature 8 ways to do more with modern JavaScript May 7, 2025 14 mins analysis Experiments in JavaScript: What’s new in reactive design May 2, 2025 3 mins feature Catching up with Angular 19 Apr 30, 2025 7 mins feature Comparing Angular, React, Vue, and Svelte: What you need to know Apr 23, 2025 12 mins Show me more Popular Articles Videos news Apple rolls out Swift, SwiftUI, and Xcode updates By Paul Krill Jun 11, 2025 3 mins Development Tools Integrated Development Environments Swift news Salesforce changes Slack API terms to block bulk data access for LLMs By John E. Dunn Jun 11, 2025 1 min APIs Artificial Intelligence Generative AI news Databricks aims to optimize agent building for enterprises with Agent Bricks By Anirban Ghoshal Jun 11, 2025 5 mins Artificial Intelligence Generative AI Software Development video How to deploy AI agents with the Google Agent Development Kit for Python Jun 10, 2025 4 mins Python video Python 3.14's new template string feature Jun 6, 2025 5 mins Python video How to use the new Python Installation Manager tool for Python 3.14 May 27, 2025 4 mins Python",
      "summary": "9-minute read",
      "url": "https://www.infoworld.com/article/3993482/ai-didnt-kill-stack-overflow.html?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 1793,
      "content_extraction_status": "success",
      "failure_reason": null
    },
    {
      "title": "Airbnb CEO Brian Chesky wants to build the everything app",
      "content": "[CONTENT EXTRACTION FAILED] Unable to extract content from: https://www.theverge.com/decoder-podcast-with-nilay-patel/677324/airbnb-ceo-brian-chesky-services-redesign-app-future-travel?utm_source=tldrnewsletter",
      "summary": "78-minute read",
      "url": "https://www.theverge.com/decoder-podcast-with-nilay-patel/677324/airbnb-ceo-brian-chesky-services-redesign-app-future-travel?utm_source=tldrnewsletter",
      "published_date": "2025-06-03T00:00:00",
      "category": "ai",
      "word_count": 0,
      "content_extraction_status": "failed",
      "failure_reason": "Content validation failed - content too short, paywall detected, or formatting issues"
    }
  ]
}