{
  "url": "https://www.lesswrong.com/posts/PAYfmG2aRbdb74mEp/a-deep-critique-of-ai-2027-s-bad-timeline-models?utm_source=tldrai",
  "title": "A deep critique of AI 2027's bad timeline models",
  "content": "39 comments , sorted by top scoring Click to highlight new comments since: Today at 2:42 PM [ - ] elifland 8d 141 35 Thanks titotal for taking the time to dig deep into our model and write up your thoughts, it's much appreciated. This comment speaks for Daniel Kokotajlo and me, not necessarily any of the other authors on the timelines forecast or AI 2027. It addresses most but not all of titotal\u2019s post. Overall view: titotal pointed out a few mistakes and communication issues which we will mostly fix. We are therefore going to give titotal a $500 bounty to represent our appreciation. However, we continue to disagree on the core points regarding whether the model\u2019s takeaways are valid and whether it was reasonable to publish a model with this level of polish. We think titotal\u2019s critiques aren\u2019t strong enough to overturn the core conclusion that superhuman coders by 2027 are a serious possibility, nor to significantly move our overall median. Moreover, we continue to think that AI 2027\u2019s timelines forecast is (unfortunately) the world\u2019s state-of-the-art, and challenge others to do better. If instead of surpassing us, people simply want to offer us critiques, that\u2019s helpful too; we hope to surpass ourselves every year in part by incorporating and responding to such critiques. Clarification regarding the updated model My apologies about quietly updating the timelines forecast with an update without announcing it; we are aiming to announce it soon. I\u2019m glad that titotal was able to see it. A few clarifications: titotal says \u201cit predicts years longer timescales than the AI2027 short story anyway.\u201d While the medians are indeed 2029 and 2030, the models still give ~25-40% to superhuman coders by the end of 2027. Other team members (e.g. Daniel K) haven\u2019t reviewed the updated model in depth, and have not integrated it into their overall views. Daniel is planning to do this soon, and will publish a blog post about it when he does. Most important disagreements I'll let titotal correct us if we misrepresent them on any of this. Whether to estimate and model dynamics for which we don't have empirical data. e.g. titotal says there is \"very little empirical validation of the model,\" and especially criticizes the modeling of superexponentiality as having no empirical backing. We agree that it would be great to have more empirical validation of more of the model components, but unfortunately that's not feasible at the moment while incorporating all of the highly relevant factors. [1] Whether to adjust our estimates based on factors outside the data. For example, titotal criticizes us for making judgmental forecasts for the date of RE-Bench saturation , rather than plugging in the logistic fit. I\u2019m strongly in favor of allowing intuitive adjustments on top of quantitative modeling when estimating parameters. [Unsure about level of disagreement] The value of a \"least bad\" timelines model. While the model is certainly imperfect due to limited time and the inherent difficulties around forecasting AGI timelines, we still think overall it\u2019s the \u201cleast bad\u201d timelines model out there and it\u2019s the model that features most prominently in my overall timelines views. I think titotal disagrees, though I\u2019m not sure which one they consider least bad (perhaps METR\u2019s simpler one in their time horizon paper ?). But even if titotal agreed that ours was \u201cleast bad,\u201d my sense is that they might still be much more negative on it than us. Some reasons I\u2019m excited about publishing a least bad model: Reasoning transparency. We wanted to justify the timelines in AI 2027, given limited time. We think it\u2019s valuable to be transparent about where our estimates come from even if the modeling is flawed in significant ways. Additionally, it allows others like titotal to critique it. Advancing the state of the art. Even if a model is flawed, it seems best to publish to inform others\u2019 opinions and to allow others to build on top of it. The likelihood of time horizon growth being superexponential, before accounting for AI R&D automation. See this section for our arguments in favor of superexponentiallity being plausible, and titotal\u2019s responses (I put it at 45% in our original model). This comment thread has further discussion. If you are very confident in no inherent superexponentiality, superhuman coders by end of 2027 become significantly less likely, though are still >10% if you agree with the rest of our modeling choices (see here for a side-by-side graph generated from my latest model). How strongly superexponential the progress would be. This section argues that our choice of superexponential function is arbitrary. While we agree that the choice is fairly arbitrary and ideally we would have uncertainty over the best function, my intuition is that titotal\u2019s proposed alternative curve feels less plausible than the one we use in the report, conditional on some level of superexponentiality. Whether the argument for superexponentiality is stronger at higher time horizons. titotal is confused about why there would sometimes be a delayed superexponential rather than starting at the simulation starting point. The reasoning here is that the conceptual argument for superexponentiality is much stronger at higher time horizons (e.g. going from 100 to 1,000 years feels likely much easier than going from 1 to 10 days, while it\u2019s less clear for 1 to 10 weeks vs. 1 to 10 days). It\u2019s unclear that the delayed superexponential is the exact right way to model that, but it\u2019s what I came up with for now. Other disagreements Intermediate speedups: Unfortunately we haven\u2019t had the chance to dig deeply into this section of titotal\u2019s critique, and it\u2019s mostly based on the original version of the model rather than the updated one so we probably will not get to this. The speedup from including AI R&D automation seems pretty reasonable intuitively at the moment (you can see a side-by-side here ). RE-Bench logistic fit ( section ): We think it\u2019s reasonable to set the ceiling of the logistic at wherever we think the maximum achievable performance would be. We don\u2019t think it makes any sense to give weight to a fit that achieves a maximum of 0.5 when we know reference solutions achieve 1.0 and we also have reason to believe it\u2019s possible to get substantially higher. We agree that we are making a guess (or with more positive connotation, \u201cestimate\u201d) about the maximum score, but it seems better than the alternative of doing no fit. Mistakes that titotal pointed out We agree that the graph we\u2019ve tweeted is not closely representative of the typical trajectory of our timelines model conditional on superhuman coders in March 2027. Sorry about that, we should have prioritized making it more precisely faithful to the model. We will fix this in future communications. They convinced us to remove the public vs. internal argument as a consideration in favor of superexponentiality ( section ). We like the analysis done regarding the inconsistency of the RE-Bench saturation forecasts with an interpolation of the time horizons progression. We agree that it\u2019s plausible that we should just not have RE-Bench in the benchmarks and gaps model; this is partially an artifact of a version of the model that existed before the METR time horizons paper. In accordance with our bounties program , we will award $500 to titotal for pointing these out. Communication issues There were several issues with communication that titotal pointed out which we agree should be clarified, and we will do so. These issues arose from lack of polish rather than malice. 2 of the most important ones: The \u201cexponential\u201d time horizon case still has superexponential growth once you account for automation of AI R&D. The forecasts for RE-Bench saturation were adjusted based on other factors on top of the logistic fit. ^ Relatedly, titotal thinks that we made our model too complicated, while I think it's important to make our best guess for how each relevant factor affects our forecast. Reply 1 [ - ] Thane Ruthenis 6d 43 20 So I'm kind of not very satisfied with this defence. Not-very-charitably put, my impression now is that all the technical details in the forecast were free parameters fine-tuned to support the authors' intuitions [1] , when they weren't outright ignored. Now, I also gather that those intuitions were themselves supported by playing around with said technical models, and there's something to be said about . I'm not saying the forecast should be completely dismissed because of that. ... But \"the authors, who are smart people with a good track record of making AI-related predictions, intuitively feel that this is sort of right, and they were able to come up with functions whose graphs fit those intuitions\" is a completely different kind of evidence compared to \"here's a bunch of straightforward extrapolations of existing trends, with non-epsilon empirical support, that the competent authors intuitively think are going to continue\". Like... I, personally, didn't put much stock in the technical-analysis part to begin with [2] , I only updated on the \"these authors have these intuitions\" part (to which I don't give trivial weight!). But if I did interpret the forecast as being based on intuitively chosen but non-tampered straightforward extrapolations of existing trends, I think I would be pretty disappointed right now. You should've maybe put a \"these graphs are for illustrative purposes only\" footnote somewhere, like this one did. I don't feel that \"this is the least-bad forecast that exists\" is a good defence. Whether an analysis is technical or vibes-based is a spectrum, but it isn't graded on a curve . I'm kind of split about this critique, since the forecast did end up as good propaganda if nothing else . But I do now feel that the marketing around it was kind of misleading, and we probably care about maintaining good epistemics here or something. ^ If you've picked which function to fit, and it's very sensitive to small parameter changes, and you pick the parameters that intuitively feel right, I think you might as well draw the graph by hand. ^ Because I don't think AGI/researcher-level AIs have been reduced to an engineering problem, I think theoretical insights are missing, which means no straight-lines extrapolation is possible and we can't do better than a memoryless exponential distribution . And whether this premise is true is itself an intuitive judgement call, and even fully rigorous technical analyses premised on an intuitive judgement call are only as rigorous as the intuitive judgement call. Reply 3 3 2 [ - ] habryka 6d 41 7 I think the actual epistemic process that happened here is something like: The AI 2027 authors had some high-level arguments that AI might be a very big deal soon They wrote down a bunch of concrete scenarios that seemed like they would follow from those arguments and checked if they sounded coherent and plausible and consistent with lots of other things they thought about the world As part of that checking, one thing they checked was whether these scenarios would be some kind of huge break from existing trends, which I do think is a hard thing to do, but is an important thing to pay attention to The right way to interpret the \"timeline forecast\" sections is not as \"here is a simple extrapolation methodology that generated our whole worldview\" but instead as a \"here is some methodology that sanity-checked that our worldview is not in obvious contradiction to reasonable assumptions about economic growth\" But like, at least for me, it's clear to me that the beliefs about takeoff and the exact timelines, could not be, and obviously should not be, considered the result of a straightforward and simple extrapolation exercise. I think such an exercise would be pretty doomed, and a claim to objectivity in that space seems misguided. I think it's plausible that some parts of the Timelines Forecast supplement ended up communicating too much objectivity here, but IDK, I think AI 2027 as a whole communicated this process pretty well, I think. Reply 2 [ - ] Thane Ruthenis 6d 10 5 But like, at least for me, it's clear to me that the beliefs about takeoff and the exact timelines, could not be, and obviously should not be, considered the result of a straightforward and simple extrapolation exercise Counterpoint: the METR agency-horizon doubling trend. It has its issues, but I think \"the point at which an AI could complete a year-long software-engineering/DL research project\" is a reasonable cutoff point for \"AI R&D is automated\", and it seems to be the kind of non-overly-fine-tuned model with non-epsilon empirical backing that I'm talking about, in a way AI 2027 graphs are not. Or maybe the distinction isn't as stark in others' minds as in mine, I dunno. As part of that checking, one thing they checked was whether these scenarios would be some kind of huge break from existing trends, which I do think is a hard thing to do Is it? See titotal's six-stories section . If you're choosing which function to fit, with a bunch of free parameters you set manually, it seems pretty trivial to come up with a \"trend\" that would fit any model you have. Reply [ - ] habryka 6d 8 8 Counterpoint: the METR agency-horizon doubling trend. It has its issues, but I think \"the point at which an AI could complete a year-long software-engineering/DL research project\" is a reasonable cutoff point for \"AI R&D is automated\", and it seems to be the kind of non-overly-fine-tuned model with non-epsilon empirical backing that I'm talking about, in a way AI 2027 graphs are not. I think the METR horizon doubling trend stuff doesn't stand on its own , and it's really not many datapoints. I also really don't think, without a huge number of assumptions, that \"the point at which an AI could complete a year-long software-engineering/DL research project\" is a good proxy for \"AI R&D automation\", and indeed I want to avoid exactly that kind of sleight of hand. I t only makes sense to someone who has a much more complicated worldview about how general AI is likely to be, how much the tasks METR measured are likely to generalize, and many other components. What it does make sense for is as a sanity-check on that broader worldview. Reply 1 1 [ - ] Thane Ruthenis 6d 4 0 I think the METR horizon doubling trend stuff doesn't stand on its own, and it's really not many datapoints. It's less about the datapoints and more about the methodology. I also really don't think, without a huge number of assumptions, that \"the point at which an AI could complete a year-long software-engineering/DL research project\" is a good proxy for \"AI R&D automation\" Fair, I very much agree. But my point here is that the METR benchmark works as some additional technical/empirical evidence towards some hypotheses over others, evidence that's derived independently from one's intuitions, in a way that more fine-tuned graphs don't work. Reply [ - ] Ben Pace 6d 2 0 Those two things sound extremely similar to me, I would appreciate some explanation/pointer to why they seem quite different. Current guess: Is the idea that automation includes also a lot of (a) management, and (b) research taste in choosing projects, such that being able to complete a year-long project is only a lower-bound, not a central target? Reply [ - ] habryka 6d 8 10 Yeah, I mean, the task distribution is just hugely different. When METR measures software-developing tasks, they mean things in the reference class of well-specified tasks with tests basically already written. As a concrete example, if you just use a random other distribution of tasks for horizon length as your base, like forecasting performance for unit of time, or writing per unit of time, or graphic design per unit of time, you get extremely drastically different time horizon curves. This doesn't make METR's curves unreasonable as a basis, but you really need a lot of assumptions to get you from \"these curves intersect one year here\" to \"the same year we will get ~fully automated AI R&D\" (and indeed I would not currently believe the latter). Reply [ - ] Xodarap 5d 5 -6 Preliminary work showing that the METR trend is approximately average: Reply 1 [ - ] habryka 5d 4 3 I don\u2019t know the details of all of these task distributions, but clearly these are not remotely sampled uniformly from the set of all tasks necessary to automate AI R&D? Reply [ - ] Thomas Kwa 3d 4 0 Yes, in particular the concern about benchmark tasks being well-specified remains. We'll need both more data (probably collected from AI R&D tasks in the wild) and more modeling to get a forecast for overall speedup. However, I do think if we have a wide enough distribution of tasks, AIs outperform humans on all of them at task lengths that should imply humans spend 1/10th the labor, but AI R&D has not been automated yet, something strange needs to be happening. So looking at different benchmarks is partial progress towards understanding the gap between long time horizons on METR's task set and actual AI R&D uplift. Reply 1 [ - ] Xodarap 4d 4 2 (agree, didn't intend to imply that they were) Reply [ - ] TurnTrout 4d 37 38 since the forecast did end up as good propaganda if nothing else Just responding to this local comment you made: I think it's wrong to make \"propaganda\" to reach end Y, even if you think end Y is important. If you have real reasons for believing something will happen, you shouldn't have to lie, exaggerate, or otherwise mislead your audience to make them believe it, too. So I'm arguing that you shouldn't have mixed feelings because ~\"it was valuable propaganda at least.\" Again, not trying to claim that AI 2027 \"lied\" - just replying to the quoted bit of reasoning. Reply 3 1 [ - ] Thane Ruthenis 4d 20 5 I phrased that badly/compressed too much. The background feeling there was that my critique may be of an overly nitpicky type that no normal person would care about, but the act-of-critiquing was still an attack on the report if viewed through the lens of a social-status game, which may (on the margins) unfairly bias someone against the report. Like, by analogy, imagine a math paper involving a valid but hard-to-follow proof of some conjecture that for some reason gets tons of negative attention due to bad formatting. This may incorrectly taint the core message by association, even though it's completely valid. Reply [ - ] elifland 6d 27 11 I'm kind of split about this critique, since the forecast did end up as good propaganda if nothing else. But I do now feel that the marketing around it was kind of misleading, and we probably care about maintaining good epistemics here or something. I'm interested in you expanding on which parts of the marketing were misleading. Here are some quick more specific thoughts: Overall AI 2027 comms In our website frontpage , I think we were pretty careful not to overclaim. We say that the forecast is our \"best guess\", \"informed by trend extrapolations, wargames, ...\" Then in the \"How did we write it?\" box we basically just say it was written iteratively and informed by wargames and feedback. In \"Why is it valuable?\" we say \"We have set ourselves an impossible task. Trying to predict how superhuman AI in 2027 would go is like trying to predict how World War 3 in 2027 would go, except that it\u2019s an even larger departure from past case studies. Yet it is still valuable to attempt, just as it is valuable for the US military to game out Taiwan scenarios.\" I don't think we said anywhere that it was backed up by straightforward, strongly empirically validated extrapolations. In our initial tweet, Daniel said it was a \"deeply researched\" scenario forecast. This still seems accurate to me, we spent quite a lot of time on it (both the scenario and supplements) and I still think our supplementary research is mostly state of the art , though I can see how people could take it too strongly. In various follow-up discussions, I think Scott and others sometimes pointed to the length of all of the supplementary research as justification for taking the scenario seriously. I still think this mostly holds up but again I think it could be interpreted in the wrong way. Probably there has been similar discussion in various podcast appearances etc., but I haven't listened to most of those and don't remember how this sort of thing was presented in the ones I did listen to. Timelines forecast specific comms We do not say prominently explicitly in the timelines forecast that it relies on a bunch of non-obvious parameter choices rather than just empirical trend extrapolation, so I agree that people could come away with the wrong impression. Plausibly we should have had / I should add a disclaimer saying something like this. I have been frustrated with previous forecasts for not communicating this well, so plausibly I'm being hypocritical. One reason I'm hesitant to add this is that I think it might update non-rationalists too much toward thinking it's useless, when in fact I think it's pretty informative . But this might be motivated reasoning toward the choice I made before. I might add a disclaimer. I didn't explicitly consider adding a prominent disclaimer previously; perhaps because I was typical minding and thinking it was obvious that any AGI timelines forecast will rely on intuitively estimated parameters. However, I think that including 3 different people/groups' forecasts very prominently does implicitly get across the idea that different parameter estimations can lead to very different results. This is especially true for including the FutureSearch aggregate, which has a within-model median of 2032 rather than 2027 or 2028. There's a graph at the top of the timelines forecast with all 3 of our distributions, and in my tweet thread about the timelines forecast this was in my top tweet. As I've said, I agree that we messed up to some extent re: the time horizon prediction graph. I might write more about this in response to TurnTrout. Not-very-charitably put, my impression now is that all the technical details in the forecast were free parameters fine-tuned to support the authors' intuitions, when they weren't outright ignored. Now, I also gather that those intuitions were themselves supported by playing around with said technical models, and there's something to be said about . I'm not saying the forecast should be completely dismissed because of that. I tried not to just fine-tune the parameters to support my existing beliefs, though I of course probably implicitly did to some extent. I agree that the level of free parameters is a reason to distrust our forecasts. FWIW, my and Daniel's timelines beliefs have both shifted some as a result of our modeling. Mine initially got shorter then got a bit longer due to the most recent update, Daniel moved his timelines longe r to 2028 in significant part because of our timelines model. ... But \"the authors, who are smart people with a good track record of making AI-related predictions, intuitively feel that this is sort of right, and they were able to come up with functions whose graphs fit those intuitions\" is a completely different kind of evidence compared to \"here's a bunch of straightforward extrapolations of existing trends, with non-epsilon empirical support, that the competent authors intuitively think are going to continue\". Mostly agree. I would say we have more than non-epsilon empirical support though because of METR's time horizons work and RE-Bench. But I agree that there are a bunch of parameters estimated that don't have much empirical support to rely on. But if I did interpret the forecast as being based on intuitively chosen but non-tampered straightforward extrapolations of existing trends, I think I would be pretty disappointed right now. I don't agree with the connotation of \"non-tampered,\" but otherwise agree re: relying on straightforward extrapolations. I don't think it's feasible to only rely on straightforward extrapolations when predicting AGI timelines. You should've maybe put a \"these graphs are for illustrative purposes only\" footnote somewhere, like this one did. I think \"illustrative purposes only\" would be too strong. The graphs are the result of an actual model that I think is reasonable to give substantial weight to in one's timelines estimates (if you're only referring to the specific graph that I've apologized for, then I agree we should have moved more in that direction re: more clear labeling). I don't feel that \"this is the least-bad forecast that exists\" is a good defence. Whether an analysis is technical or vibes-based is a spectrum, but it isn't graded on a curve . I'm not sure exactly how to respond to this. I agree that the absolute level of usefulness of the timelines forecast also matters, and I probably think that our timelines model is more useful than you do. But also I think that the relative usefulness does matter quite a bit on the decision of whether to release and publicize model. I think maybe this critique is primarily coupled with your points about communication issues. [Unlike the top-level comment, Daniel hasn't endorsed this, this is just Eli.] Reply 4 1 1 [ - ] Thane Ruthenis 6d 9 -2 I'm interested in you expanding on which parts of the marketing were misleading Mostly this part, I think: In various follow-up discussions, I think Scott and others sometimes pointed to the length of all of the supplementary research as justification for taking the scenario seriously. I still think this mostly holds up but again I think it could be interpreted in the wrong way. Like, yes, the supplementary materials definitely represent a huge amount of legitimate research that went into this. But the forecasts are \"informed by\" this research, rather than being directly derived from it, and the pointing-at kind of conveys the latter vibe. I have been frustrated with previous forecasts for not communicating this well Glad you get where I'm coming from; I wasn't wholly sure how legitimate my complaints were. One reason I'm hesitant to add [a disclaimer about non-obvious parameter choices] is that I think it might update non-rationalists too much toward thinking it's useless, when in fact I think it's pretty informative I agree that this part is tricky, hence my being hesitant about fielding this critique at all. Persuasiveness isn't something we should outright ignore, especially with something as high-profile as this. But also, the lack of such a disclaimer opens you up to takedowns such as titotal's, and if one of those becomes high-profile (which it already might have?), that'd potentially hurt the persuasiveness more than a clear statement would have. There's presumably some sort of way to have your cake and eat it too here; to correctly communicate how the forecast was generated, but in terms that wouldn't lead to it being dismissed by people at large. I think \"illustrative purposes only\" would be too strong. Yeah, sorry, I was being unnecessarily hyperbolic there. Reply [ - ] titotal 3d 12 2 I'm leaving the same comment here and in reply to daniel on my blog. First, thank you for engaging in good faith and rewarding deep critique. Hopefully this dialogue will help people understand the disagreements over AI development and modelling better, so they can make their own judgements. I think I\u2019ll hold off on replying to most of the points there, and make my judgement after Eli does an in-depth writeup of the new model. However, I did see that there was more argumentation over the superexponential curve, so I\u2019ll try out some more critiques here: not as confident about these, but hopefully it sparks discussion. The impressive achievements in LLM capabilities since GPT-2 have been driven by many factors, such as drastically increased compute, drastically increased training data, algorithmic innovations such as chain-of-thought, increases in AI workforce, etc. The extent that each contributes is a matter of debate, which we can save for when you properly write up your new model. Now, let\u2019s look for a second at what happens when the curve goes extreme: using median parameters and starting the superexponentional today, the time horizon of AI would improve from one-thousand work-years to ten-thousand work-years In around five weeks. So you release a model, and it scores 80% on 1000 work year tasks, but only like 40% on 10,000 work year tasks (the current ratio of 50% to 80% time horizons is like 4:1 or so). Then five weeks later you release a new model, and now the reliability on the much harder tasks has doubled to 80%. Why? What causes the reliability to shoot up in five weeks? The change in the amount of available compute, reference data, or labor force will not be significant in that time, and algorithmic breakthroughs do not come with regularity. It can\u2019t be due to any algorithmic speedups from AI development because that\u2019s in a different part of the model: we\u2019re talking about three weeks of normal AI development, like it\u2019s being done by openAI as it currently stands.. If the AI is only 30x faster than humans, then the time required for the AI to do the thousand year task is 33 years! So where does this come from? Will we have developed the perfect algorithm, such that AI no longer needs retraining? I think a mistake could be made in trying to transfer intuition about humans to AI here: perhaps the intuition is \u201chey, a human who is good enough to do a 1 year task well can probably be trusted to do a 10 year task\u201d. However, if a human is trying to reliably do a \u201c100 year\u201d task (a task that would take a team of a hundred about a year to do), this might involve spending several years getting an extra degree in the subject, read a ton of literature, improving their productivity, get mentored by an expert in the subject, etc. While they work on it, they learn new stuff and their actual neurons get rewired. But the AI equivalent to this would be getting new algorithms, new data, new computing power, new training. ie, becoming an entirely new model, which would take significantly more than a few weeks to be built. I think there may be some double counting going on between this superexp and the superexp from algo speedups. Reply [ - ] Tom Davidson 4d 6 0 Re intermediate speed ups : a simple fix You currently have the pace of total progress growing exponentially as AI improves. And this leads the bad back-predictions that the pace of progress used to be much slower. I think your back predictions would be fine if you said that total progress = human-driven progress + AI-driven progress, and then had only the AI part grow exponentially. Then in the back prediction the AI part would rapidly shrink but the human part would remain. Reply [ - ] Lukas Finnveden 6d 44 22 Thanks very much for this post! Really valuable to see external people dig into these sorts of models and report what they find. But these beliefs are hard to turn into precise yearly forecasts, and I think doing so will only cement overconfidence and leave people blindsided when reality turns out even weirder than you imagined. I think people are going to deal with the fact that it\u2019s really difficult to predict how a technology like AI is going to turn out. The massive blobs of uncertainty shown in AI 2027 are still severe underestimates of the uncertainty involved. If your plans for the future rely on prognostication, and this is the standard of work you are using, I think your plans are doomed. I would advise looking into plans that are robust to extreme uncertainty in how AI actually goes, and avoid actions that could blow up in your face if you turn out to be badly wrong. Does this mean that you would overall agree with a recommendation to treat 2027 as a plausible year that superhuman coders might arrive, if accompanied with significant credence on other scenarios? It seems to me like extreme uncertainty should encompass \"superhuman coders in 2027\" (given how fast recent AI progress has been), and \"not preparing for extremely fast AI progress\" feels very salient to me as a sort of action that could blow up in your face if you turn out to be badly wrong. FWIW, I would guess that the average effect of people engaging with AI 2027 is to expand the range of possible scenarios that people are imagining, such that they're now able to imagine a few more highly weird scenarios in addition to some vague \"business as usual\" baseline assumption. By comparison, I would guess it's a lot more rare for people to adopt high confidence that the AI 2027 scenario is correct. So by the lights of preventing overconfidence and the risk of getting blindsided, AI 2027 looks very valuable to me. Reply 2 [ - ] Lukas Finnveden 6d 29 24 There are a few things I am confident of, such as a software-only singularity not working Have you written up the argument for this anywhere? I'd be interested to read it. (I'm currently close to 50-50 on software singularity, and I currently think it seems extremely difficult to reach confidence that it won't happen, given how sparse and unconclusive the current empirical data is.) Reply [ - ] Jonas V 6d 26 6 Nitpick: although some of these reviewers only saw bits of it . Gary Marcus was shared the full draft including all the background research / forecast drafts. So it would be more accurate to say \"only read bits of it\". Reply [ - ] TurnTrout 7d 13 -9 I am concerned that Scott and Daniel have graphed new LLM performance on this unrelated curve and presented it as evidence in favour of their model, even if they have been clear that it is \u201cweak\u201d evidence. It\u2019s wrong to present this curve as \u201cAI 2027\u2019s prediction\u201d, as Scott did. Wow, this is really bad. I consider the inclusion of this graph to be deceptive. AFAICT this graph never should have existed to begin with. Reply [ - ] elifland 6d 30 2 I'll say various facts as best as I can recall and allow you and others to decide how bad/deceptive the time horizon prediction graph was. The prediction on the graph was formed by extrapolating a superexponential with a 15% decay. This was set to roughly get SC at the right time, based on an estimate for what time horizon is needed for SC that is similar to my median in the timelines forecast. This is essentially a simplified version of our time horizon extension model that doesn't account for AI R&D automation. Or another way to view this is that we crudely accounted for AI R&D automation by raising the decay. This was not intended to represent our overall median forecast as that is later, but instead to represent roughly the trajectory that happens in AI 2027. As is shown in titotal's post, the graph is barely in distribution for the trajectories of our timelines model which reach SC in Mar 2027, it's certainly not central. We did not check this before the AI 2027 release. Why didn't we use a central trajectory from our timelines model rather than the simplified version? This was on my TODO list, but I ran out of time. As you can imagine, we were working right up until a deadline and didn't get to many TODOs that would have been great to have. But very likely I should have prioritized it more highly, so this is my mistake. Or we should have more clearly labeled that the graph was not generated via the timelines model. If we had the correct graph, then the new model releases would have been a bit above our predicted trend, rather than right on it. So it should be a very slight update toward the plausibility of shorter timelines than AI 2027. Reply [ - ] TurnTrout 4d 17 5 Thanks, I appreciate your comments. This is essentially a simplified version of our time horizon extension model that doesn't account for AI R&D automation. Or another way to view this is that we crudely accounted for AI R&D automation by raising the decay. Why did you simplify the model for a graph? You could have plotted a trajectory to begin with, instead of making a bespoke simplification. Is it because you wanted to \"represent roughly the trajectory that happens in AI 2027\"? I get that AI 2027 is a story, but why not use your real model to sample a trajectory -- perhaps rejection sampling until you get one of the more aggressive possibilities? Or you could even rejection sample the model until you get one that matches AI 2027 pretty closely, and then draw that curve's projection (and retrojection -- wait is that even a word). I'm currently watching the tension between \"this is just a story [which doesn't have hard data behind it, take it with a big grain of salt]\" and \"here's some math supporting our estimates [but wasn't actually used for our plots or the story in any direct way].\" I'm worried that the math lends credibility without being that relevant to the real decisions . Reply 1 1 [ - ] Daniel Kokotajlo 2d 6 2 Why not use the real model to sample a trajectory? We should indeed have done that, but it was annoying and would have taken more time & this 15% thing seemed a good enough approximation given the massive error bars and uncertainty involved. It's not like we had the real trajectory sample ready to go and decided to do the 15% thing instead. I'm currently watching the tension between \"this is just a story [which doesn't have hard data behind it, take it with a big grain of salt]\" and \"here's some math supporting our estimates [but wasn't actually used for our plots or the story in any direct way].\" I'm worried that the math lends credibility without being that relevant to the real decisions. I agree we should have been more clear about various things. Like, we have our scenario. Why did we choose to depict takeoff in 2027? Because at the time we were writing, that was my median. (Over the course of writing, my median shifted to 2028. Eli's meanwhile was always longer, more like 2032 iirc. This is awkward but nbd, we all still think 2027 is plausible. We even said in the very first footnote that our actual medians were somewhat later than what the scenario depicts, and that the scenario depicts something more like our mode. And iirc I said it to Kevin Roose as well in the initial interview.) OK, so why should people take seriously our median/mode/etc. and our views on timelines more generally? Well, we weren't claiming people should trust us absolutely or anything like that. We think that we've done an unusually high amount of thinking and research on the topic and have good track records, but we aren't asking people to trust us. We put up our latest thoughts on timelines (at least, latest-at-the-time-of-writing, so early 2025) on the webpage, for all to see and critique. We are hoping and planning to continue to improve our models of everything. Reply [ - ] TurnTrout 4d 9 1 Or we should have more clearly labeled that the graph was not generated via the timelines model. Yes, I think this would have been quite good. Reply 2 [ - ] Tom Davidson 4d 11 10 I'd be interested to hear more, when just vague intuitions, for why you're confident there won't be a software only intelligence explosion? Reply [ - ] Lukas Finnveden 6d 11 5 I don\u2019t buy this claim. Just think about what a time horizon of a thousand years means: this is a task that would take an immortal CS graduate a thousand years to accomplish, with full internet access and the only requirement being that they can\u2019t be assisted another person or an LLM. An AI that could accomplish this type of task with 80% accuracy would be a superintelligence. And an infinite time horizon, interpreted literally, would be a task that a human could only accomplish if given an infinite amount of time. I think given a Graham\u2019s number of years a human could accomplish a lot, so I don\u2019t think the idea that time horizons should shoot to infinity is reasonable. But importantly, the AI would get the same resources as the human! If a CS graduate would need 1000 years to accomplish the task, the AI would get proportionally more time. So the AI wouldn't have to be a superintelligence anymore than an immortal CS graduate is a superintelligence. Similarly, given a Graham's number of years a human could accomplish a lot. But given a Graham's number of years, an AI could also accomplish a lot. Overall, the point is just that: If you think that broadly superhuman AI is possible, then it should be possible to construct an AI that can match humans on tasks of any time horizon (as long as the AI gets commensurate time). Reply [ - ] Thomas Larsen 8d 7 3 Small typo: Alog(B) = log(B^A), not log(A^B) Reply [ - ] CaseyMilkweed 7d 5 0 This critique seemed very persuasive to me. Thank you for putting it together. The timeline forecast is blended distribution of the superexponential (40% - 45%), exponential (45% - 50%), and subexponential (10%). I would think there is going to be a pretty consistent rank-ordering, where almost all of the mass of the superexponential is earlier than the almost all of the mass of the exponential. Similarly, almost all of the mass of the subexponential is going to be later than either the exponential or superexponential. This is a simplification, but running with it for a moment... Because the super-exponential block contains < 50 % of the total probability mass, the overall median will come from the exponential block, likely in the earliest 10\u201320 % of exponential outcomes (the percentile needed to lift cumulative probability to 50 % once the super-exponential weight is counted). One weird quirk of this is that the more uncertainty they build into the parameters for their exponential (i.e. the wider the lognormals for the prior distributions), the earlier their median prediction will be. Because the median is always going to end up being one of the fastest exponentials, and building in more uncertainty will just make it go faster. Titotal - is that a good way to think about it? Reply [ - ] Peter Johnson 7d 5 0 Hey! Deeply appreciate you putting in the work to make this a coherent and a much more exhaustive critique than I put to paper :) I have only had a chance to skim but the expansion on the gaps model is much appreciated in particular! (also want to stress the authors have been very good at engaging with critiques and I am happy to see that has continued here) Reply [ - ] Lukas Finnveden 6d 4 1 An 80% \u201ctime horizon\u201d of 1 hour would mean that an AI has an overall success rate of 80% on a variety of selected tasks that would take a human AI researcher 1 hour to complete, presumably taking much less time than the humans (although I couldn\u2019t find this statement explicitly). Figure 13 describes the ratio of AI cost to human cost, which is close to what you're after. (Though if you care about serial time in particular, that could differ quite a bit from cost.) Reply [ - ] brp 21h 3 0 This is good critique of the details of AI 2027 , but whether the prediction should have been for autonomous AI research by 2026 or 2033, it doesn't look like anything substantive is changing among the policy concerns that the AI 2027 raises. I think Nikola's threshold for superhuman AI is conservative enough. If we reach a point where an AI agent (or super-agent) can perform tasks equivalent to 10 human-years of programmer time with 80% accuracy, then it is likely that AI research can be divided between several agents and completely automated. In my opinion, humanity will have lost control of AI by this point: much like the PI of a research lab never knows all the technical details of how their experiments are actually performed, by this point even the leading edge of human researchers are likely to fail to understand the research they are overseeing beyond that of a surface-level abstraction. From well before the point at which humans can no longer understand AI self-improvement, all of AI 2027's warnings about social and organizational dynamics are relevant: the incentives push companies to ignore the initial warning signs of autonomous and misaligned (evil) behavior, opening the door to potential catastrophe. Your graph (\"six stories\") shows that METR's plain-old-exponential prediction would put us at this point before 2032, and the \"new normal\" METR curve based on the most recent model releases would put us there before 2028. So the current paradigm is such that super-exponential growth is not even needed to enter dystopia before 2032, and the uncertainties are such that entering dystopia before 2028 is still a possibility. Getting the details right is important, but this critique reinforces my impression that AI 2027 is important. I only hope that AI 2027 skeptics don't start pointing at the headline (\"bad\") to argue against making meaningful policy and regulatory changes. Reply [ - ] Archimedes 6d 2 2 Backcasting AI speedup feels strange to me. Below a certain usefulness threshold, the researchers don\u2019t use AI assistance for research. It doesn\u2019t actively slow them down below that threshold, so it bottoms out at whatever their unassisted rate is. Reply [ - ] Ujjwal Singhal 2d 1 0 Where H0 is your doubling time at t_start Small correction: H0 is the time horizon at t_start. Reply [ - ] lumpenspace 5d -3 0 great work. i'd like to contribute to your future research; please share a bitcoin address. i am not here often, but you can contact me on X (same username). Reply [ + ] arisAlexis 4d -15 -21 Moderation Log",
  "published_date": "2025-06-24 00:00:00",
  "category": "ai",
  "word_count": 7778,
  "content_extraction_status": "partial",
  "failure_reason": "Partial extraction: got 7778/14400 words (54.0%)",
  "feed_name": "TLDR AI",
  "scraped_at": "2025-06-28T00:42:10.629118"
}