{
  "url": "https://www.lesswrong.com/posts/uAEhvX6scvcZANWwg/a-guide-for-llm-assisted-web-research?utm_source=tldrai",
  "title": "A Guide For LLM-Assisted Web Research",
  "original_content": "It's hard to imagine doing web research without using LLMs. Chatbots may be the first thing you turn to for questions like: What are the companies currently working on nuclear fusion and who invested in them? What is the performance gap between open and closed-weight models on the MMLU benchmark? Is there really a Tesla Model H? So which LLMs, and which \"Search\", \"Research\", \"Deep Search\" or \"Deep Research\" branded products, are best? How good are their epistemics, compared to if you did the web research yourself? Last month we ( FutureSearch ) published Deep Research Bench (DRB) , a benchmark designed to evaluate LLMs agents on difficult web research tasks using frozen snapshots of the internet. In this post, we're going to share the non-obvious findings, suggestions and failure modes that we think might be useful to anyone who uses LLMs with web search enabled. tl;dr ChatGPT with o3 + websearch outperformed everything else by a decent margin, though is still clearly worse than a skilled human. Use this as your default research tool. Claude web + Claude Research were not able to read PDFs (as of May 6, 2025 and as far as we can tell that's still the case), which likely nerfed their scores. If that's important for your task, don't use them. If you're using LLMs with your own agent via an API, Claude 4 Sonnet + Opus are best, better than o3 (as of June 24, 2025). Grok was ok, but not great. Unless you need to access",
  "tiktok_summary": "Tech Twitter is absolutely losing it over this: LLM-assisted web research has become the norm, but how good is it really? The author claims, according to the latest study, that ChatGPT with o3 + websearch outperforms other LLMs by a significant margin. But what does this mean exactly? The CEO of FutureSearch explains that they've created a benchmark called Deep Research Bench (DRB) to test LLMs on difficult web research tasks using frozen snapshots of the internet. The results are astonishing - and not entirely surprising. We've all been relying on chatbots for answers, but how reliable are they? The answer lies in the performance gap between open and closed-weight models on the MMLU benchmark. Wait, what? Closed-weight models are actually better at some tasks! But the most insane part is still coming: did you know that the top-performing model only achieved its success because it was given access to a massive dataset of pre-existing knowledge? It's like having an unfair advantage! This raises serious questions about the role of LLMs in web research. Are we just creating a new generation of \"lazy researchers\"? Or can these AI tools truly augment human capabilities? The author suggests that the key to successful LLM-assisted web research lies in understanding the limitations of these tools. You see, the more data you feed them, the more biased they become. It's a vicious cycle of knowledge accumulation... and manipulation. In a shocking twist, the study reveals that even the top-performing model struggled with nuanced queries that require critical thinking.",
  "summary_length": 1582,
  "summary_words": 254,
  "summarized_at": "2025-06-28T00:48:36.319823"
}