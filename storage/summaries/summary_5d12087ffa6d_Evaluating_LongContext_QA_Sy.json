{
  "url": "https://eugeneyan.com/writing/qa-evals/?utm_source=tldrai",
  "title": "Evaluating Long\u2011Context Q&A Systems",
  "original_content": "Evaluating Long-Context Question & Answer Systems [ llm eval survey ] \u00b7 28 min read While evaluating Q&A systems is straightforward with short paragraphs, complexity increases as documents grow larger. For example, technical documentation, novels and movies, as well as multi-document scenarios. Although some of these evaluation challenges also appear in shorter contexts, long-context evaluation amplifies issues such as: Information overload: Irrelevant details in large documents obscure relevant facts, making it harder for retrievers and models to locate the right evidence for the answer. Positional variance: Evidence may appear at the beginning, middle, or end of documents, making it a challenge for models with limited effective context or those susceptible to the \u201clost in the middle\u201d problem. Multi-hop reasoning: The correct answer depends on synthesizing several distinct pieces of evidence scattered throughout the text(s), challenging the model\u2019s ability to retain and integrate information that is far apart. Hallucinations at scale: Larger contexts increase the risk of models returning plausible yet incorrect responses due to poor retrieval or limited effective context. Open-ended questions: Queries on broad themes or interpretative topics rarely have a single definitive answer, especially for large documents or corpora. In this write-up, we\u2019ll explore key evaluation metrics , how to build evaluation datasets , and methods to assess Q&A performance through human annotations and LLM-evaluators. We\u2019ll also review several benchmarks across narrative stories, technical and academic texts, and very long-context, multi-document situations. Finally, we\u2019ll wrap up with advice for evaluating long-context Q&A on our specific use cases. An overview of what we'll cover in this writeup By the way, if you want to learn more about evals, my friends Hamel and Shreya are hosting their final cohort of \u201cAI Evals for Engineers and PMs\u201d in July. Here\u2019s a 35% discount code . Key Evaluation Metrics Evaluating Q&A systems goes beyond just checking for factual accuracy. Specifically, we might want answers to be based solely on the provided text, not the model\u2019s knowledge. But even technically correct answers aren\u2019t necessarily helpful. Thus, to evaluate Q&A systems effectively, we should consider two orthogonal dimensions: Faithfulness: How strictly the answer relies on only the source document. Helpfulness: How relevant, comprehensive, and useful the response is for the user. Faithfulness measures whether an answer strictly relies only on the source document. This means the model shouldn\u2019t add external information or make things up (aka hallucinate). Faithfulness is especially important for legal agreements, financial contracts, or medical and insurance forms, where answers must be based solely on the given text. Faithfulness is synonymous with groundedness, where answers must be anchored on the original document. Faithfulness also includes the Q&A system knowing when to say, \u201cI don\u2019t know.\u201d If the source document doesn\u2019t contain the answer, the ideal response is something like, \u201cI don\u2019t have that information in the provided text.\u201d Related to this challenge are two errors by Q&A systems: False positives: When the system makes up an answer that doesn\u2019t exist in the source document (hallucinations). False negatives: When the system incorrectly states that the source document doesn\u2019t contain information that actually is present, either due to poor retrieval or attention limitations over large contexts. We also want to distinguish faithfulness from correctness. An answer might be correct based on general knowledge but still be unfaithful if it contradicts the document. Examples include patient-specific medical instructions that differ from the usual guidelines, definitions in financial or legal agreements that depart from the standard, and historical fiction with alternate timelines. Users depend on Q&A systems to return responses that are faithful to their specific documents, rather than general truths. For systems that provide citations, we can also assess citation accuracy. This evaluates if the cited text supports the answer. Benchmarks like QASPER explicitly evaluate whether models reference the right supporting evidence for the answer. This combined assessment\u2014checking both faithfulness and citation accuracy\u2014provides finer-grained metrics on overall faithfulness and evidence retrieval. However, a faithful answer isn\u2019t always a helpful answer. This is where we also want to evaluate the helpfulness of responses. Helpfulness measures whether an answer is relevant, sufficiently detailed, yet concise. Relevance means the answer directly addresses the user\u2019s question without straying off-topic. Comprehensiveness ensures the answer contains the necessary details. Conciseness balances comprehensiveness by ensuring the answer is succinct, without unnecessary details or fluff. While a brief, one-sentence response to a complex question might be faithful, it falls short of being helpful if the answer needs more details. Conversely, overly long responses filled with extraneous details can overwhelm users, making it hard for users to find the core answer they need. An ideal response should contain most, if not all, of the relevant information from the source document, in a concise way that meets the user\u2019s needs. A study by Xu et al. (2023) found that domain experts in fields like biology or economics preferred answers that were both comprehensive and faithful, particularly for long-form questions. In contrast, crowd-workers often emphasized surface aspects such as conciseness or detail. Thus, if we\u2019re building our Q&A system for power users and experts, the system should focus on returning faithful and comprehensive answers. There\u2019s a tension between faithfulness and helpfulness. An answer can be perfectly faithful yet totally unhelpful. For example, if we ask about a legal contract: \u201cWhat happens if the tenant misses a payment?\u201d A faithful yet unhelpful answer could be, \u201cClause 4.2 of the lease agreement addresses missed payments.\u201d Although technically accurate, it\u2019s not helpful as it doesn\u2019t tell us what actually happens if a payment is missed. The same goes for Q&A systems that simply copy-paste large sections from documents. A useful system should synthesize the information and return a direct answer that meaningfully addresses the user\u2019s question. All in all, the best answers achieve both faithfulness and helpfulness by: Staying grounded in the source text (faithful) Directly addressing the user\u2019s question (relevant) Providing sufficient detail and context (comprehensive) Presenting information clearly and succinctly (concise) Building an Evaluation Dataset Evaluating long-context Q&A begins with creating a robust evaluation dataset. This involves testing how well a Q&A system can navigate book-length documents to answer questions. First, we\u2019ll start with creating a variety of realistic, context-specific questions. While human annotators excel at crafting great questions, this is time-consuming and impractical at scale, especially for lengthy documents. A more efficient approach is to use language models to draft questions that annotators can then accept or edit\u2014this augments human judgment with machine speed and scale. However, just scaling with a language model isn\u2019t enough. We also need to guide the model toward generating natural, useful questions. Thus, instead of vague prompts like \u201cGenerate questions about this chapter,\u201d we can be more specific, such as: \u201cSummarize the main characters in this chapter. Then, generate one question about each character\u2019s backstory based on what we\u2019ve read so far.\u201d More precise prompting helps steer models toward producing useful questions for our evaluation dataset. This approach builds on the methodology of existing benchmarks. NarrativeQA intentionally generates questions based on summaries rather than full texts. This encourages questions that test narrative comprehension rather than shallow fact recall. For the same reason, QASPER creates questions based on abstracts from academic papers that models then answer based on the full paper. By learning from these benchmarks, we can construct evaluation datasets that effectively measure meaningful comprehension of long-context documents. We\u2019ll want to ensure question diversity when creating questions. Having a range of question types helps us evaluate the Q&A system\u2019s capabilities without overfitting to any single type of question. Depending on our use case, an evaluation dataset could include a mix of: Fact recall: These evaluate basic fact retrieval, like \u201cWho is the protagonist?\u201d, \u201cWhen was the treaty signed?\u201d, or \u201cWhat is the legal clause mentioned in Section 2.1?\u201d While simple, they confirm whether our Q&A system can reliably extract information. Definitions: These assess a model\u2019s ability to explain domain-specific content based on the document. Examples include \u201cWhat does this acronym mean in the paper?\u201d, \u201cExplain the magic system introduced in Chapter 7,\u201d or \u201cDefine the economic theory discussed on page 203.\u201d This is important for technical documents to ensure the system can handle specialized terminology in context. Summarization: These measure whether the system can identify the core ideas and coherently summarize them. For example, \u201cSummarize the main findings of the paper\u201d, \u201cRecap what has happened in the book so far\u201d, or \u201cWhat are the key themes discussed in Part 2?\u201d Inference and reasoning: These evaluate the ability to reason beyond explicitly stated facts by integrating information from different parts of the document to form a coherent answer. For example, \u201cWhy did the character make this choice?\u201d or \u201cWhat can we infer about the society from these laws?\u201d \u201cNo-Info\u201d: Unlike previous categories, these questions cannot be answered from the document. For example, \u201cWhat did Gandalf do in the final battle at Hogwarts?\u201d or \u201cWhat is the penalty for trademark infringement in this residential lease agreement?\u201d A faithful Q&A system should recognize that the required information isn\u2019t present and respond accordingly instead of making up an answer. Our Q&A evals should also be robust to the position of evidence within the document. We ensure this by having questions with evidence that appear at the beginning, middle, or end, as well as creating multi-hop questions that require details from several sections or documents. Benchmarks like HELMET evaluate how model accuracy changes based on the location of supporting information, evaluating the model\u2019s ability to pay attention to and combine information from the entire document instead of relying solely on nearby context. Methods to Assess Q&A Performance Human annotators are crucial for building a high-quality, ground-truth dataset . This is useful for calibrating automated evaluators, and with enough annotated examples, we can also train evaluation classifiers or reward models. Here\u2019s how this might look for the metrics of faithfulness and helpfulness: Faithfulness annotation involves evaluating whether an answer accurately reflects the source text. Ideally, we\u2019d like simple binary labels\u2014faithful or unfaithful\u2014but reality is rarely that straightforward. Answers typically exist on a spectrum. As a result, a mostly correct answer that misses a critical detail should be graded differently from one that incorrectly represents minor or peripheral information. Related to faithfulness is the \u201cno-info\u201d annotation . This checks whether the model correctly identifies when the provided context doesn\u2019t contain the information to answer the question. The goal here is to identify hallucinations, where the model invents answers instead of acknowledging the gap. As part of this exercise, we could have the following labels: Incorrect answer / hallucination: The model tries to answer despite missing information, even if the response sounds plausible. Incorrect refusal: The model mistakenly claims the information isn\u2019t present, perhaps due to retrieval errors or inadequate attention to the long context. Correct refusal: The model accurately recognizes the absence of necessary details and appropriately declines to answer. Helpfulness comparisons involve annotators judging which of two faithful answers better meets the user\u2019s needs. Rather than asking for absolute ratings, annotators make relative judgments, answering a straightforward question: \u201cWhich answer is more helpful?\u201d People find comparing two answers easier than assigning absolute ratings, resulting in greater consistency across annotators. When comparing helpfulness, annotators should consider: Relevance : Does one answer more directly and precisely address the question? Comprehensiveness : Does one answer include key information that the other misses? Conciseness : Is one answer more succinct and easier to understand? Here are some practical tips for setting up a reliable annotation process: Start with clear guidelines: Include examples for each category and clarify how to handle edge cases. Also, be concise\u2014it makes it easier to read the entire guide. Iterate on the guidelines: Our initial draft won\u2019t be perfect. Collect annotator feedback on unclear or challenging cases to improve our guidelines. Use qualification tasks: Before assigning actual tasks, provide annotators with practice examples with known correct answers. This ensures they understand the guidelines and can apply them consistently. Measure inter-annotator agreement: Check for consistency among annotators using metrics like Cohen\u2019s Kappa. Low agreement can indicate unclear guidelines or ambiguous scenarios needing further clarification. Consider expert annotators for specialized domains: General annotation tasks can usually be handled by crowd-workers, but domains like medicine or law often require subject-matter experts for accurate and meaningful evaluations. That said, while human annotation is traditionally considered the gold standard, it\u2019s not always practical or scalable, especially for large documents. This is where LLM-evaluators (also called \u201cLLM-as-Judge\u201d) can help. Via this approach, we provide clear criteria\u2014or our annotation guidelines\u2014to a model, and have it evaluate the quality of Q&A responses. But first, it\u2019s important to recognize why older automated metrics fall short. Historically, the language modeling community relied on n-gram-based metrics like BLEU and ROUGE, which measure word overlap between generated responses and reference answers. Although these metrics work somewhat for tasks like machine translation, they correlate poorly with human judgment on open-ended tasks such as Q&A. For example, the L-Eval benchmark highlighted the poor correlation between token-overlap metrics and human judgment for Q&A responses. A correct answer using words that differ from the reference answer can get unfairly penalized by a low ROUGE score, leading to a misleading negative signal. This is especially noticeable when model responses and reference answers vary in length. Without length normalization, token-overlap metrics can mistakenly reward verbose yet mediocre answers over concise, accurate ones. This is why model-based evaluation is increasingly popular\u2014it offers more reliable and nuanced evals than traditional metrics. We typically start by calibrating an LLM-evaluator against a high-quality, human-annotated dataset. With ground truth, we can evaluate our LLM-evaluator by measuring its recall and precision on faithfulness annotations, and its correlation with human judgments on the helpfulness comparisons. To evaluate faithfulness, we can treat answers as collections of individual claims, each of which can be verified as true or false. This is similar to approaches used in NLI-based and Q&A-based summarization metrics, and claim generation and verification . Breaking answers down into atomic claims helps us pinpoint where hallucinations occur. Here\u2019s how it works: Extract claims: Consider this response about a contract dispute: \u201cThe tenant breached the lease because they missed three payments, failed to maintain insurance coverage, and sublet the apartment without permission.\u201d This can be split into: Claim 1: The tenant missed three payments. Claim 2: The tenant failed to maintain required insurance coverage. Claim 3: The tenant sublet the apartment without permission. Verify each claim: Check each statement against the source document (in this case, the lease agreement) to confirm its accuracy. Calculate faithfulness: The proportion of claims supported by the document provides an overall faithfulness score. This fine-grained approach, as demonstrated by evaluations like SummaC , QAFactEval , and RefChecker , offers more interpretability and nuance. Rather than labeling an entire answer as faithful or not, we gain a nuanced understanding of which claims are incorrect. This also allows assigning partial credit to mostly faithful answers with minor inaccuracies. We can also go a step further by requiring the model to provide citations for each claim. This helps distinguish between two different failure modes: hallucinations (making up answers) and retrieval failures (not retrieving relevant information). To evaluate our evaluator, we can compare its judgments to human annotations on two key metrics: (i) recall (of all unfaithful claims, how many does the evaluator correctly flag?) and (ii) precision (of all claims the evaluator flags as unfaithful, how many are truly unfaithful?) Evaluating helpfulness requires a more nuanced approach because often, there isn\u2019t a definitively \u201chelpful\u201d way to answer. Different situations might call for varying levels of detail or explanation styles. Here are several strategies we can consider: Reference-based comparison works well when we have high-quality reference answers. The LLM-evaluator compares generated answers against these references to assess relevance, detail, and clarity. However, as models improve, their answers may surpass existing references, making this method less effective over time. Criteria-based evaluation assesses answers using a clearly defined rubric. This approach allows us to directly reuse our annotation guidelines, focusing on criteria like relevance, comprehensiveness, and conciseness. Pairwise comparisons are particularly useful when iteratively improving Q&A systems. By comparing newly generated answers against previously validated ones, we consistently push quality higher. This method is also ideal for A/B testing different configurations of the Q&A system. To calibrate an LLM-evaluator on helpfulness, pairwise comparisons are especially reliable. By presenting pairs of answers to annotators and LLM-evaluators, we can measure their alignment\u2014how often they agree on the more helpful answer. Correlation metrics, such as Cohen\u2019s Kappa, quantify this alignment effectively. For example, L-Eval found that GPT-4\u2019s pairwise comparisons correlated strongly with human preferences once properly calibrated. What We Can Learn from Existing Benchmarks To ground our discussion so far, let\u2019s look at some benchmarks for long-context Q&A. Besides providing a common standard, these benchmarks highlight challenges we might encounter in dataset creation and evaluation. Since these datasets are likely already part of model training data, we shouldn\u2019t rely solely on them to evaluate our Q&A system. Instead, we\u2019ll want to create evaluation datasets tailored to our use case. We\u2019ll cover six benchmarks spanning (i) narrative documents, (ii) technical and academic documents, and (iii) very long or multi-document contexts. The NarrativeQA dataset , introduced by Ko\u010disk\u00fd et al. in 2017 , is designed to test genuine narrative comprehension rather than surface-level pattern matching. Unlike earlier datasets that allowed models to answer by extracting single sentences, NarrativeQA requires synthesizing information scattered across novels and movie scripts to generate answers. First, the authors collected over 1,500 stories from Project Gutenberg and movie script websites, along with their corresponding plot summaries from Wikipedia. Annotators then generated question-answer pairs based only on these summaries, without viewing the full texts. (Conversely, models answered questions based on the full text but not the summaries.) This deliberate approach ensured that answers couldn\u2019t be found by simple text matching, focusing the evaluation on understanding the entire text. The resulting dataset contains 46,765 question-answer pairs focused on narrative comprehension. Statistics of the NarrativeQA dataset NarrativeQA evaluates whether models can integrate information dispersed throughout long narratives, such as entire books or movies, to produce coherent answers. Answers are evaluated on n-gram matching metrics such as BLEU, METEOR, and ROUGE, comparing machine-generated answers against two reference answers for each question. NarrativeQA highlights the importance of questions that go beyond simple extraction, requiring models to integrate information across the document. By generating questions from summaries instead of full texts, the authors ensured questions required holistic comprehension of the text, thus reducing superficial, extractive answering strategies. NovelQA , introduced by Wang et al. in 2024 , is a benchmark designed for evaluating reading comprehension on very long texts, often exceeding 200,000 tokens. Similar to NarrativeQA but updated for modern times, NovelQA assesses how well models understand and integrate narratives spanning entire novels. Models were evaluated in two formats: multiple-choice and open-ended generation. Two types of responses in NovelQA To build the dataset, the authors selected a diverse set of 89 English novels and collaborated closely with English literature students familiar with these works. Annotators created 2,305 questions in two phases. First, annotators used a question template and filled in entities from the novel to form valid questions (templates below). Templates used to generate questions in NovelQA Then, to enhance question diversity, annotators also freely generated challenging questions. All the questions were then reviewed by the authors, who ultimately accepted 79.4% of the questions. Each question was accompanied by a gold-standard answer and the relevant supporting evidence from the novels to ground evaluations. NovelQA evaluates a model\u2019s ability to synthesize, integrate, and recall detailed information across extremely long contexts. Questions fall into these categories: Detail-oriented (22.2%): Focus on subtle specifics requiring careful recall. Single-hop (42.8%): Answerable from adjacent sentences or closely related passages. Multi-hop (35%): Requires synthesizing information across multiple chapters. The questions cover various narrative aspects, such as characters, plot, setting, and deeper thematic meanings. The benchmark supports both multiple-choice and open-ended generative evaluation methods, with GPT-4 serving as evaluator for generative answers (achieving Cohen\u2019s Kappa of 89.25% against human judgments). Data distribution by complexity and aspect in NovelQA NovelQA\u2019s findings are a shift from the typical \u201clost in the middle\u201d problem\u2014it showed that model performance declines when evidence appears beyond the 100,000-token mark. The authors also highlighted the importance of rigorous quality control, manually reviewing all crowd-generated questions and accepting only 79.4% of question-answer pairs. Finally, explicitly linking each answer to specific supporting evidence helps with retrieval evals. Performance of models decline when evidence is beyond 100k tokens While narrative texts present one kind of challenge, comprehending dense, technical documents introduces an entirely different set of difficulties. QASPER , introduced by Dasigi et al. (2021) , addresses this by testing models on information-seeking questions on academic papers. Specifically, QASPER contains 5,049 questions on 1,585 NLP papers. Similar to NarrativeQA, these questions were crafted by NLP practitioners who had only read paper titles and abstracts. This approach ensures questions often require synthesizing information across the entire paper rather than simple text extraction. Example question, answer, and supporting evidence in QASPER First, 25 NLP practitioners selected papers that interested them and created questions based solely on titles and abstracts. Then, another group of 51 NLP experts answered these questions using the full texts. The latter group\u2019s task included determining if questions were answerable, pinpointing specific supporting evidence (such as text passages, figures, or tables), and providing clear, concise answers. (10% of questions were marked unanswerable and thus excluded.) Separating question generation from answer annotation reduced biases, as question authors had no prior knowledge of the detailed answers. QASPER evaluates models on two main aspects: answer accuracy (Answer-F1) and evidence selection (Evidence-F1). Answer-F1 measures the accuracy of model responses, regardless of whether they extract text directly or create new explanations. Evidence-F1 evaluates the model\u2019s ability to identify supporting details. This is particularly challenging, as more than half of the questions require combining evidence from multiple sections or paragraphs. The Evidence-F1 results in QASPER highlight a significant gap between answer generation and evidence retrieval\u2014even when models give accurate answers, they often struggle to identify the exact supporting passages. Additionally, limiting question creators to only titles and abstracts naturally encouraged questions\u2014and answers\u2014that required a deep understanding of the entire paper, moving beyond superficial extraction. L-Eval by An et al. (2023) covers documents ranging from 3,000 to 200,000 tokens and includes 20 diverse subtasks, 508 extensive documents, and over 2,000 human-annotated question-answer pairs. Unlike previous benchmarks that mainly relied on text-matching metrics, L-Eval also applied LLM-evaluators and measured the difference between both. To build L-Eval, the authors first created four new datasets: Coursera (educational content), SFiction (science fiction stories), CodeU (Python codebases), and LongFQA (financial earnings). They also improved five existing datasets by adding more challenging synthesis-oriented questions, such as augmenting QuALITY to require deeper comprehension of entire documents. Lastly, they reviewed and corrected 12 tasks from prior benchmarks, using Claude-100k to identify and remove inaccuracies or unanswerable questions. Statistics of datasets, question types, and domains in L-Eval L-Eval evaluates two types of tasks: closed-ended (like multiple-choice, code comprehension, true/false, and math), emphasizing precise reasoning, and open-ended (such as narrative synthesis and summarization), focusing on integrating and summarizing long-form content. Closed-ended tasks were evaluated via exact-match accuracy while open-ended tasks had human annotators rating responses from 1 (poor) to 5 (excellent). Additionally, L-Eval used language models like GPT-4 and GPT-3.5 as evaluators through pairwise comparisons for open-ended tasks. These had carefully designed prompts to reduce bias toward overly detailed answers. Traditional n-gram metrics, including ROUGE-L and F1 scores, were also used for efficiency, despite their known sensitivity to response length. L-Eval showed that traditional n-gram metrics often fail to reflect true comprehension in long-context scenarios due to mismatched answer lengths. Additionally, the benchmark demonstrated that using LLMs as evaluators in pairwise comparisons provides superior alignment with human assessments compared to traditional metrics, highlighting clear distinctions in model strengths for closed-ended versus open-ended tasks. HELMET (How to Evaluate Long-context Models Effectively and Thoroughly), introduced by Yen et al. (2025) , addresses issues in earlier benchmarks, such as unrealistic tasks and inconsistent metrics, providing a framework for evaluating long-context language models. To start, the authors identified shortcomings in existing evaluations, including limited context lengths, unreliable methods, and inadequate coverage for non-instruction-tuned models. Then, they created a benchmark with seven task categories: Retrieval-Augmented Generation (RAG), generation with citations, passage re-ranking, many-shot in-context learning, long-document question-answering, summarization, and synthetic recall. Each task contains contexts of up to 128,000 tokens, allowing controlled and consistent assessments with carefully crafted few-shot prompts and model-based metrics. Task categories, datasets, and metrics in HELMET HELMET specifically evaluates these capabilities in long-context models: Retrieval and reasoning: Natural Questions, TriviaQA, and HotpotQA test a model\u2019s ability to find relevant information within extensive contexts containing distractors. Instruction following: Generation tasks requiring citations assess whether models can follow precise formatting guidelines while staying accurate. Comparative reasoning: Passage re-ranking evaluates how well models compare and reason across multiple sections of text. In-context learning: Many-shot tasks measure a model\u2019s ability to quickly adapt and learn from multiple examples provided in-context. Long-form comprehension: Long-document question-answering and summarization tasks assess a model\u2019s capability to synthesize and understand extensive texts. HELMET showed that synthetic tasks like Needle In a Haystack aren\u2019t as useful, due to their weak correlation with real-world scenarios. Also, by carefully controlling input lengths, HELMET could evaluate model robustness to increasingly long contexts that approached previous models\u2019 limits (\u2265128K tokens). Similar to previous benchmarks, HELMET replicated the flaws in traditional n-gram metrics such as ROUGE, which can misrepresent quality in longer outputs. Instead, it recommended using model-based evaluations, using models like GPT-4o, for evaluations that align more closely with human judgment. Comparison of benchmark results across NIAH, Ruler, InfinityBench, and HELMET Loong , by Wang et al. (2024) , is a benchmark that evaluates long-context comprehension across multiple documents. While most earlier benchmarks focus on single-document scenarios, Loong presents realistic, multi-document tasks where missing any relevant document results in incorrect answers. Loong focuses on multi-document Q&A Loong consists of 1,600 evals drawn from financial reports, legal cases, and academic papers in English and Chinese, mainly from 2024. Each task includes evidence spread across multiple documents, mimicking real-world complexity. To generate questions, the authors used two methods: template-based generation, where Q&A pairs were constructed through predefined rules, and free annotation, where GPT-4o was prompted to create additional Q&A pairs. Loong evaluates a model\u2019s ability to locate, compare, cluster, and reason on evidence spread across multiple documents, typically ranging from 10,000 to over 250,000 tokens. The benchmark covers four task types: Spotlight : Finding relevant evidence from one specific document among several. Comparison : Comparing and integrating multiple pieces of information from different documents and returning the right answer. Clustering : Aggregating and grouping relevant information from multiple sources based on specific criteria. Chain of Reasoning : Integrating evidence across documents to return answers. The four evaluation tasks in Loong For evaluation, GPT-4 was used as the LLM-evaluator to score model outputs based on accuracy, hallucinations, and completeness, referencing the golden answer and task requirements. Metrics included (i) average scores (the average evaluation across all questions) and (ii) perfect rate (the percentage of questions receiving a perfect score). Interestingly, their analysis of retrieval-augmented generation (RAG) showed that using RAG reduced performance on the Loong benchmark. They hypothesized that this is because Loong\u2019s evidence is dispersed across multiple documents. While RAG helped somewhat on spotlight tasks, it performed poorly on tasks demanding deeper synthesis, such as comparison, clustering, and multi-step reasoning. The use of RAG degrades performance compared to the baseline Here are some other long-context benchmarks that you may find helpful: A Critical Evaluation of Evaluations for Long-form Question Answering Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models BizBench: A Quantitative Reasoning Benchmark for Business and Finance ELI5: Long Form Question Answering Frustratingly Hard Evidence Retrieval for QA Over Books InfinityBench: Extending Long Context Evaluation Beyond 100K Tokens LFED: A Literary Fiction Evaluation Dataset for Large Language Models LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-Context Multitasks LongReason: A Synthetic Long-Context Reasoning Benchmark via Context Expansion MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents QuALITY: Question Answering with Long Input Texts, Yes! \u2022 \u2022 \u2022 Whew, that was a lot! Here are some key takeaways: Faithfulness and helpfulness are orthogonal dimensions. An answer can be faithful yet unhelpful, or helpful yet contain hallucinated information. Faithfulness also means knowing when to say \u201cI don\u2019t know\u201d. Models should decline to answer when the context lacks information and respond correctly when it does. Traditional n-gram metrics struggle on Q&A. Use LLM-evaluators instead. They\u2019re better at evaluating semantic quality and align more closely with human judgment. The location of evidence matters. Across the benchmarks discussed, some models struggled with the \u201clost in the middle\u201d effect while others had poor performance when the evidence was beyond the 100,000 token mark. Using RAG can reduce performance , especially for tasks requiring cohesive reasoning across evidence dispersed across a single or multiple documents. Did I miss anything important? Any other metrics, methods, or benchmarks you\u2019d suggest I look into? Please let me know ! By the way, if you want to learn more about evals, my friends Hamel and Shreya are hosting their final cohort of \u201cAI Evals for Engineers and PMs\u201d in July. Here\u2019s a 35% discount code . References An, Chenxin, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. \u201cL-Eval: Instituting Standardized Evaluation for Long Context Language Models.\u201d arXiv. https://doi.org/10.48550/arXiv.2307.11088. Bai, Yushi, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, et al. 2024. \u201cLongBench: A Bilingual, Multitask Benchmark for Long Context Understanding.\u201d arXiv. https://doi.org/10.48550/arXiv.2308.14508. Bai, Yushi, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, et al. 2025. \u201cLongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-Context Multitasks.\u201d arXiv. https://doi.org/10.48550/arXiv.2412.15204. Dasigi, Pradeep, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. \u201cA Dataset of Information-Seeking Questions and Answers Anchored in Research Papers.\u201d arXiv. https://doi.org/10.48550/arXiv.2105.03011. Dong, Zican, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2024. \u201cBAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models.\u201d arXiv. https://doi.org/10.48550/arXiv.2309.13345. Fabbri, Alexander, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. \u201cQAFactEval: Improved QA-Based Factual Consistency Evaluation for Summarization.\u201d In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Seattle, United States: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.naacl-main.187. Fan, Angela, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. \u201cELI5: Long Form Question Answering.\u201d arXiv. https://doi.org/10.48550/arXiv.1907.09190. Feng, Song, Siva Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021. \u201cMultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents.\u201d In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 6162\u201376. https://doi.org/10.18653/v1/2021.emnlp-main.498. Hu, Xiangkun, Dongyu Ru, Lin Qiu, Qipeng Guo, Tianhang Zhang, Yang Xu, Yun Luo, Pengfei Liu, Yue Zhang, and Zheng Zhang. 2024. \u201cRefChecker: Reference-Based Fine-Grained Hallucination Checker and Benchmark for Large Language Models.\u201d arXiv. https://doi.org/10.48550/arXiv.2405.14486. Ko\u010disk\u00fd, Tom\u00e1\u0161, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2017. \u201cThe NarrativeQA Reading Comprehension Challenge.\u201d arXiv. https://doi.org/10.48550/arXiv.1712.07040. Koncel-Kedziorski, Rik, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, and Chris Tanner. 2024. \u201cBizBench: A Quantitative Reasoning Benchmark for Business and Finance.\u201d arXiv. https://doi.org/10.48550/arXiv.2311.06602. Kuratov, Yuri, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. 2024. \u201cBABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack.\u201d arXiv. https://doi.org/10.48550/arXiv.2406.10149. Laban, Philippe, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2021. \u201cSummaC: Re-Visiting NLI-Based Models for Inconsistency Detection in Summarization.\u201d arXiv. https://doi.org/10.48550/arXiv.2111.09525. Ling, Zhan, Kang Liu, Kai Yan, Yifan Yang, Weijian Lin, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, and Jiecao Chen. 2025. \u201cLongReason: A Synthetic Long-Context Reasoning Benchmark via Context Expansion.\u201d arXiv. https://doi.org/10.48550/arXiv.2501.15089. Mou, Xiangyang, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui Su. 2020. \u201cFrustratingly Hard Evidence Retrieval for QA Over Books.\u201d arXiv. https://doi.org/10.48550/arXiv.2007.09878. Pang, Richard Yuanzhe, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, et al. 2022. \u201cQuALITY: Question Answering with Long Input Texts, Yes!\u201d arXiv. https://doi.org/10.48550/arXiv.2112.08608. Wang, Chonghua, Haodong Duan, Songyang Zhang, Dahua Lin, and Kai Chen. 2024. \u201cAda-LEval: Evaluating Long-Context LLMs with Length-Adaptable Benchmarks.\u201d arXiv. https://doi.org/10.48550/arXiv.2404.06480. Wang, Cunxiang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, et al. 2024. \u201cNovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens.\u201d arXiv. https://doi.org/10.48550/arXiv.2403.12766. Wang, Minzheng, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, et al. 2024. \u201cLeave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA.\u201d arXiv. https://doi.org/10.48550/arXiv.2406.17419. Xu, Fangyuan, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. \u201cA Critical Evaluation of Evaluations for Long-Form Question Answering.\u201d arXiv. https://doi.org/10.48550/arXiv.2305.18201. Yen, Howard, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. 2025. \u201cHELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly.\u201d arXiv. https://doi.org/10.48550/arXiv.2410.02694. Yu, Linhao, Qun Liu, and Deyi Xiong. 2024. \u201cLFED: A Literary Fiction Evaluation Dataset for Large Language Models.\u201d arXiv. https://doi.org/10.48550/arXiv.2405.10166. Zhang, Xinrong, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, et al. 2024. \u201cInfinityBench: Extending Long Context Evaluation Beyond 100K Tokens.\u201d arXiv. https://doi.org/10.48550/arXiv.2402.13718. If you found this useful, please cite this write-up as: Yan, Ziyou. (Jun 2025). Evaluating Long-Context Question & Answer Systems. eugeneyan.com. https://eugeneyan.com/writing/qa-evals/. or @article{yan2025qa, title = {Evaluating Long-Context Question & Answer Systems}, author = {Yan, Ziyou}, journal = {eugeneyan.com}, year = {2025}, month = {Jun}, url = {https://eugeneyan.com/writing/qa-evals/} } Share on: Browse related tags: [ llm eval survey ] or Search \u00ab AI Engineer 2025 - Improving RecSys & Search with LLM techniques Join 11,100+ readers getting updates on machine learning, RecSys, LLMs, and engineering. Get email updates",
  "tiktok_summary": "BREAKING: Evaluating LongContext Q&A Systems! This is a test summary for dry run mode. #TechNews #AI #TikTok",
  "summary_length": 108,
  "summary_words": 17,
  "summarized_at": "2025-06-28T20:07:51.306226"
}